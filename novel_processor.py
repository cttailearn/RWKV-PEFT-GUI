import gradio as gr
import json
import re
import jieba
from collections import Counter
import os
import requests
import subprocess
import sys
from pathlib import Path

# DeepSeek APIé…ç½®
DEEPSEEK_API_KEY = ""  # ç”¨æˆ·éœ€è¦å¡«å…¥è‡ªå·±çš„APIå¯†é’¥
DEEPSEEK_API_URL = "https://api.deepseek.com/v1/chat/completions"

def extract_keywords_with_ai(text, api_key):
    """ä½¿ç”¨DeepSeek APIæå–å…³é”®è¯"""
    if not api_key:
        return extract_keywords(text)  # å›é€€åˆ°æœ¬åœ°æ–¹æ³•
    
    try:
        headers = {
            "Authorization": f"Bearer {api_key}",
            "Content-Type": "application/json"
        }
        
        prompt = f"è¯·ä»ä»¥ä¸‹æ–‡æœ¬ä¸­æå–3-5ä¸ªæœ€é‡è¦çš„å…³é”®è¯ï¼Œç”¨é¡¿å·åˆ†éš”ï¼Œåªè¿”å›å…³é”®è¯ï¼Œä¸è¦å…¶ä»–å†…å®¹ï¼š\n\n{text[:500]}"  # é™åˆ¶æ–‡æœ¬é•¿åº¦
        
        data = {
            "model": "deepseek-chat",
            "messages": [
                {"role": "user", "content": prompt}
            ],
            "max_tokens": 50,
            "temperature": 0.3
        }
        
        response = requests.post(DEEPSEEK_API_URL, headers=headers, json=data, timeout=10)
        
        if response.status_code == 200:
            result = response.json()
            keywords = result['choices'][0]['message']['content'].strip()
            return keywords if keywords else extract_keywords(text)
        else:
            print(f"APIè¯·æ±‚å¤±è´¥: {response.status_code}")
            return extract_keywords(text)
            
    except Exception as e:
        print(f"APIè°ƒç”¨å‡ºé”™: {str(e)}")
        return extract_keywords(text)

def extract_keywords(text, num_keywords=5):
    """ä»æ–‡æœ¬ä¸­æå–å…³é”®è¯"""
    # ä½¿ç”¨jiebaåˆ†è¯
    words = jieba.cut(text)
    # è¿‡æ»¤åœç”¨è¯å’Œæ ‡ç‚¹ç¬¦å·
    stop_words = {'çš„', 'äº†', 'åœ¨', 'æ˜¯', 'æˆ‘', 'æœ‰', 'å’Œ', 'å°±', 'ä¸', 'äºº', 'éƒ½', 'ä¸€', 'ä¸€ä¸ª', 'ä¸Š', 'ä¹Ÿ', 'å¾ˆ', 'åˆ°', 'è¯´', 'è¦', 'å»', 'ä½ ', 'ä¼š', 'ç€', 'æ²¡æœ‰', 'çœ‹', 'å¥½', 'è‡ªå·±', 'è¿™'}
    filtered_words = [word for word in words if len(word) > 1 and word not in stop_words and word.isalpha()]
    
    # ç»Ÿè®¡è¯é¢‘
    word_freq = Counter(filtered_words)
    # è¿”å›æœ€å¸¸è§çš„å…³é”®è¯
    keywords = [word for word, freq in word_freq.most_common(num_keywords)]
    return 'ã€'.join(keywords) if keywords else 'ç»­å†™å°è¯´'

def clean_text(text):
    """æ¸…ç†æ–‡æœ¬ï¼Œå»é™¤å¤šä½™çš„ç©ºè¡Œå’Œæ ¼å¼"""
    # å»é™¤å¤šä½™çš„ç©ºç™½å­—ç¬¦
    text = re.sub(r'\s+', ' ', text)
    # å»é™¤å¯èƒ½çš„æ ‡é¢˜æ ‡è®°
    text = re.sub(r'^ç¬¬[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹å\d]+ç« .*?\n', '', text, flags=re.MULTILINE)
    text = re.sub(r'^ç« èŠ‚.*?\n', '', text, flags=re.MULTILINE)
    # å»é™¤ç©ºè¡Œ
    lines = [line.strip() for line in text.split('\n') if line.strip()]
    return '\n'.join(lines)

def smart_split_paragraphs(text, min_length=100, max_length=500, use_smart=True):
    """æ™ºèƒ½åˆ†å‰²æ®µè½"""
    if use_smart:
        # æ™ºèƒ½åˆ†æ®µï¼šè€ƒè™‘å¯¹è¯ã€åœºæ™¯è½¬æ¢ç­‰
        paragraphs = []
        
        # å…ˆæŒ‰è‡ªç„¶æ®µåˆ†å‰²
        natural_paragraphs = [p.strip() for p in text.split('\n\n') if p.strip()]
        
        for para in natural_paragraphs:
            if len(para) <= max_length and len(para) >= min_length:
                paragraphs.append(para)
            elif len(para) > max_length:
                # å¯¹è¿‡é•¿æ®µè½è¿›è¡ŒäºŒæ¬¡åˆ†å‰²
                sub_paragraphs = split_long_paragraph(para, min_length, max_length)
                paragraphs.extend(sub_paragraphs)
            else:
                # å¯¹è¿‡çŸ­æ®µè½å°è¯•åˆå¹¶
                if paragraphs and len(paragraphs[-1] + para) <= max_length:
                    paragraphs[-1] += '\n' + para
                else:
                    paragraphs.append(para)
        
        return paragraphs
    else:
        return split_into_paragraphs(text, min_length, max_length)

def split_long_paragraph(text, min_length=100, max_length=500):
    """åˆ†å‰²è¿‡é•¿çš„æ®µè½"""
    # ä¼˜å…ˆæŒ‰å¯¹è¯åˆ†å‰²
    if '"' in text or '"' in text or '"' in text:
        # ä½¿ç”¨ç®€å•çš„æ–¹æ³•æŒ‰å¯¹è¯æ ‡è®°åˆ†å‰²
        parts = re.split(r'(["""]+[^"""]*["""]+)', text)
        result = []
        current = ""
        
        for part in parts:
            if len(current + part) <= max_length:
                current += part
            else:
                if current and len(current) >= min_length:
                    result.append(current)
                current = part
        
        if current and len(current) >= min_length:
            result.append(current)
        
        return result if result else split_into_paragraphs(text, min_length, max_length)
    else:
        return split_into_paragraphs(text, min_length, max_length)

def split_into_paragraphs(text, min_length=100, max_length=500):
    """å°†æ–‡æœ¬åˆ†å‰²æˆåˆé€‚é•¿åº¦çš„æ®µè½"""
    # æŒ‰å¥å·ã€é—®å·ã€æ„Ÿå¹å·åˆ†å‰²
    sentences = re.split(r'[ã€‚ï¼ï¼Ÿ]', text)
    paragraphs = []
    current_paragraph = ""
    
    for sentence in sentences:
        sentence = sentence.strip()
        if not sentence:
            continue
            
        # å¦‚æœå½“å‰æ®µè½åŠ ä¸Šæ–°å¥å­è¶…è¿‡æœ€å¤§é•¿åº¦ï¼Œåˆ™ä¿å­˜å½“å‰æ®µè½
        if len(current_paragraph + sentence) > max_length and len(current_paragraph) >= min_length:
            if current_paragraph:
                paragraphs.append(current_paragraph)
            current_paragraph = sentence
        else:
            if current_paragraph:
                current_paragraph += sentence + "ã€‚"
            else:
                current_paragraph = sentence + "ã€‚"
    
    # æ·»åŠ æœ€åä¸€ä¸ªæ®µè½
    if current_paragraph and len(current_paragraph) >= min_length:
        paragraphs.append(current_paragraph)
    
    return paragraphs

def process_novel(file_obj, min_paragraph_length, max_paragraph_length, use_smart_split, use_ai_keywords, api_key, custom_instruction):
    """å¤„ç†å°è¯´æ–‡ä»¶å¹¶ç”Ÿæˆè®­ç»ƒæ•°æ®"""
    if file_obj is None:
        return "è¯·ä¸Šä¼ å°è¯´æ–‡ä»¶", "", []
    
    try:
        # ä¿®å¤æ–‡ä»¶è¯»å–é”™è¯¯
        if hasattr(file_obj, 'read'):
            content = file_obj.read()
        else:
            # å¦‚æœfile_objæ˜¯æ–‡ä»¶è·¯å¾„å­—ç¬¦ä¸²
            with open(file_obj, 'r', encoding='utf-8', errors='ignore') as f:
                content = f.read()
        
        if isinstance(content, bytes):
            content = content.decode('utf-8', errors='ignore')
        
        # æ¸…ç†æ–‡æœ¬
        cleaned_text = clean_text(content)
        
        # åˆ†å‰²æ®µè½ï¼ˆä½¿ç”¨æ™ºèƒ½åˆ†æ®µæˆ–ä¼ ç»Ÿåˆ†æ®µï¼‰
        if use_smart_split:
            paragraphs = smart_split_paragraphs(cleaned_text, min_paragraph_length, max_paragraph_length, True)
        else:
            paragraphs = split_into_paragraphs(cleaned_text, min_paragraph_length, max_paragraph_length)
        
        if len(paragraphs) < 2:
            return "æ–‡æœ¬å¤ªçŸ­ï¼Œæ— æ³•ç”Ÿæˆè®­ç»ƒæ•°æ®", "", []
        
        # ç”Ÿæˆè®­ç»ƒæ•°æ®
        training_data = []
        paragraph_data = []  # ç”¨äºç•Œé¢æ˜¾ç¤ºå’Œç¼–è¾‘
        
        for i in range(len(paragraphs) - 1):
            input_text = paragraphs[i]
            response_text = paragraphs[i + 1]
            
            # æå–å…³é”®è¯ï¼ˆä½¿ç”¨AIæˆ–æœ¬åœ°æ–¹æ³•ï¼‰
            if use_ai_keywords and api_key:
                keywords = extract_keywords_with_ai(response_text, api_key)
            else:
                keywords = extract_keywords(response_text)
            
            # ä½¿ç”¨è‡ªå®šä¹‰æŒ‡ä»¤æ¨¡æ¿æˆ–é»˜è®¤æ¨¡æ¿
            if custom_instruction:
                instruction = custom_instruction.replace('{keywords}', keywords)
            else:
                instruction = f"è¯·æ ¹æ®å…³é”®è¯'{keywords}'ç»­å†™å°è¯´æ®µè½"
            
            data_item = {
                "text": f"Instruction: {instruction}\n\nInput: {input_text}\n\nResponse: {response_text}"
            }
            training_data.append(data_item)
            
            # ä¿å­˜æ®µè½æ•°æ®ç”¨äºç•Œé¢ç¼–è¾‘
            paragraph_data.append({
                "index": i,
                "input": input_text,
                "response": response_text,
                "keywords": keywords,
                "instruction": instruction
            })
        
        # è½¬æ¢ä¸ºJSONLæ ¼å¼
        jsonl_output = "\n".join([json.dumps(item, ensure_ascii=False) for item in training_data])
        
        status_message = f"å¤„ç†å®Œæˆï¼ç”Ÿæˆäº† {len(training_data)} æ¡è®­ç»ƒæ•°æ®"
        
        return status_message, jsonl_output, paragraph_data
        
    except Exception as e:
        return f"å¤„ç†æ–‡ä»¶æ—¶å‡ºé”™: {str(e)}", "", []

def save_jsonl(jsonl_content, filename):
    """ä¿å­˜JSONLå†…å®¹åˆ°æ–‡ä»¶"""
    if not jsonl_content:
        return "æ²¡æœ‰å†…å®¹å¯ä¿å­˜"
    
    try:
        with open(filename, 'w', encoding='utf-8') as f:
            f.write(jsonl_content)
        return f"æ–‡ä»¶å·²ä¿å­˜ä¸º: {filename}"
    except Exception as e:
        return f"ä¿å­˜æ–‡ä»¶æ—¶å‡ºé”™: {str(e)}"

def convert_jsonl_to_binidx(jsonl_file, output_prefix, tokenizer_type="RWKVTokenizer"):
    """å°†JSONLæ–‡ä»¶è½¬æ¢ä¸ºbinidxæ ¼å¼"""
    try:
        # æ£€æŸ¥json2binidxå·¥å…·æ˜¯å¦å­˜åœ¨ï¼Œä½¿ç”¨ç»å¯¹è·¯å¾„
        current_dir = Path(__file__).parent
        tool_dir = current_dir / "RWKV-PEFT" / "json2binidx_tool"
        if not tool_dir.exists():
            return "é”™è¯¯ï¼šæ‰¾ä¸åˆ°json2binidx_toolç›®å½•"
        
        preprocess_script = tool_dir / "tools" / "preprocess_data.py"
        if not preprocess_script.exists():
            return "é”™è¯¯ï¼šæ‰¾ä¸åˆ°preprocess_data.pyè„šæœ¬"
        
        # é€‰æ‹©tokenizeræ–‡ä»¶
        if tokenizer_type == "RWKVTokenizer":
            vocab_file = tool_dir / "rwkv_vocab_v20230424.txt"
        else:
            vocab_file = tool_dir / "20B_tokenizer.json"
        
        if not vocab_file.exists():
            return f"é”™è¯¯ï¼šæ‰¾ä¸åˆ°tokenizeræ–‡ä»¶: {vocab_file}"
        
        # æ„å»ºè½¬æ¢å‘½ä»¤
        cmd = [
            sys.executable,
            str(preprocess_script),
            "--input", jsonl_file,
            "--output-prefix", output_prefix,
            "--vocab", str(vocab_file),
            "--dataset-impl", "mmap",
            "--tokenizer-type", tokenizer_type,
            "--append-eod"
        ]
        
        # æ‰§è¡Œè½¬æ¢
        result = subprocess.run(cmd, capture_output=True, text=True, cwd=str(tool_dir))
        
        if result.returncode == 0:
            return f"è½¬æ¢æˆåŠŸï¼ç”Ÿæˆæ–‡ä»¶: {output_prefix}.bin å’Œ {output_prefix}.idx"
        else:
            return f"è½¬æ¢å¤±è´¥: {result.stderr}"
            
    except Exception as e:
        return f"è½¬æ¢è¿‡ç¨‹ä¸­å‡ºé”™: {str(e)}"

def update_paragraph_data(paragraph_data, index, new_keywords, new_instruction):
    """æ›´æ–°æ®µè½æ•°æ®"""
    if 0 <= index < len(paragraph_data):
        paragraph_data[index]['keywords'] = new_keywords
        paragraph_data[index]['instruction'] = new_instruction
        
        # é‡æ–°ç”Ÿæˆè®­ç»ƒæ•°æ®é¡¹
        input_text = paragraph_data[index]['input']
        response_text = paragraph_data[index]['response']
        
        data_item = {
            "text": f"Instruction: {new_instruction}\n\nInput: {input_text}\n\nResponse: {response_text}"
        }
        
        return data_item
    return None

def regenerate_jsonl_from_paragraph_data(paragraph_data):
    """ä»æ®µè½æ•°æ®é‡æ–°ç”ŸæˆJSONL"""
    training_data = []
    
    for item in paragraph_data:
        data_item = {
            "text": f"Instruction: {item['instruction']}\n\nInput: {item['input']}\n\nResponse: {item['response']}"
        }
        training_data.append(data_item)
    
    return "\n".join([json.dumps(item, ensure_ascii=False) for item in training_data])

def parse_manual_selection(selection_text, max_count):
    """è§£ææ‰‹åŠ¨é€‰æ‹©çš„æ®µè½ç¼–å·"""
    if not selection_text.strip():
        return []
    
    selected_indices = []
    try:
        parts = selection_text.split(',')
        for part in parts:
            part = part.strip()
            if '-' in part:
                # å¤„ç†èŒƒå›´é€‰æ‹©ï¼Œå¦‚ "5-8"
                start, end = part.split('-')
                start_idx = int(start.strip()) - 1  # è½¬æ¢ä¸º0åŸºç´¢å¼•
                end_idx = int(end.strip()) - 1
                for i in range(start_idx, end_idx + 1):
                    if 0 <= i < max_count:
                        selected_indices.append(i)
            else:
                # å¤„ç†å•ä¸ªé€‰æ‹©
                idx = int(part) - 1  # è½¬æ¢ä¸º0åŸºç´¢å¼•
                if 0 <= idx < max_count:
                    selected_indices.append(idx)
        
        # å»é‡å¹¶æ’åº
        selected_indices = sorted(list(set(selected_indices)))
        return selected_indices
    
    except Exception as e:
        return []

def generate_paragraph_list_html(paragraph_data, selected_indices=None):
    """ç”Ÿæˆæ®µè½åˆ—è¡¨çš„HTML"""
    if not paragraph_data:
        return "<div style='padding: 10px; border: 1px solid #ddd; border-radius: 5px; max-height: 400px; overflow-y: auto;'>è¯·å…ˆåŠ è½½JSONLæ–‡ä»¶</div>"
    
    if selected_indices is None:
        selected_indices = []
    
    html_parts = []
    html_parts.append("<div style='padding: 10px; border: 1px solid #ddd; border-radius: 5px; max-height: 400px; overflow-y: auto;'>")
    
    for i, item in enumerate(paragraph_data):
        checked = "checked" if i in selected_indices else ""
        input_preview = item['input'][:80] + "..." if len(item['input']) > 80 else item['input']
        keywords_preview = item['keywords'][:30] + "..." if len(item['keywords']) > 30 else item['keywords']
        
        html_parts.append(f"""
        <div style='margin-bottom: 10px; padding: 8px; border: 1px solid #eee; border-radius: 3px; background-color: {'#f0f8ff' if checked else '#fafafa'};'>
            <label style='display: flex; align-items: flex-start; cursor: pointer;'>
                <input type='checkbox' {checked} onchange='toggleParagraph({i})' style='margin-right: 8px; margin-top: 2px;'>
                <div style='flex: 1;'>
                    <div style='font-weight: bold; color: #333; margin-bottom: 4px;'>æ®µè½ {i+1}</div>
                    <div style='font-size: 12px; color: #666; margin-bottom: 2px;'>å…³é”®è¯: {keywords_preview}</div>
                    <div style='font-size: 11px; color: #888; line-height: 1.3;'>{input_preview}</div>
                </div>
            </label>
        </div>
        """)
    
    html_parts.append("</div>")
    
    # æ·»åŠ JavaScript
    html_parts.append("""
    <script>
    function toggleParagraph(index) {
        // è¿™é‡Œéœ€è¦é€šè¿‡Gradioçš„æ¥å£æ¥æ›´æ–°é€‰æ‹©çŠ¶æ€
        console.log('Toggle paragraph:', index);
    }
    </script>
    """)
    
    return "".join(html_parts)

def load_jsonl_file(file_path):
    """åŠ è½½JSONLæ–‡ä»¶å¹¶è§£æä¸ºæ®µè½æ•°æ®"""
    if not file_path:
        return [], "è¯·é€‰æ‹©JSONLæ–‡ä»¶"
    
    try:
        paragraph_data = []
        with open(file_path, 'r', encoding='utf-8') as f:
            for i, line in enumerate(f):
                line = line.strip()
                if not line:
                    continue
                
                data = json.loads(line)
                text = data.get('text', '')
                
                # è§£æinstruction, input, response
                parts = text.split('\n\n')
                if len(parts) >= 3:
                    instruction_part = parts[0].replace('Instruction: ', '')
                    input_part = parts[1].replace('Input: ', '')
                    response_part = parts[2].replace('Response: ', '')
                    
                    # ä»instructionä¸­æå–å…³é”®è¯
                    keywords = extract_keywords_from_instruction(instruction_part)
                    
                    paragraph_data.append({
                        'index': i,
                        'input': input_part,
                        'response': response_part,
                        'keywords': keywords,
                        'instruction': instruction_part
                    })
        
        return paragraph_data, f"æˆåŠŸåŠ è½½ {len(paragraph_data)} ä¸ªæ®µè½"
    
    except Exception as e:
        return [], f"åŠ è½½æ–‡ä»¶å¤±è´¥: {str(e)}"

def extract_keywords_from_instruction(instruction):
    """ä»æŒ‡ä»¤ä¸­æå–å…³é”®è¯"""
    # å°è¯•ä»æŒ‡ä»¤ä¸­æå–å…³é”®è¯
    import re
    match = re.search(r"å…³é”®è¯['\"](.*?)['\"]", instruction)
    if match:
        return match.group(1)
    return "ç»­å†™å°è¯´"

def delete_selected_paragraphs(paragraph_data, selected_indices):
    """åˆ é™¤é€‰ä¸­çš„æ®µè½"""
    if not selected_indices:
        return paragraph_data, "æ²¡æœ‰é€‰æ‹©è¦åˆ é™¤çš„æ®µè½"
    
    try:
        # å°†é€‰æ‹©çš„ç´¢å¼•è½¬æ¢ä¸ºå®é™…ç´¢å¼•
        indices_to_delete = []
        for selected in selected_indices:
            idx = int(selected.split(":")[0].replace("æ®µè½ ", "")) - 1
            indices_to_delete.append(idx)
        
        # æŒ‰é™åºæ’åºï¼Œä»åå¾€å‰åˆ é™¤
        indices_to_delete.sort(reverse=True)
        
        new_paragraph_data = paragraph_data.copy()
        for idx in indices_to_delete:
            if 0 <= idx < len(new_paragraph_data):
                new_paragraph_data.pop(idx)
        
        # é‡æ–°ç¼–å·
        for i, item in enumerate(new_paragraph_data):
            item['index'] = i
        
        return new_paragraph_data, f"æˆåŠŸåˆ é™¤ {len(indices_to_delete)} ä¸ªæ®µè½"
    
    except Exception as e:
        return paragraph_data, f"åˆ é™¤å¤±è´¥: {str(e)}"

def ai_regenerate_keywords_batch(paragraph_data, selected_indices, api_key, prompt_template):
    """æ‰¹é‡ä½¿ç”¨AIé‡æ–°ç”Ÿæˆå…³é”®è¯"""
    if not selected_indices:
        return paragraph_data, "æ²¡æœ‰é€‰æ‹©è¦å¤„ç†çš„æ®µè½"
    
    if not api_key:
        return paragraph_data, "è¯·è¾“å…¥APIå¯†é’¥"
    
    try:
        # å°†é€‰æ‹©çš„ç´¢å¼•è½¬æ¢ä¸ºå®é™…ç´¢å¼•
        indices_to_process = []
        for selected in selected_indices:
            idx = int(selected.split(":")[0].replace("æ®µè½ ", "")) - 1
            indices_to_process.append(idx)
        
        new_paragraph_data = paragraph_data.copy()
        success_count = 0
        
        for idx in indices_to_process:
            if 0 <= idx < len(new_paragraph_data):
                response_text = new_paragraph_data[idx]['response']
                
                # ä½¿ç”¨è‡ªå®šä¹‰æç¤ºè¯æ¨¡æ¿
                prompt = prompt_template.replace('{text}', response_text[:500])
                
                # è°ƒç”¨AI API
                new_keywords = extract_keywords_with_ai_custom(response_text, api_key, prompt)
                
                if new_keywords and new_keywords != extract_keywords(response_text):
                    new_paragraph_data[idx]['keywords'] = new_keywords
                    new_paragraph_data[idx]['instruction'] = f"è¯·æ ¹æ®å…³é”®è¯'{new_keywords}'ç»­å†™å°è¯´æ®µè½"
                    success_count += 1
        
        return new_paragraph_data, f"æˆåŠŸä¸º {success_count} ä¸ªæ®µè½é‡æ–°ç”Ÿæˆå…³é”®è¯"
    
    except Exception as e:
        return paragraph_data, f"AIå¤„ç†å¤±è´¥: {str(e)}"

def extract_keywords_with_ai_custom(text, api_key, custom_prompt):
    """ä½¿ç”¨è‡ªå®šä¹‰æç¤ºè¯çš„AIå…³é”®è¯æå–"""
    if not api_key:
        return extract_keywords(text)
    
    try:
        headers = {
            "Authorization": f"Bearer {api_key}",
            "Content-Type": "application/json"
        }
        
        data = {
            "model": "deepseek-chat",
            "messages": [
                {"role": "user", "content": custom_prompt}
            ],
            "max_tokens": 50,
            "temperature": 0.3
        }
        
        response = requests.post(DEEPSEEK_API_URL, headers=headers, json=data, timeout=10)
        
        if response.status_code == 200:
            result = response.json()
            keywords = result['choices'][0]['message']['content'].strip()
            return keywords if keywords else extract_keywords(text)
        else:
            return extract_keywords(text)
            
    except Exception as e:
        return extract_keywords(text)

# åˆ›å»ºGradioç•Œé¢
def create_interface():
    
    with gr.Blocks(title="å°è¯´è®­ç»ƒæ•°æ®ç”Ÿæˆå™¨", theme=gr.themes.Ocean()) as demo:
        # çŠ¶æ€å˜é‡å­˜å‚¨æ®µè½æ•°æ®
        paragraph_data_state = gr.State([])
        
        gr.Markdown("# ğŸš€ å°è¯´è®­ç»ƒæ•°æ®ç”Ÿæˆå™¨")
        gr.Markdown("ä¸Šä¼ å°è¯´æ–‡ä»¶ï¼Œè‡ªåŠ¨ç”ŸæˆæŒ‡ä»¤å¾®è°ƒæ ¼å¼çš„è®­ç»ƒæ•°æ®ï¼Œæ”¯æŒAIå…³é”®è¯æå–å’Œæ™ºèƒ½åˆ†æ®µ")
        
        with gr.Tabs():
            # ä¸»è¦å¤„ç†æ ‡ç­¾é¡µ
            with gr.TabItem("ğŸ“š æ–‡ä»¶å¤„ç†"):
                with gr.Row():
                    with gr.Column(scale=1):
                        gr.Markdown("### ğŸ“ æ–‡ä»¶ä¸Šä¼ ")
                        file_input = gr.File(
                            label="ä¸Šä¼ å°è¯´æ–‡ä»¶",
                            file_types=[".txt", ".md"],
                            type="filepath"
                        )
                        
                        gr.Markdown("### âš™ï¸ å¤„ç†è®¾ç½®")
                        with gr.Row():
                            min_length = gr.Slider(
                                minimum=50,
                                maximum=300,
                                value=100,
                                step=10,
                                label="æœ€å°æ®µè½é•¿åº¦"
                            )
                            max_length = gr.Slider(
                                minimum=200,
                                maximum=800,
                                value=500,
                                step=50,
                                label="æœ€å¤§æ®µè½é•¿åº¦"
                            )
                        
                        use_smart_split = gr.Checkbox(
                            label="ğŸ§  ä½¿ç”¨æ™ºèƒ½åˆ†æ®µ",
                            value=True,
                            info="è€ƒè™‘å¯¹è¯ã€åœºæ™¯è½¬æ¢ç­‰è¿›è¡Œæ™ºèƒ½åˆ†æ®µ"
                        )
                        
                        gr.Markdown("### ğŸ¤– AIè®¾ç½®")
                        use_ai_keywords = gr.Checkbox(
                            label="ä½¿ç”¨AIæå–å…³é”®è¯",
                            value=False,
                            info="ä½¿ç”¨DeepSeek APIæå–æ›´å‡†ç¡®çš„å…³é”®è¯"
                        )
                        
                        api_key_input = gr.Textbox(
                            label="DeepSeek API Key",
                            type="password",
                            placeholder="sk-...",
                            visible=False
                        )
                        
                        custom_instruction = gr.Textbox(
                            label="è‡ªå®šä¹‰æŒ‡ä»¤æ¨¡æ¿",
                            placeholder="è¯·æ ¹æ®å…³é”®è¯'{keywords}'ç»­å†™å°è¯´æ®µè½",
                            info="ä½¿ç”¨{keywords}ä½œä¸ºå…³é”®è¯å ä½ç¬¦"
                        )
                        
                        process_btn = gr.Button("ğŸš€ å¤„ç†æ–‡ä»¶", variant="primary", size="lg")
                        
                    with gr.Column(scale=2):
                        status_output = gr.Textbox(
                            label="ğŸ“Š å¤„ç†çŠ¶æ€",
                            interactive=False
                        )
                        
                        jsonl_output = gr.Textbox(
                            label="ğŸ“„ ç”Ÿæˆçš„JSONLæ•°æ®",
                            lines=15,
                            max_lines=25,
                            interactive=False
                        )
                        
                        with gr.Row():
                            filename_input = gr.Textbox(
                                value="novel_training_data.jsonl",
                                label="ğŸ’¾ ä¿å­˜æ–‡ä»¶å",
                                scale=3
                            )
                            save_btn = gr.Button("ğŸ’¾ ä¿å­˜JSONL", scale=1)
                        
                        save_status = gr.Textbox(
                            label="ğŸ’¾ ä¿å­˜çŠ¶æ€",
                            interactive=False
                        )
            
            # æ®µè½ç¼–è¾‘æ ‡ç­¾é¡µ
            with gr.TabItem("âœï¸ æ®µè½ç¼–è¾‘"):
                with gr.Row():
                    with gr.Column():
                        gr.Markdown("### ğŸ“ JSONLæ–‡ä»¶ä¸Šä¼ ")
                        jsonl_upload = gr.File(
                            label="ä¸Šä¼ JSONLæ–‡ä»¶",
                            file_types=[".jsonl"],
                            type="filepath"
                        )
                        load_jsonl_btn = gr.Button("ğŸ“‚ åŠ è½½JSONLæ–‡ä»¶", variant="secondary")
                        
                        gr.Markdown("### ğŸ“Š æ®µè½ç»Ÿè®¡")
                        paragraph_stats = gr.Textbox(
                            label="ç»Ÿè®¡ä¿¡æ¯",
                            interactive=False
                        )

                        gr.Markdown("### ğŸ“ æ®µè½ç®¡ç†")
                        with gr.Row():
                            select_all_btn = gr.Button("âœ… å…¨é€‰", scale=1)
                            deselect_all_btn = gr.Button("âŒ å–æ¶ˆå…¨é€‰", scale=1)
                        
                        # æ®µè½é€‰æ‹©çŠ¶æ€
                        selected_paragraphs_state = gr.State([])
                        
                        # æ‰‹åŠ¨é€‰æ‹©æ®µè½è¾“å…¥æ¡†
                        manual_selection = gr.Textbox(
                            label="æ‰‹åŠ¨é€‰æ‹©æ®µè½ (è¾“å…¥æ®µè½ç¼–å·ï¼Œç”¨é€—å·åˆ†éš”ï¼Œå¦‚: 1,3,5-8)",
                            placeholder="ä¾‹å¦‚: 1,3,5-8 è¡¨ç¤ºé€‰æ‹©æ®µè½1,3,5,6,7,8",
                            value=""
                        )
                        
                        update_selection_btn = gr.Button("ğŸ”„ æ›´æ–°é€‰æ‹©", variant="secondary", size="sm")
                        
                        # æ®µè½åˆ—è¡¨æ˜¾ç¤ºåŒºåŸŸ
                        paragraph_list_html = gr.HTML(
                            value="<div style='padding: 10px; border: 1px solid #ddd; border-radius: 5px; max-height: 400px; overflow-y: auto;'>è¯·å…ˆåŠ è½½JSONLæ–‡ä»¶</div>",
                            label="æ®µè½åˆ—è¡¨"
                        )
                        
                        with gr.Row():
                            delete_selected_btn = gr.Button("ğŸ—‘ï¸ åˆ é™¤é€‰ä¸­", variant="stop", scale=1)
                            ai_regenerate_btn = gr.Button("ğŸ¤– AIé‡æ–°ç”Ÿæˆå…³é”®è¯", variant="primary", scale=1)
                        
                        gr.Markdown("### ğŸ¤– AIè®¾ç½®")
                        ai_prompt_template = gr.Textbox(
                            label="AIæç¤ºè¯æ¨¡æ¿",
                            value="è¯·ä»ä»¥ä¸‹æ–‡æœ¬ä¸­æå–3-5ä¸ªæœ€é‡è¦çš„å…³é”®è¯ï¼Œç”¨é¡¿å·åˆ†éš”ï¼Œåªè¿”å›å…³é”®è¯ï¼š\n\n{text}",
                            lines=3,
                            info="ä½¿ç”¨{text}ä½œä¸ºæ–‡æœ¬å ä½ç¬¦"
                        )
                        
                        ai_api_key_edit = gr.Textbox(
                            label="DeepSeek API Key",
                            type="password",
                            placeholder="sk-..."
                        )

                    with gr.Column():    
                        gr.Markdown("### ğŸ·ï¸ å•ä¸ªæ®µè½ç¼–è¾‘")
                        single_paragraph_selector = gr.Dropdown(
                            label="é€‰æ‹©è¦ç¼–è¾‘çš„æ®µè½",
                            choices=[],
                            interactive=True
                        )
                        
                        edit_keywords = gr.Textbox(
                            label="å…³é”®è¯",
                            placeholder="å…³é”®è¯1ã€å…³é”®è¯2ã€å…³é”®è¯3"
                        )
                        
                        edit_instruction = gr.Textbox(
                            label="æŒ‡ä»¤",
                            placeholder="è¯·æ ¹æ®å…³é”®è¯'å…³é”®è¯'ç»­å†™å°è¯´æ®µè½"
                        )
                        
                        update_single_btn = gr.Button("ğŸ”„ æ›´æ–°å•ä¸ªæ®µè½", variant="secondary")
                        regenerate_btn = gr.Button("ğŸ”„ é‡æ–°ç”ŸæˆJSONL", variant="primary")
                        
                    with gr.Column(scale=2):
                        
                        
                        gr.Markdown("### ğŸ“– æ®µè½å†…å®¹é¢„è§ˆ")
                        input_preview = gr.Textbox(
                            label="Input (å‰ä¸€æ®µ)",
                            lines=5,
                            interactive=False
                        )
                        
                        response_preview = gr.Textbox(
                            label="Response (åä¸€æ®µ)",
                            lines=5,
                            interactive=False
                        )
                        
                        current_instruction_preview = gr.Textbox(
                            label="å½“å‰æŒ‡ä»¤",
                            lines=2,
                            interactive=False
                        )
                        
                        operation_status = gr.Textbox(
                            label="æ“ä½œçŠ¶æ€",
                            interactive=False
                        )
            
            # æ ¼å¼è½¬æ¢æ ‡ç­¾é¡µ
            with gr.TabItem("ğŸ”„ æ ¼å¼è½¬æ¢"):
                with gr.Row():
                    with gr.Column():
                        gr.Markdown("### ğŸ“Š JSONLè½¬BinIdx")
                        gr.Markdown("å°†JSONLæ ¼å¼è½¬æ¢ä¸ºRWKVè®­ç»ƒæ‰€éœ€çš„BinIdxæ ¼å¼")
                        
                        convert_jsonl_file = gr.File(
                            label="é€‰æ‹©JSONLæ–‡ä»¶",
                            file_types=[".jsonl"]
                        )
                        
                        output_prefix = gr.Textbox(
                            label="è¾“å‡ºæ–‡ä»¶å‰ç¼€",
                            value="./data/novel_data",
                            placeholder="./data/novel_data"
                        )
                        
                        tokenizer_type = gr.Radio(
                            choices=["RWKVTokenizer", "HFTokenizer"],
                            value="RWKVTokenizer",
                            label="Tokenizerç±»å‹",
                            info="RWKVTokenizerç”¨äºå¤šè¯­è¨€æ¨¡å‹ï¼ŒHFTokenizerç”¨äºGPT-NeoXæ¨¡å‹"
                        )
                        
                        convert_btn = gr.Button("ğŸ”„ å¼€å§‹è½¬æ¢", variant="primary")
                        
                    with gr.Column():
                        convert_status = gr.Textbox(
                            label="ğŸ”„ è½¬æ¢çŠ¶æ€",
                            lines=10,
                            interactive=False
                        )
        
        # äº‹ä»¶ç»‘å®š
        def toggle_api_key_visibility(use_ai):
            return gr.update(visible=use_ai)
        
        def process_and_update(file_obj, min_len, max_len, smart_split, ai_keywords, api_key, custom_instr):
            status, jsonl, paragraphs = process_novel(file_obj, min_len, max_len, smart_split, ai_keywords, api_key, custom_instr)
            
            # æ›´æ–°æ®µè½é€‰æ‹©å™¨
            choices = [f"æ®µè½ {i+1}: {p['input'][:50]}..." for i, p in enumerate(paragraphs)]
            
            # ç¡®ä¿è¿”å›çš„æ˜¯æ­£ç¡®çš„ç±»å‹
            return status, jsonl, paragraphs, gr.update(choices=choices, value=None)
        
        def load_jsonl_and_update(file_path):
            """åŠ è½½JSONLæ–‡ä»¶å¹¶æ›´æ–°ç•Œé¢"""
            if not file_path:
                return [], [], "", generate_paragraph_list_html([]), "è¯·é€‰æ‹©JSONLæ–‡ä»¶"
            
            paragraph_data, status = load_jsonl_file(file_path)
            
            # æ›´æ–°ä¸‹æ‹‰é€‰æ‹©å™¨é€‰é¡¹
            dropdown_choices = [f"æ®µè½ {i+1}: {p['input'][:50]}..." for i, p in enumerate(paragraph_data)]
            
            # ç”Ÿæˆæ®µè½åˆ—è¡¨HTML
            paragraph_html = generate_paragraph_list_html(paragraph_data, [])
            
            # ç»Ÿè®¡ä¿¡æ¯
            stats = f"æ€»æ®µè½æ•°: {len(paragraph_data)}"
            
            return paragraph_data, dropdown_choices, stats, paragraph_html, status
        
        def select_all_paragraphs(paragraph_data):
            """å…¨é€‰æ®µè½"""
            if not paragraph_data:
                return [], generate_paragraph_list_html([])
            
            selected_indices = list(range(len(paragraph_data)))
            paragraph_html = generate_paragraph_list_html(paragraph_data, selected_indices)
            return selected_indices, paragraph_html
        
        def deselect_all_paragraphs(paragraph_data):
            """å–æ¶ˆå…¨é€‰"""
            if not paragraph_data:
                return [], generate_paragraph_list_html([])
            
            paragraph_html = generate_paragraph_list_html(paragraph_data, [])
            return [], paragraph_html
        
        def delete_selected_and_update(paragraph_data, selected_indices):
            """åˆ é™¤é€‰ä¸­æ®µè½å¹¶æ›´æ–°ç•Œé¢"""
            if not selected_indices:
                return paragraph_data, [], generate_paragraph_list_html(paragraph_data, []), f"æ€»æ®µè½æ•°: {len(paragraph_data)}", "æ²¡æœ‰é€‰æ‹©è¦åˆ é™¤çš„æ®µè½"
            
            # åˆ é™¤é€‰ä¸­çš„æ®µè½
            indices_to_delete = sorted(selected_indices, reverse=True)
            new_paragraph_data = paragraph_data.copy()
            
            for idx in indices_to_delete:
                if 0 <= idx < len(new_paragraph_data):
                    new_paragraph_data.pop(idx)
            
            # é‡æ–°ç¼–å·
            for i, item in enumerate(new_paragraph_data):
                item['index'] = i
            
            # æ›´æ–°ä¸‹æ‹‰é€‰æ‹©å™¨é€‰é¡¹
            dropdown_choices = [f"æ®µè½ {i+1}: {p['input'][:50]}..." for i, p in enumerate(new_paragraph_data)]
            
            # ç”Ÿæˆæ–°çš„æ®µè½åˆ—è¡¨HTML
            paragraph_html = generate_paragraph_list_html(new_paragraph_data, [])
            
            # ç»Ÿè®¡ä¿¡æ¯
            stats = f"æ€»æ®µè½æ•°: {len(new_paragraph_data)}"
            
            status = f"æˆåŠŸåˆ é™¤ {len(indices_to_delete)} ä¸ªæ®µè½"
            
            return new_paragraph_data, dropdown_choices, [], paragraph_html, stats, status
        
        def ai_regenerate_and_update(paragraph_data, selected_indices, api_key, prompt_template):
            """AIæ‰¹é‡é‡æ–°ç”Ÿæˆå…³é”®è¯å¹¶æ›´æ–°ç•Œé¢"""
            if not selected_indices:
                return paragraph_data, generate_paragraph_list_html(paragraph_data, []), f"æ€»æ®µè½æ•°: {len(paragraph_data)}", "æ²¡æœ‰é€‰æ‹©è¦å¤„ç†çš„æ®µè½"
            
            if not api_key:
                return paragraph_data, generate_paragraph_list_html(paragraph_data, selected_indices), f"æ€»æ®µè½æ•°: {len(paragraph_data)}", "è¯·è¾“å…¥APIå¯†é’¥"
            
            try:
                new_paragraph_data = paragraph_data.copy()
                success_count = 0
                
                for idx in selected_indices:
                    if 0 <= idx < len(new_paragraph_data):
                        response_text = new_paragraph_data[idx]['response']
                        
                        # ä½¿ç”¨è‡ªå®šä¹‰æç¤ºè¯æ¨¡æ¿
                        prompt = prompt_template.replace('{text}', response_text[:500])
                        
                        # è°ƒç”¨AI API
                        new_keywords = extract_keywords_with_ai_custom(response_text, api_key, prompt)
                        
                        if new_keywords and new_keywords != extract_keywords(response_text):
                            new_paragraph_data[idx]['keywords'] = new_keywords
                            new_paragraph_data[idx]['instruction'] = f"è¯·æ ¹æ®å…³é”®è¯'{new_keywords}'ç»­å†™å°è¯´æ®µè½"
                            success_count += 1
                
                # ç”Ÿæˆæ›´æ–°åçš„æ®µè½åˆ—è¡¨HTML
                paragraph_html = generate_paragraph_list_html(new_paragraph_data, selected_indices)
                
                # ç»Ÿè®¡ä¿¡æ¯
                stats = f"æ€»æ®µè½æ•°: {len(new_paragraph_data)}"
                status = f"æˆåŠŸä¸º {success_count} ä¸ªæ®µè½é‡æ–°ç”Ÿæˆå…³é”®è¯"
                
                return new_paragraph_data, paragraph_html, stats, status
            
            except Exception as e:
                return paragraph_data, generate_paragraph_list_html(paragraph_data, selected_indices), f"æ€»æ®µè½æ•°: {len(paragraph_data)}", f"AIå¤„ç†å¤±è´¥: {str(e)}"
        
        def update_manual_selection(paragraph_data, selection_text):
            """æ›´æ–°æ‰‹åŠ¨é€‰æ‹©çš„æ®µè½"""
            if not paragraph_data:
                return [], generate_paragraph_list_html([])
            
            selected_indices = parse_manual_selection(selection_text, len(paragraph_data))
            paragraph_html = generate_paragraph_list_html(paragraph_data, selected_indices)
            
            return selected_indices, paragraph_html
        
        def update_paragraph_preview(paragraph_data, selected_index):
            if not paragraph_data or selected_index is None:
                return "", "", "", ""
            
            try:
                idx = int(selected_index.split(":")[0].replace("æ®µè½ ", "")) - 1
                if 0 <= idx < len(paragraph_data):
                    item = paragraph_data[idx]
                    return item['input'], item['response'], item['keywords'], item['instruction']
            except:
                pass
            
            return "", "", "", ""
        
        def update_paragraph_item(paragraph_data, selected_index, new_keywords, new_instruction):
            if not paragraph_data or selected_index is None:
                return paragraph_data, "è¯·å…ˆé€‰æ‹©æ®µè½"
            
            try:
                idx = int(selected_index.split(":")[0].replace("æ®µè½ ", "")) - 1
                if 0 <= idx < len(paragraph_data):
                    # åˆ›å»ºæ–°çš„åˆ—è¡¨å‰¯æœ¬ä»¥é¿å…çŠ¶æ€ç®¡ç†é—®é¢˜
                    new_paragraph_data = paragraph_data.copy()
                    new_paragraph_data[idx] = paragraph_data[idx].copy()
                    new_paragraph_data[idx]['keywords'] = new_keywords
                    new_paragraph_data[idx]['instruction'] = new_instruction
                    return new_paragraph_data, f"æ®µè½ {idx+1} æ›´æ–°æˆåŠŸ"
            except:
                pass
            
            return paragraph_data, "æ›´æ–°å¤±è´¥"
        
        def regenerate_jsonl(paragraph_data):
            if not paragraph_data:
                return "", "æ²¡æœ‰æ•°æ®å¯ç”Ÿæˆ"
            
            jsonl = regenerate_jsonl_from_paragraph_data(paragraph_data)
            return jsonl, "JSONLé‡æ–°ç”ŸæˆæˆåŠŸ"
        
        def convert_to_binidx(jsonl_file, prefix, tokenizer):
            if jsonl_file is None:
                return "è¯·é€‰æ‹©JSONLæ–‡ä»¶"
            
            return convert_jsonl_to_binidx(jsonl_file.name, prefix, tokenizer)
        
        # ç»‘å®šäº‹ä»¶
        use_ai_keywords.change(
            fn=toggle_api_key_visibility,
            inputs=[use_ai_keywords],
            outputs=[api_key_input]
        )
        
        process_btn.click(
            fn=process_and_update,
            inputs=[file_input, min_length, max_length, use_smart_split, use_ai_keywords, api_key_input, custom_instruction],
            outputs=[status_output, jsonl_output, paragraph_data_state, single_paragraph_selector]
        )
        
        # JSONLæ–‡ä»¶ä¸Šä¼ äº‹ä»¶
        load_jsonl_btn.click(
            fn=load_jsonl_and_update,
            inputs=[jsonl_upload],
            outputs=[paragraph_data_state, single_paragraph_selector, paragraph_stats, paragraph_list_html, operation_status]
        )
        
        # å…¨é€‰/å–æ¶ˆå…¨é€‰äº‹ä»¶
        select_all_btn.click(
            fn=select_all_paragraphs,
            inputs=[paragraph_data_state],
            outputs=[selected_paragraphs_state, paragraph_list_html]
        )
        
        deselect_all_btn.click(
            fn=deselect_all_paragraphs,
            inputs=[paragraph_data_state],
            outputs=[selected_paragraphs_state, paragraph_list_html]
        )
        
        # åˆ é™¤é€‰ä¸­æ®µè½äº‹ä»¶
        delete_selected_btn.click(
            fn=delete_selected_and_update,
            inputs=[paragraph_data_state, selected_paragraphs_state],
            outputs=[paragraph_data_state, single_paragraph_selector, selected_paragraphs_state, paragraph_list_html, paragraph_stats, operation_status]
        )
        
        # AIæ‰¹é‡é‡æ–°ç”Ÿæˆå…³é”®è¯äº‹ä»¶
        ai_regenerate_btn.click(
            fn=ai_regenerate_and_update,
            inputs=[paragraph_data_state, selected_paragraphs_state, ai_api_key_edit, ai_prompt_template],
            outputs=[paragraph_data_state, paragraph_list_html, paragraph_stats, operation_status]
        )
        
        # æ‰‹åŠ¨é€‰æ‹©æ›´æ–°äº‹ä»¶
        update_selection_btn.click(
            fn=update_manual_selection,
            inputs=[paragraph_data_state, manual_selection],
            outputs=[selected_paragraphs_state, paragraph_list_html]
        )
        
        single_paragraph_selector.change(
            fn=update_paragraph_preview,
            inputs=[paragraph_data_state, single_paragraph_selector],
            outputs=[input_preview, response_preview, edit_keywords, current_instruction_preview]
        )
        
        update_single_btn.click(
            fn=update_paragraph_item,
            inputs=[paragraph_data_state, single_paragraph_selector, edit_keywords, edit_instruction],
            outputs=[paragraph_data_state, operation_status]
        )
        
        regenerate_btn.click(
            fn=regenerate_jsonl,
            inputs=[paragraph_data_state],
            outputs=[jsonl_output, operation_status]
        )
        
        save_btn.click(
            fn=save_jsonl,
            inputs=[jsonl_output, filename_input],
            outputs=[save_status]
        )
        
        convert_btn.click(
            fn=convert_to_binidx,
            inputs=[convert_jsonl_file, output_prefix, tokenizer_type],
            outputs=[convert_status]
        )
        
        # æ·»åŠ ä½¿ç”¨è¯´æ˜
        with gr.Accordion("ğŸ“– ä½¿ç”¨è¯´æ˜", open=False):
            gr.Markdown("""
            ## ğŸš€ åŠŸèƒ½ç‰¹ç‚¹
            
            ### ğŸ“š æ–‡ä»¶å¤„ç†
            1. **æ™ºèƒ½åˆ†æ®µ**: è€ƒè™‘å¯¹è¯ã€åœºæ™¯è½¬æ¢ç­‰è¿›è¡Œæ™ºèƒ½æ®µè½åˆ†å‰²
            2. **AIå…³é”®è¯æå–**: ä½¿ç”¨DeepSeek APIæå–æ›´å‡†ç¡®çš„å…³é”®è¯
            3. **è‡ªå®šä¹‰æŒ‡ä»¤**: æ”¯æŒè‡ªå®šä¹‰æŒ‡ä»¤æ¨¡æ¿ï¼Œä½¿ç”¨{keywords}ä½œä¸ºå ä½ç¬¦
            
            ### âœï¸ æ®µè½ç¼–è¾‘
            1. **å¯è§†åŒ–ç¼–è¾‘**: æŸ¥çœ‹å’Œç¼–è¾‘æ¯ä¸ªæ®µè½çš„å…³é”®è¯å’ŒæŒ‡ä»¤
            2. **å®æ—¶é¢„è§ˆ**: å®æ—¶æŸ¥çœ‹æ®µè½å†…å®¹å’Œå½“å‰æŒ‡ä»¤
            3. **æ‰¹é‡æ›´æ–°**: æ”¯æŒé‡æ–°ç”Ÿæˆæ•´ä¸ªJSONLæ–‡ä»¶
            
            ### ğŸ”„ æ ¼å¼è½¬æ¢
            1. **BinIdxè½¬æ¢**: å°†JSONLè½¬æ¢ä¸ºRWKVè®­ç»ƒæ ¼å¼
            2. **å¤šç§Tokenizer**: æ”¯æŒRWKVå’ŒGPT-NeoX tokenizer
            
            ## ğŸ“ æ•°æ®æ ¼å¼
            
            ç”Ÿæˆçš„è®­ç»ƒæ•°æ®æ ¼å¼ï¼š
            ```
            Instruction: è¯·æ ¹æ®å…³é”®è¯'æ­¦åŠŸã€å†…åŠ›ã€ä¿®ç‚¼'ç»­å†™å°è¯´æ®µè½
            
            Input: å‰ä¸€æ®µå°è¯´å†…å®¹...
            
            Response: åä¸€æ®µå°è¯´å†…å®¹...
            ```
            
            ## ğŸ”§ ä½¿ç”¨æ­¥éª¤
            
            1. **ä¸Šä¼ æ–‡ä»¶**: é€‰æ‹©å°è¯´æ–‡æœ¬æ–‡ä»¶
            2. **é…ç½®å‚æ•°**: è®¾ç½®æ®µè½é•¿åº¦ã€åˆ†æ®µæ–¹å¼ç­‰
            3. **AIè®¾ç½®**: å¯é€‰æ‹©ä½¿ç”¨DeepSeek APIæå–å…³é”®è¯
            4. **å¤„ç†æ–‡ä»¶**: ç”Ÿæˆè®­ç»ƒæ•°æ®
            5. **ç¼–è¾‘ä¼˜åŒ–**: åœ¨æ®µè½ç¼–è¾‘é¡µé¢è°ƒæ•´å…³é”®è¯å’ŒæŒ‡ä»¤
            6. **ä¿å­˜å¯¼å‡º**: ä¿å­˜JSONLæ–‡ä»¶æˆ–è½¬æ¢ä¸ºBinIdxæ ¼å¼
            """)
    
    return demo

if __name__ == "__main__":
    # ç¡®ä¿jiebaå·²å®‰è£…
    try:
        import jieba
    except ImportError:
        print("è¯·å…ˆå®‰è£…jieba: pip install jieba")
        exit(1)
    
    demo = create_interface()
    demo.launch(share=False, server_name="127.0.0.1", server_port=None, show_error=True)