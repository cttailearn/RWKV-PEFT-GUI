import gradio as gr
import json
import re
import os
import requests
import subprocess
import sys
from pathlib import Path
import openai
from typing import Dict, Any, Optional, List, Tuple, Generator
import logging
from dataclasses import dataclass
from urllib.parse import urlparse
from tenacity import retry, stop_after_attempt, wait_exponential, retry_if_exception_type
import threading
import time
import uuid
from concurrent.futures import ThreadPoolExecutor, as_completed
from queue import Queue
import gc
import psutil
from datetime import datetime

# å¯é€‰ä¾èµ–
try:
    import chardet
    HAS_CHARDET = True
except ImportError:
    HAS_CHARDET = False

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.StreamHandler(),
        logging.FileHandler('novel_dataset_processing.log', encoding='utf-8')
    ]
)
logger = logging.getLogger(__name__)

# ==================== è¿›åº¦ç®¡ç†å™¨ ====================

class ProgressManager:
    """è¿›åº¦ç®¡ç†å™¨ - å¤„ç†å¤§æ–‡ä»¶æ—¶çš„è¿›åº¦æ˜¾ç¤ºå’Œæ—¥å¿—è¾“å‡º"""
    
    def __init__(self):
        self.total_files = 0
        self.processed_files = 0
        self.total_size = 0
        self.processed_size = 0
        self.start_time = None
        self.current_file = ""
        self.status_queue = Queue()
        self.is_processing = False
        
    def start_processing(self, total_files: int, total_size: int):
        """å¼€å§‹å¤„ç†"""
        self.total_files = total_files
        self.processed_files = 0
        self.total_size = total_size
        self.processed_size = 0
        self.start_time = time.time()
        self.is_processing = True
        
        # è®°å½•å¼€å§‹ä¿¡æ¯
        memory_info = psutil.virtual_memory()
        logger.info(f"="*60)
        logger.info(f"å¼€å§‹å¤„ç†å¤§é‡æ–‡æœ¬æ•°æ®")
        logger.info(f"æ€»æ–‡ä»¶æ•°: {total_files}")
        logger.info(f"æ€»å¤§å°: {self.format_size(total_size)}")
        logger.info(f"å½“å‰å†…å­˜ä½¿ç”¨: {self.format_size(memory_info.used)} / {self.format_size(memory_info.total)}")
        logger.info(f"å¼€å§‹æ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        logger.info(f"="*60)
        
        print(f"\nğŸš€ å¼€å§‹å¤„ç† {total_files} ä¸ªæ–‡ä»¶ï¼Œæ€»å¤§å° {self.format_size(total_size)}")
        print(f"ğŸ“Š å†…å­˜ä½¿ç”¨: {self.format_size(memory_info.used)} / {self.format_size(memory_info.total)}")
        
    def update_file_progress(self, file_path: str, file_size: int):
        """æ›´æ–°æ–‡ä»¶å¤„ç†è¿›åº¦"""
        self.processed_files += 1
        self.processed_size += file_size
        self.current_file = file_path
        
        # è®¡ç®—è¿›åº¦
        file_progress = (self.processed_files / self.total_files) * 100
        size_progress = (self.processed_size / self.total_size) * 100 if self.total_size > 0 else 0
        
        # è®¡ç®—é€Ÿåº¦å’Œé¢„ä¼°æ—¶é—´
        elapsed_time = time.time() - self.start_time
        if elapsed_time > 0:
            speed = self.processed_size / elapsed_time
            remaining_size = self.total_size - self.processed_size
            eta = remaining_size / speed if speed > 0 else 0
        else:
            speed = 0
            eta = 0
            
        # å†…å­˜ä½¿ç”¨æƒ…å†µ
        memory_info = psutil.virtual_memory()
        
        # æ—¥å¿—è¾“å‡º
        logger.info(f"å¤„ç†æ–‡ä»¶ [{self.processed_files}/{self.total_files}]: {os.path.basename(file_path)}")
        logger.info(f"æ–‡ä»¶å¤§å°: {self.format_size(file_size)}, ç´¯è®¡å¤„ç†: {self.format_size(self.processed_size)}")
        logger.info(f"è¿›åº¦: {file_progress:.1f}% (æŒ‰æ–‡ä»¶) | {size_progress:.1f}% (æŒ‰å¤§å°)")
        logger.info(f"å¤„ç†é€Ÿåº¦: {self.format_size(speed)}/s, é¢„è®¡å‰©ä½™: {self.format_time(eta)}")
        logger.info(f"å†…å­˜ä½¿ç”¨: {self.format_size(memory_info.used)} ({memory_info.percent:.1f}%)")
        
        # ç»ˆç«¯è¾“å‡º
        print(f"\rğŸ“ [{self.processed_files}/{self.total_files}] {os.path.basename(file_path)[:50]:<50} "
              f"{file_progress:5.1f}% | {self.format_size(speed)}/s | ETA: {self.format_time(eta)}", end="", flush=True)
        
        # å†…å­˜è­¦å‘Š
        if memory_info.percent > 80:
            logger.warning(f"å†…å­˜ä½¿ç”¨ç‡è¿‡é«˜: {memory_info.percent:.1f}%ï¼Œå»ºè®®é‡Šæ”¾å†…å­˜")
            print(f"\nâš ï¸  å†…å­˜ä½¿ç”¨ç‡: {memory_info.percent:.1f}%")
            
    def update_paragraph_progress(self, current: int, total: int, file_name: str):
        """æ›´æ–°æ®µè½å¤„ç†è¿›åº¦"""
        progress = (current / total) * 100 if total > 0 else 0
        
        if current % 100 == 0 or current == total:  # æ¯100ä¸ªæ®µè½æˆ–å®Œæˆæ—¶è¾“å‡º
            logger.info(f"æ–‡ä»¶ {os.path.basename(file_name)} - æ®µè½å¤„ç†è¿›åº¦: {current}/{total} ({progress:.1f}%)")
            print(f"\r  ğŸ“ å¤„ç†æ®µè½: {current}/{total} ({progress:5.1f}%)", end="", flush=True)
            
    def log_memory_usage(self, context: str = ""):
        """è®°å½•å†…å­˜ä½¿ç”¨æƒ…å†µ"""
        memory_info = psutil.virtual_memory()
        process = psutil.Process()
        process_memory = process.memory_info()
        
        logger.info(f"å†…å­˜ä½¿ç”¨æƒ…å†µ {context}:")
        logger.info(f"  ç³»ç»Ÿå†…å­˜: {self.format_size(memory_info.used)} / {self.format_size(memory_info.total)} ({memory_info.percent:.1f}%)")
        logger.info(f"  è¿›ç¨‹å†…å­˜: {self.format_size(process_memory.rss)}")
        
    def finish_processing(self, total_paragraphs: int, total_training_data: int):
        """å®Œæˆå¤„ç†"""
        self.is_processing = False
        elapsed_time = time.time() - self.start_time
        
        logger.info(f"="*60)
        logger.info(f"å¤„ç†å®Œæˆï¼")
        logger.info(f"æ€»è€—æ—¶: {self.format_time(elapsed_time)}")
        logger.info(f"å¤„ç†æ–‡ä»¶: {self.processed_files} ä¸ª")
        logger.info(f"å¤„ç†æ•°æ®: {self.format_size(self.processed_size)}")
        logger.info(f"ç”Ÿæˆæ®µè½: {total_paragraphs} ä¸ª")
        logger.info(f"è®­ç»ƒæ•°æ®: {total_training_data} æ¡")
        logger.info(f"å¹³å‡é€Ÿåº¦: {self.format_size(self.processed_size / elapsed_time)}/s")
        logger.info(f"å®Œæˆæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        logger.info(f"="*60)
        
        print(f"\n\nâœ… å¤„ç†å®Œæˆï¼")
        print(f"â±ï¸  æ€»è€—æ—¶: {self.format_time(elapsed_time)}")
        print(f"ğŸ“Š å¤„ç†äº† {self.processed_files} ä¸ªæ–‡ä»¶ï¼Œ{self.format_size(self.processed_size)} æ•°æ®")
        print(f"ğŸ“ ç”Ÿæˆ {total_paragraphs} ä¸ªæ®µè½ï¼Œ{total_training_data} æ¡è®­ç»ƒæ•°æ®")
        print(f"ğŸš€ å¹³å‡é€Ÿåº¦: {self.format_size(self.processed_size / elapsed_time)}/s")
        
    @staticmethod
    def format_size(size_bytes: float) -> str:
        """æ ¼å¼åŒ–æ–‡ä»¶å¤§å°"""
        if size_bytes == 0:
            return "0B"
        
        for unit in ['B', 'KB', 'MB', 'GB', 'TB']:
            if size_bytes < 1024.0:
                return f"{size_bytes:.1f}{unit}"
            size_bytes /= 1024.0
        return f"{size_bytes:.1f}PB"
        
    @staticmethod
    def format_time(seconds: float) -> str:
        """æ ¼å¼åŒ–æ—¶é—´"""
        if seconds < 60:
            return f"{seconds:.1f}s"
        elif seconds < 3600:
            return f"{seconds/60:.1f}m"
        else:
            return f"{seconds/3600:.1f}h"

# ==================== AIæ¨¡å‹é…ç½® ====================

@dataclass
class ModelConfig:
    """AIæ¨¡å‹é…ç½®ç±»"""
    name: str
    base_url: str
    api_key: str
    model_name: str
    timeout: int = 60
    max_retries: int = 3
    temperature: float = 0.7
    max_tokens: Optional[int] = None
    
    def __post_init__(self):
        """é…ç½®éªŒè¯"""
        if not self.base_url or not self.api_key or not self.model_name:
            raise ValueError("base_url, api_key, model_name ä¸èƒ½ä¸ºç©º")
        
        # éªŒè¯URLæ ¼å¼
        parsed = urlparse(self.base_url)
        if not parsed.scheme or not parsed.netloc:
            raise ValueError("base_url æ ¼å¼æ— æ•ˆ")

class AIModelClient:
    """AIæ¨¡å‹å®¢æˆ·ç«¯"""
    @classmethod
    def load_custom_configs(cls) -> Dict[str, Dict]:
        """åŠ è½½è‡ªå®šä¹‰æ¨¡å‹é…ç½®"""
        config_file = "novel_model_configs.json"
        try:
            if os.path.exists(config_file):
                with open(config_file, 'r', encoding='utf-8') as f:
                    return json.load(f)
        except Exception as e:
            logger.warning(f"åŠ è½½æ¨¡å‹é…ç½®å¤±è´¥: {str(e)}")
        return {}
    
    @classmethod
    def save_custom_config(cls, name: str, config: Dict[str, Any]):
        """ä¿å­˜è‡ªå®šä¹‰æ¨¡å‹é…ç½®"""
        config_file = "novel_model_configs.json"
        try:
            configs = cls.load_custom_configs()
            configs[name] = config
            with open(config_file, 'w', encoding='utf-8') as f:
                json.dump(configs, f, ensure_ascii=False, indent=2)
            logger.info(f"æ¨¡å‹é…ç½® {name} å·²ä¿å­˜")
        except Exception as e:
            logger.error(f"ä¿å­˜æ¨¡å‹é…ç½®å¤±è´¥: {str(e)}")
    
    @classmethod
    def get_all_configs(cls) -> Dict[str, Dict]:
        """è·å–æ‰€æœ‰æ¨¡å‹é…ç½®ï¼ˆä»…è‡ªå®šä¹‰ï¼‰"""
        # åªè¿”å›è‡ªå®šä¹‰é…ç½®ï¼Œä¸å†æœ‰é¢„è®¾æ¨¡å‹
        return cls.load_custom_configs()
    
    @classmethod
    def delete_config(cls, name: str) -> bool:
        """åˆ é™¤æ¨¡å‹é…ç½®"""
        try:
            configs = cls.load_custom_configs()
            if name in configs:
                del configs[name]
                config_file = "novel_model_configs.json"
                with open(config_file, 'w', encoding='utf-8') as f:
                    json.dump(configs, f, ensure_ascii=False, indent=2)
                logger.info(f"æ¨¡å‹é…ç½® {name} å·²åˆ é™¤")
                return True
            return False
        except Exception as e:
            logger.error(f"åˆ é™¤æ¨¡å‹é…ç½®å¤±è´¥: {str(e)}")
            return False
    
    def __init__(self, config: ModelConfig):
        self.config = config
        self.client = openai.OpenAI(
            base_url=config.base_url,
            api_key=config.api_key,
            timeout=config.timeout,
            max_retries=config.max_retries
        )
        logger.info(f"AIæ¨¡å‹å®¢æˆ·ç«¯å·²åˆå§‹åŒ–: {config.name}")
    
    @retry(
        stop=stop_after_attempt(3),
        wait=wait_exponential(multiplier=1, min=4, max=10),
        retry=retry_if_exception_type((openai.RateLimitError, openai.APITimeoutError))
    )
    def extract_keywords(self, text: str, prompt: str) -> str:
        """ä½¿ç”¨AIæ¨¡å‹æå–å…³é”®è¯"""
        try:
            messages = [
                {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„å°è¯´åˆ†æåŠ©æ‰‹ï¼Œæ“…é•¿ä»æ–‡æœ¬ä¸­æå–å…³é”®è¯ã€‚"},
                {"role": "user", "content": f"{prompt}\n\næ–‡æœ¬å†…å®¹ï¼š{text[:500]}"}
            ]
            
            response = self.client.chat.completions.create(
                model=self.config.model_name,
                messages=messages,
                temperature=self.config.temperature,
                max_tokens=self.config.max_tokens or 50
            )
            
            return response.choices[0].message.content.strip()
            
        except Exception as e:
            logger.error(f"AIå…³é”®è¯æå–å¤±è´¥: {str(e)}")
            raise
    
    def test_connection(self) -> bool:
        """æµ‹è¯•è¿æ¥"""
        try:
            response = self.extract_keywords("æµ‹è¯•æ–‡æœ¬", "è¯·å›å¤'è¿æ¥æˆåŠŸ'")
            return "è¿æ¥æˆåŠŸ" in response or "æˆåŠŸ" in response
        except Exception:
            return False

# ==================== æç¤ºè¯ä»»åŠ¡ç®¡ç† ====================

class PromptTaskManager:
    """æç¤ºè¯ä»»åŠ¡ç®¡ç†å™¨"""
    
    def __init__(self):
        self.tasks_file = "novel_prompt_tasks.json"
        self.default_tasks = {
            "å°è¯´å…³é”®è¯æå–": "è¯·ä»ä»¥ä¸‹å°è¯´æ–‡æœ¬ä¸­æå–3-5ä¸ªæœ€é‡è¦çš„å…³é”®è¯ï¼Œç”¨é¡¿å·åˆ†éš”ï¼Œåªè¿”å›å…³é”®è¯ï¼Œä¸è¦å…¶ä»–å†…å®¹ã€‚",
            "æ­¦ä¾ å°è¯´å…³é”®è¯": "è¯·ä»ä»¥ä¸‹æ­¦ä¾ å°è¯´æ–‡æœ¬ä¸­æå–å…³é”®è¯ï¼Œé‡ç‚¹å…³æ³¨æ­¦åŠŸã€äººç‰©ã€åœºæ™¯ç­‰å…ƒç´ ï¼Œç”¨é¡¿å·åˆ†éš”ã€‚",
            "ç°ä»£å°è¯´å…³é”®è¯": "è¯·ä»ä»¥ä¸‹ç°ä»£å°è¯´æ–‡æœ¬ä¸­æå–å…³é”®è¯ï¼Œé‡ç‚¹å…³æ³¨æƒ…æ„Ÿã€åœºæ™¯ã€äººç‰©å…³ç³»ç­‰ï¼Œç”¨é¡¿å·åˆ†éš”ã€‚",
            "ç„å¹»å°è¯´å…³é”®è¯": "è¯·ä»ä»¥ä¸‹ç„å¹»å°è¯´æ–‡æœ¬ä¸­æå–å…³é”®è¯ï¼Œé‡ç‚¹å…³æ³¨ä¿®ç‚¼ã€æ³•æœ¯ã€å¢ƒç•Œç­‰å…ƒç´ ï¼Œç”¨é¡¿å·åˆ†éš”ã€‚",
            "è¨€æƒ…å°è¯´å…³é”®è¯": "è¯·ä»ä»¥ä¸‹è¨€æƒ…å°è¯´æ–‡æœ¬ä¸­æå–å…³é”®è¯ï¼Œé‡ç‚¹å…³æ³¨æƒ…æ„Ÿã€å…³ç³»ã€åœºæ™¯ç­‰ï¼Œç”¨é¡¿å·åˆ†éš”ã€‚",
            "ç§‘å¹»å°è¯´å…³é”®è¯": "è¯·ä»ä»¥ä¸‹ç§‘å¹»å°è¯´æ–‡æœ¬ä¸­æå–å…³é”®è¯ï¼Œé‡ç‚¹å…³æ³¨ç§‘æŠ€ã€æœªæ¥ã€æ¢ç´¢ç­‰å…ƒç´ ï¼Œç”¨é¡¿å·åˆ†éš”ã€‚"
        }
        self.tasks = self.load_tasks()
    
    def load_tasks(self) -> Dict[str, str]:
        """åŠ è½½æç¤ºè¯ä»»åŠ¡"""
        tasks = self.default_tasks.copy()
        
        try:
            if os.path.exists(self.tasks_file):
                with open(self.tasks_file, 'r', encoding='utf-8') as f:
                    saved_tasks = json.load(f)
                tasks.update(saved_tasks)
                logger.info(f"å·²åŠ è½½ {len(saved_tasks)} ä¸ªè‡ªå®šä¹‰ä»»åŠ¡")
        except Exception as e:
            logger.warning(f"åŠ è½½ä»»åŠ¡æ–‡ä»¶å¤±è´¥: {str(e)}")
        
        return tasks
    
    def save_tasks(self):
        """ä¿å­˜æç¤ºè¯ä»»åŠ¡"""
        try:
            custom_tasks = {k: v for k, v in self.tasks.items() if k not in self.default_tasks or v != self.default_tasks.get(k)}
            with open(self.tasks_file, 'w', encoding='utf-8') as f:
                json.dump(custom_tasks, f, ensure_ascii=False, indent=2)
            logger.info(f"å·²ä¿å­˜ {len(custom_tasks)} ä¸ªä»»åŠ¡")
        except Exception as e:
            logger.error(f"ä¿å­˜ä»»åŠ¡æ–‡ä»¶å¤±è´¥: {str(e)}")
    
    def add_task(self, name: str, prompt: str):
        """æ·»åŠ æˆ–æ›´æ–°ä»»åŠ¡"""
        self.tasks[name] = prompt
        self.save_tasks()
        logger.info(f"ä»»åŠ¡ '{name}' å·²ä¿å­˜")
    
    def delete_task(self, name: str) -> bool:
        """åˆ é™¤ä»»åŠ¡"""
        if name in self.default_tasks:
            self.tasks[name] = self.default_tasks[name]
            self.save_tasks()
            logger.info(f"é»˜è®¤ä»»åŠ¡ '{name}' å·²é‡ç½®")
            return True
        
        if name in self.tasks:
            del self.tasks[name]
            self.save_tasks()
            logger.info(f"è‡ªå®šä¹‰ä»»åŠ¡ '{name}' å·²åˆ é™¤")
            return True
        return False
    
    def get_task_names(self) -> List[str]:
        """è·å–æ‰€æœ‰ä»»åŠ¡åç§°"""
        return list(self.tasks.keys())
    
    def get_task_prompt(self, name: str) -> str:
        """è·å–ä»»åŠ¡æç¤ºè¯"""
        return self.tasks.get(name, "")
    
    def is_default_task(self, name: str) -> bool:
        """åˆ¤æ–­æ˜¯å¦ä¸ºé»˜è®¤ä»»åŠ¡"""
        return name in self.default_tasks

# ==================== æ–‡ä»¶å¤„ç†æ¨¡å— ====================

class FileProcessor:
    """æ–‡ä»¶å¤„ç†å™¨ - ä¼˜åŒ–å¤§æ–‡ä»¶å¤„ç†"""
    
    # å¤§æ–‡ä»¶å¤„ç†é…ç½®
    CHUNK_SIZE = 8 * 1024 * 1024  # 8MB å—å¤§å°
    MAX_MEMORY_USAGE = 2 * 1024 * 1024 * 1024  # 2GB æœ€å¤§å†…å­˜ä½¿ç”¨
    LARGE_FILE_THRESHOLD = 100 * 1024 * 1024  # 100MB å¤§æ–‡ä»¶é˜ˆå€¼
    
    @staticmethod
    def detect_encoding(file_path: str) -> str:
        """æ£€æµ‹æ–‡ä»¶ç¼–ç """
        if HAS_CHARDET:
            try:
                with open(file_path, 'rb') as f:
                    raw_data = f.read(10000)
                result = chardet.detect(raw_data)
                if result['encoding'] and result['confidence'] > 0.7:
                    return result['encoding']
            except Exception:
                pass
        
        encodings = ['utf-8', 'utf-8-sig', 'gbk', 'gb2312', 'gb18030', 'big5', 'latin1']
        
        for encoding in encodings:
            try:
                with open(file_path, 'r', encoding=encoding) as f:
                    f.read(1000)  # åªè¯»å–å°‘é‡å†…å®¹è¿›è¡Œæµ‹è¯•
                return encoding
            except (UnicodeDecodeError, UnicodeError):
                continue
        
        return 'utf-8'
    
    @staticmethod
    def get_file_size(file_path: str) -> int:
        """è·å–æ–‡ä»¶å¤§å°"""
        try:
            return os.path.getsize(file_path)
        except Exception:
            return 0
    
    @staticmethod
    def read_text_file_streaming(file_path: str) -> Generator[str, None, None]:
        """æµå¼è¯»å–å¤§æ–‡æœ¬æ–‡ä»¶"""
        try:
            encoding = FileProcessor.detect_encoding(file_path)
            file_size = FileProcessor.get_file_size(file_path)
            
            logger.info(f"å¼€å§‹æµå¼è¯»å–æ–‡ä»¶: {file_path} ({ProgressManager.format_size(file_size)})")
            
            with open(file_path, 'r', encoding=encoding) as f:
                while True:
                    chunk = f.read(FileProcessor.CHUNK_SIZE)
                    if not chunk:
                        break
                    yield chunk
                    
        except Exception as e:
            logger.error(f"æµå¼è¯»å–æ–‡ä»¶å¤±è´¥ {file_path}: {str(e)}")
            raise Exception(f"è¯»å–æ–‡ä»¶å¤±è´¥: {str(e)}")
    
    @staticmethod
    def read_text_file(file_path: str) -> str:
        """è¯»å–æ–‡æœ¬æ–‡ä»¶ - æ™ºèƒ½é€‰æ‹©è¯»å–æ–¹å¼"""
        try:
            file_size = FileProcessor.get_file_size(file_path)
            
            # å°æ–‡ä»¶ç›´æ¥è¯»å–
            if file_size < FileProcessor.LARGE_FILE_THRESHOLD:
                encoding = FileProcessor.detect_encoding(file_path)
                with open(file_path, 'r', encoding=encoding) as f:
                    return f.read()
            
            # å¤§æ–‡ä»¶æµå¼è¯»å–å¹¶æ‹¼æ¥
            logger.info(f"æ£€æµ‹åˆ°å¤§æ–‡ä»¶ {file_path} ({ProgressManager.format_size(file_size)})ï¼Œä½¿ç”¨æµå¼è¯»å–")
            content_chunks = []
            total_size = 0
            
            for chunk in FileProcessor.read_text_file_streaming(file_path):
                content_chunks.append(chunk)
                total_size += len(chunk.encode('utf-8'))
                
                # å†…å­˜ä½¿ç”¨æ£€æŸ¥
                memory_info = psutil.virtual_memory()
                if memory_info.percent > 85:  # å†…å­˜ä½¿ç”¨è¶…è¿‡85%
                    logger.warning(f"å†…å­˜ä½¿ç”¨ç‡è¿‡é«˜ ({memory_info.percent:.1f}%)ï¼Œæš‚åœè¯»å–")
                    gc.collect()  # å¼ºåˆ¶åƒåœ¾å›æ”¶
                    time.sleep(0.1)  # çŸ­æš‚æš‚åœ
                    
                if total_size > FileProcessor.MAX_MEMORY_USAGE:
                    logger.warning(f"æ–‡ä»¶è¿‡å¤§ï¼Œå·²è¯»å– {ProgressManager.format_size(total_size)}ï¼Œåœæ­¢è¯»å–")
                    break
            
            return ''.join(content_chunks)
            
        except Exception as e:
            raise Exception(f"è¯»å–æ–‡ä»¶å¤±è´¥: {str(e)}")
    
    @staticmethod
    def scan_directory(directory_path: str) -> List[Tuple[str, int]]:
        """æ‰«æç›®å½•ï¼Œè·å–æ–‡ä»¶åˆ—è¡¨å’Œå¤§å°ä¿¡æ¯"""
        supported_extensions = ['.txt', '.md', '.text']
        files_info = []
        
        try:
            directory = Path(directory_path)
            logger.info(f"å¼€å§‹æ‰«æç›®å½•: {directory_path}")
            
            for file_path in directory.rglob('*'):
                if file_path.is_file() and file_path.suffix.lower() in supported_extensions:
                    try:
                        file_size = FileProcessor.get_file_size(str(file_path))
                        files_info.append((str(file_path), file_size))
                    except Exception as e:
                        logger.warning(f"æ— æ³•è·å–æ–‡ä»¶ä¿¡æ¯ {file_path}: {str(e)}")
            
            # æŒ‰æ–‡ä»¶å¤§å°æ’åºï¼Œå°æ–‡ä»¶ä¼˜å…ˆå¤„ç†
            files_info.sort(key=lambda x: x[1])
            
            total_size = sum(size for _, size in files_info)
            logger.info(f"æ‰«æå®Œæˆ: æ‰¾åˆ° {len(files_info)} ä¸ªæ–‡ä»¶ï¼Œæ€»å¤§å° {ProgressManager.format_size(total_size)}")
            
            return files_info
            
        except Exception as e:
            raise Exception(f"æ‰«æç›®å½•å¤±è´¥: {str(e)}")
    
    @staticmethod
    def process_directory_streaming(directory_path: str, progress_manager: ProgressManager) -> Generator[Tuple[str, str], None, None]:
        """æµå¼å¤„ç†ç›®å½•ä¸­çš„æ‰€æœ‰æ–‡æœ¬æ–‡ä»¶"""
        try:
            # æ‰«æç›®å½•
            files_info = FileProcessor.scan_directory(directory_path)
            
            if not files_info:
                logger.warning(f"ç›®å½• {directory_path} ä¸­æ²¡æœ‰æ‰¾åˆ°æ”¯æŒçš„æ–‡æœ¬æ–‡ä»¶")
                return
            
            total_files = len(files_info)
            total_size = sum(size for _, size in files_info)
            
            # å¼€å§‹å¤„ç†
            progress_manager.start_processing(total_files, total_size)
            
            for file_path, file_size in files_info:
                try:
                    logger.info(f"å¼€å§‹å¤„ç†æ–‡ä»¶: {file_path}")
                    
                    # å†…å­˜æ£€æŸ¥
                    memory_info = psutil.virtual_memory()
                    if memory_info.percent > 90:
                        logger.warning(f"å†…å­˜ä½¿ç”¨ç‡è¿‡é«˜ ({memory_info.percent:.1f}%)ï¼Œæ‰§è¡Œåƒåœ¾å›æ”¶")
                        gc.collect()
                        time.sleep(0.5)
                    
                    # è¯»å–æ–‡ä»¶å†…å®¹
                    content = FileProcessor.read_text_file(file_path)
                    
                    # æ›´æ–°è¿›åº¦
                    progress_manager.update_file_progress(file_path, file_size)
                    
                    yield (file_path, content)
                    
                    # é‡Šæ”¾å†…å­˜
                    del content
                    gc.collect()
                    
                except Exception as e:
                    logger.warning(f"è·³è¿‡æ–‡ä»¶ {file_path}: {str(e)}")
                    progress_manager.update_file_progress(file_path, file_size)
                    continue
                    
        except Exception as e:
            raise Exception(f"æµå¼å¤„ç†ç›®å½•å¤±è´¥: {str(e)}")
    
    @staticmethod
    def process_directory(directory_path: str) -> List[Tuple[str, str]]:
        """å¤„ç†ç›®å½•ä¸­çš„æ‰€æœ‰æ–‡æœ¬æ–‡ä»¶ - å…¼å®¹æ€§æ–¹æ³•"""
        progress_manager = ProgressManager()
        files_content = []
        
        try:
            for file_path, content in FileProcessor.process_directory_streaming(directory_path, progress_manager):
                files_content.append((file_path, content))
            
            return files_content
            
        except Exception as e:
            raise Exception(f"å¤„ç†ç›®å½•å¤±è´¥: {str(e)}")

# ==================== æ–‡æœ¬å¤„ç†æ¨¡å— ====================

# æœ¬åœ°å…³é”®è¯æå–åŠŸèƒ½å·²ç§»é™¤ï¼Œç°åœ¨å®Œå…¨ä¾èµ–AIæ¨¡å‹è¿›è¡Œå…³é”®è¯æå–

def clean_text(text):
    """æ¸…ç†æ–‡æœ¬"""
    text = re.sub(r'\s+', ' ', text)
    text = re.sub(r'^ç¬¬[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹å\d]+ç« .*?\n', '', text, flags=re.MULTILINE)
    text = re.sub(r'^ç« èŠ‚.*?\n', '', text, flags=re.MULTILINE)
    lines = [line.strip() for line in text.split('\n') if line.strip()]
    return '\n'.join(lines)

def smart_split_paragraphs(text, min_length=100, max_length=500, use_smart=True):
    """æ™ºèƒ½åˆ†å‰²æ®µè½ - ä¼˜åŒ–ç‰ˆ"""
    if use_smart:
        return advanced_smart_split(text, min_length, max_length)
    else:
        return split_into_paragraphs(text, min_length, max_length)

def advanced_smart_split(text, min_length=100, max_length=500):
    """é«˜çº§æ™ºèƒ½åˆ†æ®µç®—æ³•"""
    paragraphs = []
    
    # 1. é¦–å…ˆæŒ‰è‡ªç„¶æ®µè½åˆ†å‰²
    natural_paragraphs = [p.strip() for p in text.split('\n\n') if p.strip()]
    
    for para in natural_paragraphs:
        if len(para) <= max_length and len(para) >= min_length:
            paragraphs.append(para)
        elif len(para) > max_length:
            # å¯¹è¿‡é•¿æ®µè½è¿›è¡Œæ™ºèƒ½åˆ†å‰²
            sub_paragraphs = intelligent_split_long_paragraph(para, min_length, max_length)
            paragraphs.extend(sub_paragraphs)
        else:
            # çŸ­æ®µè½å°è¯•åˆå¹¶
            if paragraphs and can_merge_paragraphs(paragraphs[-1], para, max_length):
                paragraphs[-1] = merge_paragraphs(paragraphs[-1], para)
            else:
                paragraphs.append(para)
    
    # 2. åå¤„ç†ï¼šå¤„ç†è¿‡çŸ­çš„æ®µè½
    paragraphs = post_process_short_paragraphs(paragraphs, min_length, max_length)
    
    return paragraphs

def intelligent_split_long_paragraph(text, min_length=100, max_length=500):
    """æ™ºèƒ½åˆ†å‰²é•¿æ®µè½"""
    # 1. ä¼˜å…ˆæŒ‰å¯¹è¯åˆ†å‰²
    if has_dialogue(text):
        return split_by_dialogue(text, min_length, max_length)
    
    # 2. æŒ‰åœºæ™¯è½¬æ¢åˆ†å‰²
    scene_splits = detect_scene_transitions(text)
    if scene_splits:
        return split_by_scenes(text, scene_splits, min_length, max_length)
    
    # 3. æŒ‰æƒ…æ„Ÿå˜åŒ–åˆ†å‰²
    emotion_splits = detect_emotion_changes(text)
    if emotion_splits:
        return split_by_emotions(text, emotion_splits, min_length, max_length)
    
    # 4. å›é€€åˆ°åŸºç¡€åˆ†å‰²
    return split_long_paragraph(text, min_length, max_length)

def has_dialogue(text):
    """æ£€æµ‹æ˜¯å¦åŒ…å«å¯¹è¯"""
    dialogue_patterns = [
        r'[""''][^""'']*[""'']',  # å¼•å·å¯¹è¯
        r'[ï¼š:]\s*[""''][^""'']*[""'']',  # å†’å·+å¼•å·
        r'è¯´é“?[ï¼š:]',  # è¯´é“/è¯´:
        r'[é—®ç­”å›]é“?[ï¼š:]',  # é—®é“/ç­”é“/å›é“
    ]
    
    for pattern in dialogue_patterns:
        if re.search(pattern, text):
            return True
    return False

def split_by_dialogue(text, min_length=100, max_length=500):
    """æŒ‰å¯¹è¯åˆ†å‰²"""
    # æŒ‰å¯¹è¯æ ‡è®°åˆ†å‰²
    dialogue_pattern = r'([""''][^""'']*[""'']|[^""'']+)'
    parts = re.findall(dialogue_pattern, text)
    
    result = []
    current = ""
    
    for part in parts:
        part = part.strip()
        if not part:
            continue
            
        if len(current + part) <= max_length:
            current += part
        else:
            if current and len(current) >= min_length:
                result.append(current)
            current = part
    
    if current and len(current) >= min_length:
        result.append(current)
    
    return result if result else [text]

def detect_scene_transitions(text):
    """æ£€æµ‹åœºæ™¯è½¬æ¢"""
    scene_markers = [
        r'(çªç„¶|å¿½ç„¶|è¿™æ—¶|æ­¤æ—¶|æ¥ç€|ç„¶å|äºæ˜¯|éšå³)',
        r'(ä¸€ä¼šå„¿|ç‰‡åˆ»|ä¸ä¹…|è¿‡äº†|åŠæ™Œ|è‰¯ä¹…)',
        r'(è½¬èº«|å›å¤´|æŠ¬å¤´|ä½å¤´|èµ·èº«|åä¸‹)',
        r'(èµ°å‘|æ¥åˆ°|åˆ°äº†|è¿›å…¥|ç¦»å¼€|è¿”å›)',
        r'(ç¬¬äºŒå¤©|æ¬¡æ—¥|ç¿Œæ—¥|é»„æ˜|å¤œæ™š|æ¸…æ™¨)',
    ]
    
    splits = []
    for pattern in scene_markers:
        for match in re.finditer(pattern, text):
            splits.append(match.start())
    
    return sorted(set(splits))

def split_by_scenes(text, splits, min_length=100, max_length=500):
    """æŒ‰åœºæ™¯åˆ†å‰²"""
    if not splits:
        return [text]
    
    result = []
    start = 0
    
    for split_pos in splits:
        if split_pos - start >= min_length:
            segment = text[start:split_pos].strip()
            if segment:
                result.append(segment)
            start = split_pos
    
    # æ·»åŠ æœ€åä¸€æ®µ
    if start < len(text):
        segment = text[start:].strip()
        if segment:
            if result and len(result[-1] + segment) <= max_length:
                result[-1] += segment
            else:
                result.append(segment)
    
    return result if result else [text]

def detect_emotion_changes(text):
    """æ£€æµ‹æƒ…æ„Ÿå˜åŒ–"""
    emotion_markers = [
        r'(æ„¤æ€’|ç”Ÿæ°”|æ¼ç«|æš´æ€’)',
        r'(é«˜å…´|å¼€å¿ƒ|å–œæ‚¦|å…´å¥‹)',
        r'(æ‚²ä¼¤|éš¾è¿‡|ä¼¤å¿ƒ|ç—›è‹¦)',
        r'(æƒŠè®¶|éœ‡æƒŠ|åƒæƒŠ|è¯§å¼‚)',
        r'(ææƒ§|å®³æ€•|æ‹…å¿ƒ|ç´§å¼ )',
        r'(å¹³é™|å†·é™|æ·¡ç„¶|å®‰è¯¦)',
    ]
    
    splits = []
    for pattern in emotion_markers:
        for match in re.finditer(pattern, text):
            splits.append(match.start())
    
    return sorted(set(splits))

def split_by_emotions(text, splits, min_length=100, max_length=500):
    """æŒ‰æƒ…æ„Ÿå˜åŒ–åˆ†å‰²"""
    return split_by_scenes(text, splits, min_length, max_length)

def can_merge_paragraphs(para1, para2, max_length):
    """åˆ¤æ–­ä¸¤ä¸ªæ®µè½æ˜¯å¦å¯ä»¥åˆå¹¶"""
    if len(para1 + '\n' + para2) > max_length:
        return False
    
    # æ£€æŸ¥è¯­ä¹‰è¿è´¯æ€§
    # å¦‚æœç¬¬ä¸€æ®µä»¥å¥å·ç»“å°¾ï¼Œç¬¬äºŒæ®µä»¥å¤§å†™å­—æ¯å¼€å¤´ï¼Œå¯èƒ½æ˜¯æ–°çš„ä¸»é¢˜
    if para1.endswith(('ã€‚', 'ï¼', 'ï¼Ÿ')) and para2 and para2[0].isupper():
        return False
    
    return True

def merge_paragraphs(para1, para2):
    """åˆå¹¶ä¸¤ä¸ªæ®µè½"""
    return para1 + '\n' + para2

def post_process_short_paragraphs(paragraphs, min_length, max_length):
    """åå¤„ç†è¿‡çŸ­çš„æ®µè½"""
    if not paragraphs:
        return paragraphs
    
    result = []
    i = 0
    
    while i < len(paragraphs):
        current = paragraphs[i]
        
        # å¦‚æœå½“å‰æ®µè½å¤ªçŸ­ï¼Œå°è¯•ä¸ä¸‹ä¸€æ®µåˆå¹¶
        if len(current) < min_length and i + 1 < len(paragraphs):
            next_para = paragraphs[i + 1]
            if len(current + '\n' + next_para) <= max_length:
                result.append(current + '\n' + next_para)
                i += 2  # è·³è¿‡ä¸‹ä¸€æ®µ
                continue
        
        result.append(current)
        i += 1
    
    return result

def split_long_paragraph(text, min_length=100, max_length=500):
    """åˆ†å‰²è¿‡é•¿æ®µè½"""
    if '"' in text or '"' in text or '"' in text:
        parts = re.split(r'(["""]+[^"""]*["""]+)', text)
        result = []
        current = ""
        
        for part in parts:
            if len(current + part) <= max_length:
                current += part
            else:
                if current and len(current) >= min_length:
                    result.append(current)
                current = part
        
        if current and len(current) >= min_length:
            result.append(current)
        
        return result if result else split_into_paragraphs(text, min_length, max_length)
    else:
        return split_into_paragraphs(text, min_length, max_length)

def split_into_paragraphs(text, min_length=100, max_length=500):
    """æŒ‰å¥å­åˆ†å‰²æ®µè½"""
    sentences = re.split(r'[ã€‚ï¼ï¼Ÿ]', text)
    paragraphs = []
    current_paragraph = ""
    
    for sentence in sentences:
        sentence = sentence.strip()
        if not sentence:
            continue
            
        if len(current_paragraph + sentence) > max_length and len(current_paragraph) >= min_length:
            if current_paragraph:
                paragraphs.append(current_paragraph)
            current_paragraph = sentence
        else:
            if current_paragraph:
                current_paragraph += sentence + "ã€‚"
            else:
                current_paragraph = sentence + "ã€‚"
    
    if current_paragraph and len(current_paragraph) >= min_length:
        paragraphs.append(current_paragraph)
    
    return paragraphs

# ==================== å…¨å±€å˜é‡ ====================

current_model_client = None
task_manager = PromptTaskManager()
current_paragraph_data = []
progress_manager = ProgressManager()

# ==================== ç•Œé¢å‡½æ•° ====================

def load_model(selected_model: str, custom_name: str, custom_base_url: str, 
               custom_api_key: str, custom_model_name: str, save_custom: bool = False) -> Tuple[str, str]:
    """åŠ è½½AIæ¨¡å‹"""
    global current_model_client
    
    try:
        if selected_model and selected_model != "æ–°å»ºæ¨¡å‹":
            # åŠ è½½å·²ä¿å­˜çš„æ¨¡å‹é…ç½®
            all_configs = AIModelClient.get_all_configs()
            if selected_model not in all_configs:
                return f"é”™è¯¯ï¼šæœªæ‰¾åˆ°æ¨¡å‹é…ç½® {selected_model}", ""
            
            config_dict = all_configs[selected_model].copy()
            config = ModelConfig(**config_dict)
        else:
            # åˆ›å»ºæ–°çš„æ¨¡å‹é…ç½®
            if not all([custom_name, custom_base_url, custom_api_key, custom_model_name]):
                return "é”™è¯¯ï¼šæ¨¡å‹é…ç½®ä¿¡æ¯ä¸å®Œæ•´", ""
            
            config = ModelConfig(
                name=custom_name,
                base_url=custom_base_url,
                api_key=custom_api_key,
                model_name=custom_model_name
            )
            
            # ä¿å­˜æ–°çš„æ¨¡å‹é…ç½®
            if save_custom and custom_name:
                config_dict = {
                    "name": custom_name,
                    "base_url": custom_base_url,
                    "api_key": custom_api_key,
                    "model_name": custom_model_name
                }
                AIModelClient.save_custom_config(custom_name, config_dict)
        
        current_model_client = AIModelClient(config)
        
        logger.info(f"æ­£åœ¨æµ‹è¯•æ¨¡å‹è¿æ¥: {config.name}")
        if current_model_client.test_connection():
            message = f"âœ… æ¨¡å‹ {config.name} åŠ è½½æˆåŠŸå¹¶è¿æ¥æ­£å¸¸"
            logger.info(message)
            updated_choices = list(AIModelClient.get_all_configs().keys()) + ["æ–°å»ºæ¨¡å‹"]
            return message, gr.Dropdown(choices=updated_choices)
        else:
            message = f"âš ï¸ æ¨¡å‹ {config.name} åŠ è½½æˆåŠŸä½†è¿æ¥æµ‹è¯•å¤±è´¥ï¼Œè¯·æ£€æŸ¥é…ç½®"
            logger.warning(message)
            updated_choices = list(AIModelClient.get_all_configs().keys()) + ["æ–°å»ºæ¨¡å‹"]
            return message, gr.Dropdown(choices=updated_choices)
            
    except Exception as e:
        error_msg = f"âŒ æ¨¡å‹åŠ è½½å¤±è´¥: {str(e)}"
        logger.error(error_msg)
        return error_msg, ""

def delete_model_config(model_name: str) -> Tuple[str, gr.Dropdown]:
    """åˆ é™¤æ¨¡å‹é…ç½®"""
    if not model_name or model_name == "æ–°å»ºæ¨¡å‹":
        return "è¯·é€‰æ‹©è¦åˆ é™¤çš„æ¨¡å‹é…ç½®", gr.Dropdown()
    
    if AIModelClient.delete_config(model_name):
        updated_choices = list(AIModelClient.get_all_configs().keys()) + ["æ–°å»ºæ¨¡å‹"]
        return f"æ¨¡å‹é…ç½® '{model_name}' å·²åˆ é™¤", gr.Dropdown(choices=updated_choices)
    else:
        return f"åˆ é™¤æ¨¡å‹é…ç½® '{model_name}' å¤±è´¥", gr.Dropdown()

def get_task_prompt(task_name: str) -> str:
    """è·å–ä»»åŠ¡æç¤ºè¯"""
    return task_manager.get_task_prompt(task_name)

def add_custom_task(task_name: str, task_prompt: str) -> Tuple[str, gr.Dropdown, gr.Dropdown]:
    """æ·»åŠ è‡ªå®šä¹‰ä»»åŠ¡"""
    if not task_name or not task_prompt:
        task_choices = task_manager.get_task_names()
        return "ä»»åŠ¡åç§°å’Œæç¤ºè¯ä¸èƒ½ä¸ºç©º", gr.Dropdown(choices=task_choices), gr.Dropdown(choices=task_choices)
    
    task_manager.add_task(task_name, task_prompt)
    updated_choices = task_manager.get_task_names()
    return f"ä»»åŠ¡ '{task_name}' å·²ä¿å­˜", gr.Dropdown(choices=updated_choices, value=task_name), gr.Dropdown(choices=updated_choices, value=task_name)

def edit_task(task_name: str, task_prompt: str) -> Tuple[str, gr.Dropdown, gr.Dropdown]:
    """ç¼–è¾‘ä»»åŠ¡"""
    if not task_name:
        task_choices = task_manager.get_task_names()
        return "è¯·é€‰æ‹©è¦ç¼–è¾‘çš„ä»»åŠ¡", gr.Dropdown(choices=task_choices), gr.Dropdown(choices=task_choices)
    
    if not task_prompt.strip():
        task_choices = task_manager.get_task_names()
        return "æç¤ºè¯ä¸èƒ½ä¸ºç©º", gr.Dropdown(choices=task_choices), gr.Dropdown(choices=task_choices)
    
    task_manager.add_task(task_name, task_prompt.strip())
    updated_choices = task_manager.get_task_names()
    return f"ä»»åŠ¡ '{task_name}' å·²æ›´æ–°", gr.Dropdown(choices=updated_choices, value=task_name), gr.Dropdown(choices=updated_choices, value=task_name)

def delete_task(task_name: str) -> Tuple[str, gr.Dropdown, gr.Dropdown]:
    """åˆ é™¤ä»»åŠ¡"""
    if not task_name:
        task_choices = task_manager.get_task_names()
        return "è¯·é€‰æ‹©è¦åˆ é™¤çš„ä»»åŠ¡", gr.Dropdown(choices=task_choices), gr.Dropdown(choices=task_choices)
    
    if task_manager.delete_task(task_name):
        updated_choices = task_manager.get_task_names()
        return f"ä»»åŠ¡ '{task_name}' å·²åˆ é™¤", gr.Dropdown(choices=updated_choices), gr.Dropdown(choices=updated_choices)
    else:
        task_choices = task_manager.get_task_names()
        return f"åˆ é™¤ä»»åŠ¡ '{task_name}' å¤±è´¥", gr.Dropdown(choices=task_choices), gr.Dropdown(choices=task_choices)

def process_files_streaming(file_input, directory_input, min_length, max_length, use_smart_split, 
                           use_ai_keywords, prompt_task, custom_instruction, output_dir) -> Generator[str, None, Tuple[str, str, List]]:
    """æµå¼å¤„ç†æ–‡ä»¶æˆ–ç›®å½• - å¤§æ•°æ®ä¼˜åŒ–ç‰ˆæœ¬ï¼Œæ”¯æŒå¢é‡ä¿å­˜"""
    global current_paragraph_data, progress_manager
    
    try:
        # åˆå§‹åŒ–
        all_paragraph_data = []
        training_data = []
        batch_size = 500  # å‡å°æ‰¹å¤„ç†å¤§å°ï¼Œæ›´é¢‘ç¹ä¿å­˜
        current_batch = []
        
        # å¢é‡ä¿å­˜é…ç½®
        incremental_save_interval = 1000  # æ¯1000æ¡æ•°æ®ä¿å­˜ä¸€æ¬¡
        temp_save_counter = 0
        temp_files = []  # ä¸´æ—¶æ–‡ä»¶åˆ—è¡¨
        
        yield "ğŸ” å¼€å§‹æ‰«ææ–‡ä»¶..."
        
        # åˆ›å»ºä¸´æ—¶ä¿å­˜ç›®å½•
        temp_dir = None
        if output_dir:
            temp_dir = os.path.join(output_dir, f"temp_{int(time.time())}")
            os.makedirs(temp_dir, exist_ok=True)
            yield f"ğŸ“ åˆ›å»ºä¸´æ—¶ä¿å­˜ç›®å½•: {temp_dir}"
        
        # è·å–æ–‡ä»¶æµ
        if file_input is not None:
            # å¤„ç†å•ä¸ªæ–‡ä»¶
            if hasattr(file_input, 'name'):
                file_path = file_input.name
            else:
                file_path = str(file_input)
            
            file_size = FileProcessor.get_file_size(file_path)
            progress_manager.start_processing(1, file_size)
            
            yield f"ğŸ“ å¤„ç†å•ä¸ªæ–‡ä»¶: {os.path.basename(file_path)} ({ProgressManager.format_size(file_size)})"
            
            content = FileProcessor.read_text_file(file_path)
            file_stream = [(file_path, content)]
            progress_manager.update_file_progress(file_path, file_size)
            
        elif directory_input:
            # å¤„ç†ç›®å½• - ä½¿ç”¨æµå¼å¤„ç†
            yield f"ğŸ“‚ æ‰«æç›®å½•: {directory_input}"
            file_stream = FileProcessor.process_directory_streaming(directory_input, progress_manager)
            
        else:
            yield "âŒ è¯·é€‰æ‹©æ–‡ä»¶æˆ–è¾“å…¥ç›®å½•è·¯å¾„"
            return "è¯·é€‰æ‹©æ–‡ä»¶æˆ–è¾“å…¥ç›®å½•è·¯å¾„", "", []
        
        yield "ğŸ“ å¼€å§‹å¤„ç†æ–‡æœ¬å†…å®¹..."
        
        # æµå¼å¤„ç†æ¯ä¸ªæ–‡ä»¶
        total_paragraphs = 0
        processed_files = 0
        
        for file_path, content in file_stream:
            try:
                processed_files += 1
                yield f"\nğŸ“„ å¤„ç†æ–‡ä»¶ [{processed_files}]: {os.path.basename(file_path)}"
                
                # å†…å­˜ä½¿ç”¨æ£€æŸ¥
                progress_manager.log_memory_usage(f"å¤„ç†æ–‡ä»¶ {os.path.basename(file_path)} å‰")
                
                # æ¸…ç†æ–‡æœ¬
                yield "  ğŸ§¹ æ¸…ç†æ–‡æœ¬..."
                cleaned_text = clean_text(content)
                
                # åˆ†å‰²æ®µè½
                yield "  âœ‚ï¸ åˆ†å‰²æ®µè½..."
                if use_smart_split:
                    paragraphs = smart_split_paragraphs(cleaned_text, min_length, max_length, True)
                else:
                    paragraphs = split_into_paragraphs(cleaned_text, min_length, max_length)
                
                if len(paragraphs) < 2:
                    logger.warning(f"æ–‡ä»¶ {file_path} æ®µè½å¤ªå°‘ï¼Œè·³è¿‡")
                    yield f"  âš ï¸ æ®µè½å¤ªå°‘ ({len(paragraphs)})ï¼Œè·³è¿‡"
                    continue
                
                yield f"  ğŸ“Š æ‰¾åˆ° {len(paragraphs)} ä¸ªæ®µè½ï¼Œå¼€å§‹ç”Ÿæˆè®­ç»ƒæ•°æ®..."
                
                # æ‰¹é‡å¤„ç†æ®µè½
                file_training_data = []
                for i in range(len(paragraphs) - 1):
                    # æ›´æ–°æ®µè½è¿›åº¦
                    if i % 50 == 0 or i == len(paragraphs) - 2:
                        progress_manager.update_paragraph_progress(i + 1, len(paragraphs) - 1, file_path)
                    
                    input_text = paragraphs[i]
                    response_text = paragraphs[i + 1]
                    
                    # æå–å…³é”®è¯ - ä»…ä½¿ç”¨AIæ¨¡å‹
                    if use_ai_keywords:
                        if not current_model_client:
                            raise Exception("å¯ç”¨AIå…³é”®è¯æå–ä½†æœªé…ç½®AIæ¨¡å‹ï¼Œè¯·å…ˆåœ¨AIè®¾ç½®ä¸­é…ç½®å¹¶åŠ è½½æ¨¡å‹")
                        if not prompt_task:
                            raise Exception("å¯ç”¨AIå…³é”®è¯æå–ä½†æœªé€‰æ‹©æç¤ºè¯ä»»åŠ¡ï¼Œè¯·é€‰æ‹©ä¸€ä¸ªæç¤ºè¯ä»»åŠ¡")
                        
                        try:
                            prompt = task_manager.get_task_prompt(prompt_task)
                            keywords = current_model_client.extract_keywords(response_text, prompt)
                        except Exception as e:
                            logger.error(f"AIå…³é”®è¯æå–å¤±è´¥: {str(e)}")
                            raise Exception(f"AIå…³é”®è¯æå–å¤±è´¥: {str(e)}")
                    else:
                        # ä¸ä½¿ç”¨AIå…³é”®è¯æå–æ—¶ï¼Œä½¿ç”¨é»˜è®¤å…³é”®è¯
                        keywords = "ç»­å†™å°è¯´"
                    
                    # ç”ŸæˆæŒ‡ä»¤
                    if custom_instruction:
                        instruction = custom_instruction.replace('{keywords}', keywords)
                    else:
                        instruction = f"è¯·æ ¹æ®å…³é”®è¯'{keywords}'ç»­å†™å°è¯´æ®µè½"
                    
                    data_item = {
                        "text": f"Instruction: {instruction}\n\nInput: {input_text}\n\nResponse: {response_text}"
                    }
                    file_training_data.append(data_item)
                    
                    # ä¿å­˜æ®µè½æ•°æ®
                    paragraph_item = {
                        "index": len(all_paragraph_data),
                        "file_path": file_path,
                        "input": input_text,
                        "response": response_text,
                        "keywords": keywords,
                        "instruction": instruction
                    }
                    all_paragraph_data.append(paragraph_item)
                    
                    # æ‰¹é‡å¤„ç†å’Œå¢é‡ä¿å­˜
                    current_batch.extend(file_training_data[-1:])
                    if len(current_batch) >= batch_size:
                        training_data.extend(current_batch)
                        current_batch = []
                        
                        # å¢é‡ä¿å­˜æ£€æŸ¥
                        if temp_dir and len(training_data) - temp_save_counter >= incremental_save_interval:
                            temp_save_counter = len(training_data)
                            temp_file = os.path.join(temp_dir, f"batch_{len(temp_files)+1:04d}.jsonl")
                            
                            # ä¿å­˜å½“å‰æ‰¹æ¬¡æ•°æ®
                            with open(temp_file, 'w', encoding='utf-8') as f:
                                start_idx = max(0, len(training_data) - incremental_save_interval)
                                batch_data = training_data[start_idx:]
                                for item in batch_data:
                                    f.write(json.dumps(item, ensure_ascii=False) + "\n")
                            
                            temp_files.append(temp_file)
                            yield f"  ğŸ’¾ å¢é‡ä¿å­˜: {len(training_data)} æ¡æ•°æ®å·²ä¿å­˜åˆ°ä¸´æ—¶æ–‡ä»¶ {os.path.basename(temp_file)}"
                        
                        gc.collect()  # é‡Šæ”¾å†…å­˜
                        yield f"  ğŸ“Š å·²å¤„ç† {len(training_data)} æ¡è®­ç»ƒæ•°æ® (å†…å­˜: {psutil.virtual_memory().percent:.1f}%)"
                
                total_paragraphs += len(paragraphs) - 1
                yield f"  âœ… å®Œæˆæ–‡ä»¶å¤„ç†ï¼Œç”Ÿæˆ {len(file_training_data)} æ¡è®­ç»ƒæ•°æ®"
                
                # é‡Šæ”¾æ–‡ä»¶å†…å®¹å†…å­˜
                del content, cleaned_text, paragraphs, file_training_data
                gc.collect()
                
            except Exception as e:
                logger.error(f"å¤„ç†æ–‡ä»¶ {file_path} å¤±è´¥: {str(e)}")
                yield f"  âŒ å¤„ç†å¤±è´¥: {str(e)}"
                continue
        
        # å¤„ç†å‰©ä½™æ‰¹æ¬¡
        if current_batch:
            training_data.extend(current_batch)
            yield f"ğŸ’¾ å¤„ç†æœ€åæ‰¹æ¬¡ï¼Œæ€»è®¡ {len(training_data)} æ¡è®­ç»ƒæ•°æ®"
        
        # æœ€åä¸€æ¬¡å¢é‡ä¿å­˜
        if temp_dir and training_data and len(training_data) > temp_save_counter:
            temp_file = os.path.join(temp_dir, f"batch_{len(temp_files)+1:04d}.jsonl")
            with open(temp_file, 'w', encoding='utf-8') as f:
                remaining_data = training_data[temp_save_counter:]
                for item in remaining_data:
                    f.write(json.dumps(item, ensure_ascii=False) + "\n")
            temp_files.append(temp_file)
            yield f"ğŸ’¾ ä¿å­˜å‰©ä½™ {len(training_data) - temp_save_counter} æ¡æ•°æ®åˆ° {os.path.basename(temp_file)}"
        
        current_paragraph_data = all_paragraph_data
        
        # å®Œæˆå¤„ç†
        progress_manager.finish_processing(total_paragraphs, len(training_data))
        
        # åˆå¹¶ä¸´æ—¶æ–‡ä»¶å¹¶ç”Ÿæˆæœ€ç»ˆJSONL
        yield "ğŸ“„ åˆå¹¶ä¸´æ—¶æ–‡ä»¶å¹¶ç”Ÿæˆæœ€ç»ˆJSONL..."
        
        if output_dir and temp_files:
            # åˆå¹¶æ‰€æœ‰ä¸´æ—¶æ–‡ä»¶
            output_file = os.path.join(output_dir, f"novel_dataset_{int(time.time())}.jsonl")
            yield f"ğŸ’¾ åˆå¹¶ {len(temp_files)} ä¸ªä¸´æ—¶æ–‡ä»¶åˆ°æœ€ç»ˆæ–‡ä»¶..."
            
            with open(output_file, 'w', encoding='utf-8') as final_file:
                total_written = 0
                for i, temp_file in enumerate(temp_files):
                    yield f"  ğŸ“„ åˆå¹¶æ–‡ä»¶ {i+1}/{len(temp_files)}: {os.path.basename(temp_file)}"
                    with open(temp_file, 'r', encoding='utf-8') as tf:
                        for line in tf:
                            final_file.write(line)
                            total_written += 1
                            if total_written % 5000 == 0:
                                yield f"    ğŸ’¾ å·²å†™å…¥ {total_written} æ¡æ•°æ®"
            
            # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
            yield "ğŸ§¹ æ¸…ç†ä¸´æ—¶æ–‡ä»¶..."
            for temp_file in temp_files:
                try:
                    os.remove(temp_file)
                except Exception as e:
                    logger.warning(f"åˆ é™¤ä¸´æ—¶æ–‡ä»¶å¤±è´¥ {temp_file}: {str(e)}")
            
            try:
                os.rmdir(temp_dir)
            except Exception as e:
                logger.warning(f"åˆ é™¤ä¸´æ—¶ç›®å½•å¤±è´¥ {temp_dir}: {str(e)}")
            
            # ç”Ÿæˆå†…å­˜ä¸­çš„JSONLç”¨äºæ˜¾ç¤ºï¼ˆé™åˆ¶å¤§å°ï¼‰
            if len(training_data) <= 1000:
                jsonl_output = "\n".join([json.dumps(item, ensure_ascii=False) for item in training_data])
            else:
                # åªæ˜¾ç¤ºå‰1000æ¡
                sample_data = training_data[:1000]
                jsonl_output = "\n".join([json.dumps(item, ensure_ascii=False) for item in sample_data])
                jsonl_output += f"\n\n# æ³¨æ„: ç”±äºæ•°æ®é‡è¿‡å¤§ï¼Œæ­¤å¤„ä»…æ˜¾ç¤ºå‰1000æ¡æ•°æ®\n# å®Œæ•´æ•°æ®å·²ä¿å­˜åˆ°æ–‡ä»¶: {output_file}"
            
            status_message = f"âœ… å¤„ç†å®Œæˆï¼\nğŸ“Š å¤„ç†äº† {processed_files} ä¸ªæ–‡ä»¶\nğŸ“ ç”Ÿæˆäº† {len(training_data)} æ¡è®­ç»ƒæ•°æ®\nğŸ’¾ å·²ä¿å­˜åˆ°: {output_file}\nğŸ”„ ä½¿ç”¨äº† {len(temp_files)} ä¸ªä¸´æ—¶æ–‡ä»¶è¿›è¡Œå¢é‡ä¿å­˜"
        else:
            # æ— è¾“å‡ºç›®å½•æ—¶ï¼Œç”Ÿæˆå†…å­˜JSONL
            if len(training_data) <= 1000:
                jsonl_output = "\n".join([json.dumps(item, ensure_ascii=False) for item in training_data])
            else:
                sample_data = training_data[:1000]
                jsonl_output = "\n".join([json.dumps(item, ensure_ascii=False) for item in sample_data])
                jsonl_output += f"\n\n# æ³¨æ„: ç”±äºæ•°æ®é‡è¿‡å¤§ï¼Œæ­¤å¤„ä»…æ˜¾ç¤ºå‰1000æ¡æ•°æ®"
            
            status_message = f"âœ… å¤„ç†å®Œæˆï¼\nğŸ“Š å¤„ç†äº† {processed_files} ä¸ªæ–‡ä»¶\nğŸ“ ç”Ÿæˆäº† {len(training_data)} æ¡è®­ç»ƒæ•°æ®\nâš ï¸ æœªæŒ‡å®šè¾“å‡ºç›®å½•ï¼Œæ•°æ®ä»…ä¿å­˜åœ¨å†…å­˜ä¸­"
        
        yield f"\n{status_message}"
        return status_message, jsonl_output, all_paragraph_data
        
    except Exception as e:
        error_msg = f"âŒ å¤„ç†å¤±è´¥: {str(e)}"
        logger.error(error_msg)
        yield error_msg
        return error_msg, "", []

def process_files_with_progress(file_input, directory_input, min_length, max_length, use_smart_split, 
                               use_ai_keywords, prompt_task, custom_instruction, output_dir):
    """å¸¦å®æ—¶è¿›åº¦æ˜¾ç¤ºçš„æ–‡ä»¶å¤„ç†å‡½æ•° - æ”¯æŒæµå¼è¾“å‡º"""
    global progress_manager
    
    try:
        # é‡ç½®è¿›åº¦ç®¡ç†å™¨
        progress_manager = ProgressManager()
        
        # å®æ—¶è¿›åº¦çŠ¶æ€
        status_messages = []
        final_result = None
        current_progress = "ç­‰å¾…å¼€å§‹å¤„ç†..."
        current_file_progress = "0/0 æ–‡ä»¶"
        current_memory = "0 MB"
        last_update_time = time.time()
        
        # æµå¼å¤„ç†ç”Ÿæˆå™¨
        stream_generator = process_files_streaming(file_input, directory_input, min_length, max_length, 
                                                 use_smart_split, use_ai_keywords, prompt_task, 
                                                 custom_instruction, output_dir)
        
        for message in stream_generator:
            if isinstance(message, tuple):
                # æœ€ç»ˆç»“æœ
                final_result = message
                break
            else:
                # è¿›åº¦æ¶ˆæ¯
                status_messages.append(message)
                print(message)  # å®æ—¶è¾“å‡ºåˆ°ç»ˆç«¯
                
                # æ›´æ–°è¿›åº¦ä¿¡æ¯
                current_time = time.time()
                if "æ‰«ææ–‡ä»¶" in message or "æ‰«æç›®å½•" in message:
                    current_progress = "ğŸ” æ‰«ææ–‡ä»¶ä¸­..."
                elif "åˆ›å»ºä¸´æ—¶ä¿å­˜ç›®å½•" in message:
                    current_progress = "ğŸ“ å‡†å¤‡å¢é‡ä¿å­˜..."
                elif "å¤„ç†æ–‡ä»¶" in message and "[" in message:
                    # æå–æ–‡ä»¶è¿›åº¦
                    if "å¤„ç†æ–‡ä»¶ [" in message:
                        try:
                            file_num = message.split("[")[1].split("]")[0]
                            current_file_progress = f"{file_num} æ–‡ä»¶å¤„ç†ä¸­"
                        except:
                            pass
                    current_progress = "ğŸ“„ å¤„ç†æ–‡ä»¶ä¸­..."
                elif "æ¸…ç†æ–‡æœ¬" in message:
                    current_progress = "ğŸ§¹ æ¸…ç†æ–‡æœ¬ä¸­..."
                elif "åˆ†å‰²æ®µè½" in message:
                    current_progress = "âœ‚ï¸ åˆ†å‰²æ®µè½ä¸­..."
                elif "ç”Ÿæˆè®­ç»ƒæ•°æ®" in message:
                    current_progress = "ğŸ“ ç”Ÿæˆè®­ç»ƒæ•°æ®ä¸­..."
                elif "å¢é‡ä¿å­˜" in message:
                    current_progress = "ğŸ’¾ å¢é‡ä¿å­˜ä¸­..."
                elif "åˆå¹¶ä¸´æ—¶æ–‡ä»¶" in message:
                    current_progress = "ğŸ“„ åˆå¹¶æ–‡ä»¶ä¸­..."
                elif "æ¸…ç†ä¸´æ—¶æ–‡ä»¶" in message:
                    current_progress = "ğŸ§¹ æ¸…ç†ä¸´æ—¶æ–‡ä»¶ä¸­..."
                elif "å¤„ç†å®Œæˆ" in message:
                    current_progress = "âœ… å¤„ç†å®Œæˆï¼"
                
                # è·å–å†…å­˜ä½¿ç”¨æƒ…å†µ (é™åˆ¶æ›´æ–°é¢‘ç‡)
                if current_time - last_update_time >= 1.0:  # æ¯ç§’æ›´æ–°ä¸€æ¬¡
                    try:
                        process = psutil.Process()
                        memory_mb = process.memory_info().rss / 1024 / 1024
                        system_memory = psutil.virtual_memory()
                        current_memory = f"è¿›ç¨‹: {memory_mb:.1f}MB | ç³»ç»Ÿ: {system_memory.percent:.1f}%"
                        last_update_time = current_time
                    except:
                        pass
                
                # å®æ—¶è¿”å›å½“å‰çŠ¶æ€ (æ¯æ¡æ¶ˆæ¯éƒ½è¿”å›)
                full_log = "\n".join(status_messages[-50:])  # åªä¿ç•™æœ€è¿‘50æ¡æ¶ˆæ¯
                yield "", [], current_progress, current_file_progress, current_memory, full_log
        
        # å¤„ç†æœ€ç»ˆç»“æœ
        full_log = "\n".join(status_messages)
        
        if final_result:
            status, jsonl, paragraph_data = final_result
            current_progress = "âœ… å¤„ç†å®Œæˆï¼"
            yield jsonl, paragraph_data, current_progress, current_file_progress, current_memory, full_log
        else:
            yield "", [], current_progress, current_file_progress, current_memory, full_log
            
    except Exception as e:
        error_msg = f"âŒ å¤„ç†å¤±è´¥: {str(e)}"
        logger.error(error_msg)
        yield "", [], "âŒ å¤„ç†å¤±è´¥", "0/0 æ–‡ä»¶", "0 MB", error_msg
def process_files(file_input, directory_input, min_length, max_length, use_smart_split, 
                 use_ai_keywords, prompt_task, custom_instruction, output_dir):
    """å¤„ç†æ–‡ä»¶æˆ–ç›®å½• - å…¼å®¹æ€§åŒ…è£…å™¨"""
    try:
        # ä½¿ç”¨æµå¼å¤„ç†
        status_messages = []
        final_result = None
        
        for message in process_files_streaming(file_input, directory_input, min_length, max_length, 
                                             use_smart_split, use_ai_keywords, prompt_task, 
                                             custom_instruction, output_dir):
            if isinstance(message, tuple):
                # æœ€ç»ˆç»“æœ
                final_result = message
                break
            else:
                # è¿›åº¦æ¶ˆæ¯
                status_messages.append(message)
                print(message)  # å®æ—¶è¾“å‡ºåˆ°ç»ˆç«¯
        
        if final_result:
            return final_result
        else:
            return "å¤„ç†å®Œæˆ", "", []
            
    except Exception as e:
        error_msg = f"âŒ å¤„ç†å¤±è´¥: {str(e)}"
        logger.error(error_msg)
        return error_msg, "", []

def save_jsonl(jsonl_content, filename, output_dir):
    """ä¿å­˜JSONLæ–‡ä»¶"""
    if not jsonl_content:
        return "æ²¡æœ‰å†…å®¹å¯ä¿å­˜"
    
    try:
        if not filename:
            filename = f"novel_dataset_{int(time.time())}.jsonl"
        
        if not filename.endswith('.jsonl'):
            filename += '.jsonl'
        
        if output_dir:
            os.makedirs(output_dir, exist_ok=True)
            filepath = os.path.join(output_dir, filename)
        else:
            filepath = filename
        
        with open(filepath, 'w', encoding='utf-8') as f:
            f.write(jsonl_content)
        
        return f"âœ… æ–‡ä»¶å·²ä¿å­˜: {filepath}"
    except Exception as e:
        return f"âŒ ä¿å­˜å¤±è´¥: {str(e)}"

def update_paragraph_keywords(paragraph_data, index, new_keywords, new_instruction):
    """æ›´æ–°æ®µè½å…³é”®è¯å’ŒæŒ‡ä»¤"""
    try:
        if 0 <= index < len(paragraph_data):
            paragraph_data[index]['keywords'] = new_keywords
            paragraph_data[index]['instruction'] = new_instruction
            return paragraph_data, f"âœ… æ®µè½ {index+1} æ›´æ–°æˆåŠŸ"
        else:
            return paragraph_data, "âŒ æ®µè½ç´¢å¼•æ— æ•ˆ"
    except Exception as e:
        return paragraph_data, f"âŒ æ›´æ–°å¤±è´¥: {str(e)}"

def regenerate_jsonl_from_paragraphs(paragraph_data):
    """ä»æ®µè½æ•°æ®é‡æ–°ç”ŸæˆJSONL"""
    try:
        training_data = []
        for item in paragraph_data:
            data_item = {
                "text": f"Instruction: {item['instruction']}\n\nInput: {item['input']}\n\nResponse: {item['response']}"
            }
            training_data.append(data_item)
        
        jsonl_output = "\n".join([json.dumps(item, ensure_ascii=False) for item in training_data])
        return jsonl_output, f"âœ… é‡æ–°ç”Ÿæˆå®Œæˆï¼Œå…± {len(training_data)} æ¡æ•°æ®"
    except Exception as e:
        return "", f"âŒ ç”Ÿæˆå¤±è´¥: {str(e)}"

def convert_jsonl_to_binidx(jsonl_file, output_prefix, tokenizer_type="RWKVTokenizer"):
    """å°†JSONLæ–‡ä»¶è½¬æ¢ä¸ºbinidxæ ¼å¼"""
    try:
        current_dir = Path(__file__).parent
        tool_dir = current_dir / "json2binidx_tool"
        if not tool_dir.exists():
            return "é”™è¯¯ï¼šæ‰¾ä¸åˆ°json2binidx_toolç›®å½•"
        
        preprocess_script = tool_dir / "tools" / "preprocess_data.py"
        if not preprocess_script.exists():
            return "é”™è¯¯ï¼šæ‰¾ä¸åˆ°preprocess_data.pyè„šæœ¬"
        
        if tokenizer_type == "RWKVTokenizer":
            vocab_file = tool_dir / "rwkv_vocab_v20230424.txt"
        else:
            vocab_file = tool_dir / "20B_tokenizer.json"
        
        if not vocab_file.exists():
            return f"é”™è¯¯ï¼šæ‰¾ä¸åˆ°tokenizeræ–‡ä»¶: {vocab_file}"
        
        cmd = [
            sys.executable,
            str(preprocess_script),
            "--input", jsonl_file,
            "--output-prefix", output_prefix,
            "--vocab", str(vocab_file),
            "--dataset-impl", "mmap",
            "--tokenizer-type", tokenizer_type,
            "--append-eod"
        ]
        
        result = subprocess.run(cmd, capture_output=True, text=True, cwd=str(tool_dir))
        
        if result.returncode == 0:
            return f"âœ… è½¬æ¢æˆåŠŸï¼ç”Ÿæˆæ–‡ä»¶: {output_prefix}.bin å’Œ {output_prefix}.idx"
        else:
            return f"âŒ è½¬æ¢å¤±è´¥: {result.stderr}"
            
    except Exception as e:
        return f"âŒ è½¬æ¢è¿‡ç¨‹ä¸­å‡ºé”™: {str(e)}"

# ==================== ç•Œé¢åˆ›å»º ====================

def create_interface():
    """åˆ›å»ºGradioç•Œé¢"""
    
    with gr.Blocks(title="å°è¯´è®­ç»ƒæ•°æ®ç”Ÿæˆå™¨", theme=gr.themes.Ocean()) as demo:
        gr.Markdown("# ğŸ“š å°è¯´è®­ç»ƒæ•°æ®ç”Ÿæˆå™¨")
        gr.Markdown("æ™ºèƒ½å¤„ç†å°è¯´æ–‡æœ¬ï¼Œç”Ÿæˆé«˜è´¨é‡çš„è®­ç»ƒæ•°æ®é›†")
        
        # çŠ¶æ€å˜é‡
        paragraph_data_state = gr.State([])
        
        with gr.Tabs():
            # AIè®¾ç½®æ ‡ç­¾é¡µ
            with gr.TabItem("ğŸ¤– AIè®¾ç½®"):
                with gr.Row():
                    with gr.Column(scale=1):
                        gr.Markdown("### æ¨¡å‹é…ç½®")
                        
                        model_preset = gr.Dropdown(
                            choices=list(AIModelClient.get_all_configs().keys()) + ["æ–°å»ºæ¨¡å‹"],
                            value="æ–°å»ºæ¨¡å‹" if not AIModelClient.get_all_configs() else list(AIModelClient.get_all_configs().keys())[0],
                            label="é€‰æ‹©æ¨¡å‹"
                        )
                        
                        with gr.Group(visible=True) as custom_model_group:
                            custom_model_name = gr.Textbox(label="æ¨¡å‹åç§°")
                            custom_base_url = gr.Textbox(label="APIåœ°å€")
                            custom_api_key = gr.Textbox(label="APIå¯†é’¥", type="password")
                            custom_model_id = gr.Textbox(label="æ¨¡å‹ID")
                            save_custom_config = gr.Checkbox(label="ä¿å­˜è‡ªå®šä¹‰é…ç½®")
                        
                        with gr.Row():
                            load_model_btn = gr.Button("ğŸ”„ åŠ è½½æ¨¡å‹", variant="primary")
                            delete_model_btn = gr.Button("ğŸ—‘ï¸ åˆ é™¤æ¨¡å‹", variant="stop")
                        model_status = gr.Textbox(label="æ¨¡å‹çŠ¶æ€", interactive=False)
                        
                        def toggle_custom_model(preset):
                            return gr.Group(visible=(preset == "æ–°å»ºæ¨¡å‹"))
                        
                        model_preset.change(
                            fn=toggle_custom_model,
                            inputs=[model_preset],
                            outputs=[custom_model_group]
                        )
                    
                    with gr.Column(scale=1):
                        gr.Markdown("### æç¤ºè¯ç®¡ç†")
                        
                        task_dropdown = gr.Dropdown(
                            choices=task_manager.get_task_names(),
                            value="å°è¯´å…³é”®è¯æå–",
                            label="é€‰æ‹©æç¤ºè¯ä»»åŠ¡"
                        )
                        
                        current_prompt = gr.Textbox(
                            label="å½“å‰æç¤ºè¯",
                            value=task_manager.get_task_prompt("å°è¯´å…³é”®è¯æå–"),
                            lines=3,
                            interactive=True
                        )
                        
                        with gr.Row():
                            new_task_name = gr.Textbox(label="æ–°ä»»åŠ¡åç§°", scale=2)
                            add_task_btn = gr.Button("â• æ·»åŠ ", scale=1)
                        
                        new_task_prompt = gr.Textbox(label="æ–°ä»»åŠ¡æç¤ºè¯", lines=3)
                        
                        with gr.Row():
                            edit_task_btn = gr.Button("âœï¸ ç¼–è¾‘å½“å‰ä»»åŠ¡")
                            delete_task_btn = gr.Button("ğŸ—‘ï¸ åˆ é™¤ä»»åŠ¡")
                        
                        task_status = gr.Textbox(label="æ“ä½œçŠ¶æ€", interactive=False)
            
            # æ–‡ä»¶å¤„ç†æ ‡ç­¾é¡µ
            with gr.TabItem("ğŸ“ æ–‡ä»¶å¤„ç†"):
                with gr.Row():
                    with gr.Column(scale=1):
                        gr.Markdown("### è¾“å…¥è®¾ç½®")
                        
                        file_input = gr.File(label="ä¸Šä¼ å•ä¸ªæ–‡ä»¶", file_types=[".txt", ".md"])
                        directory_input = gr.Textbox(label="æˆ–è¾“å…¥ç›®å½•è·¯å¾„")
                        
                        gr.Markdown("### å¤„ç†å‚æ•°")
                        
                        with gr.Row():
                            min_length = gr.Slider(50, 300, value=100, label="æœ€å°æ®µè½é•¿åº¦")
                            max_length = gr.Slider(300, 1000, value=500, label="æœ€å¤§æ®µè½é•¿åº¦")
                        
                        use_smart_split = gr.Checkbox(label="å¯ç”¨æ™ºèƒ½åˆ†æ®µ", value=True)
                        use_ai_keywords = gr.Checkbox(label="ä½¿ç”¨AIæå–å…³é”®è¯", value=True)
                        
                        prompt_task_select = gr.Dropdown(
                            choices=task_manager.get_task_names(),
                            value="å°è¯´å…³é”®è¯æå–",
                            label="å…³é”®è¯æå–ä»»åŠ¡"
                        )
                        
                        custom_instruction = gr.Textbox(
                            label="è‡ªå®šä¹‰æŒ‡ä»¤æ¨¡æ¿ï¼ˆä½¿ç”¨{keywords}ä½œä¸ºå ä½ç¬¦ï¼‰",
                            placeholder="è¯·æ ¹æ®å…³é”®è¯'{keywords}'ç»­å†™å°è¯´æ®µè½"
                        )
                    
                    with gr.Column(scale=1):
                        gr.Markdown("### è¾“å‡ºè®¾ç½®")
                        
                        output_dir = gr.Textbox(
                            label="ä¿å­˜ç›®å½•",
                            value="./output",
                            placeholder="è¾“å…¥ä¿å­˜ç›®å½•è·¯å¾„"
                        )
                        
                        process_btn = gr.Button("ğŸš€ å¼€å§‹å¤„ç†", variant="primary", size="lg")
                        
                        # è¿›åº¦æ˜¾ç¤ºåŒºåŸŸ
                        gr.Markdown("### å¤„ç†è¿›åº¦")
                        
                        with gr.Group():
                            progress_info = gr.Textbox(
                                label="å½“å‰çŠ¶æ€",
                                value="ç­‰å¾…å¼€å§‹å¤„ç†...",
                                interactive=False
                            )
                            
                            file_progress = gr.Textbox(
                                label="æ–‡ä»¶è¿›åº¦",
                                value="0/0 æ–‡ä»¶",
                                interactive=False
                            )
                            
                            memory_info = gr.Textbox(
                                label="å†…å­˜ä½¿ç”¨",
                                value="0 MB",
                                interactive=False
                            )
                        
                        processing_status = gr.Textbox(
                            label="è¯¦ç»†æ—¥å¿—",
                            lines=8,
                            interactive=False,
                            max_lines=15
                        )
                        
                        jsonl_output = gr.Textbox(
                            label="ç”Ÿæˆçš„JSONLæ•°æ®",
                            lines=6,
                            max_lines=15
                        )
            
            # æ®µè½ç¼–è¾‘æ ‡ç­¾é¡µ
            with gr.TabItem("âœï¸ æ®µè½ç¼–è¾‘"):
                with gr.Row():
                    with gr.Column(scale=1):
                        gr.Markdown("### æ®µè½åˆ—è¡¨")
                        
                        paragraph_count = gr.Textbox(label="æ®µè½æ€»æ•°", interactive=False)
                        
                        paragraph_selector = gr.Dropdown(
                            label="é€‰æ‹©æ®µè½",
                            choices=[],
                            interactive=True
                        )
                        
                        with gr.Row():
                            prev_btn = gr.Button("â¬…ï¸ ä¸Šä¸€ä¸ª")
                            next_btn = gr.Button("â¡ï¸ ä¸‹ä¸€ä¸ª")
                        
                        file_source = gr.Textbox(label="æ–‡ä»¶æ¥æº", interactive=False)
                    
                    with gr.Column(scale=2):
                        gr.Markdown("### æ®µè½å†…å®¹")
                        
                        input_preview = gr.Textbox(
                            label="è¾“å…¥æ®µè½",
                            lines=5,
                            interactive=False
                        )
                        
                        response_preview = gr.Textbox(
                            label="å“åº”æ®µè½",
                            lines=5,
                            interactive=False
                        )
                        
                        edit_keywords = gr.Textbox(label="å…³é”®è¯")
                        edit_instruction = gr.Textbox(label="æŒ‡ä»¤")
                        
                        with gr.Row():
                            update_btn = gr.Button("ğŸ’¾ æ›´æ–°æ®µè½", variant="primary")
                            regenerate_btn = gr.Button("ğŸ”„ é‡æ–°ç”ŸæˆJSONL")
                        
                        edit_status = gr.Textbox(label="ç¼–è¾‘çŠ¶æ€", interactive=False)
            
            # æ•°æ®å¯¼å‡ºæ ‡ç­¾é¡µ
            with gr.TabItem("ğŸ’¾ æ•°æ®å¯¼å‡º"):
                with gr.Row():
                    with gr.Column():
                        gr.Markdown("### JSONLä¿å­˜")
                        
                        save_filename = gr.Textbox(
                            label="æ–‡ä»¶å",
                            placeholder="ç•™ç©ºè‡ªåŠ¨ç”Ÿæˆ"
                        )
                        
                        save_output_dir = gr.Textbox(
                            label="ä¿å­˜ç›®å½•",
                            value="./output"
                        )
                        
                        save_jsonl_btn = gr.Button("ğŸ’¾ ä¿å­˜JSONL", variant="primary")
                        save_status = gr.Textbox(label="ä¿å­˜çŠ¶æ€", interactive=False)
                    
                    with gr.Column():
                        gr.Markdown("### BinIdxè½¬æ¢")
                        
                        convert_jsonl_file = gr.File(label="é€‰æ‹©JSONLæ–‡ä»¶", file_types=[".jsonl"])
                        
                        convert_output_prefix = gr.Textbox(
                            label="è¾“å‡ºå‰ç¼€",
                            value="novel_dataset"
                        )
                        
                        tokenizer_type = gr.Dropdown(
                            choices=["RWKVTokenizer", "GPTNeoXTokenizer"],
                            value="RWKVTokenizer",
                            label="Tokenizerç±»å‹"
                        )
                        
                        convert_btn = gr.Button("ğŸ”„ è½¬æ¢ä¸ºBinIdx")
                        convert_status = gr.Textbox(label="è½¬æ¢çŠ¶æ€", interactive=False)
        
        # äº‹ä»¶ç»‘å®š
        
        # AIè®¾ç½®äº‹ä»¶
        load_model_btn.click(
            fn=load_model,
            inputs=[model_preset, custom_model_name, custom_base_url, custom_api_key, custom_model_id, save_custom_config],
            outputs=[model_status, model_preset]
        )
        
        task_dropdown.change(
            fn=get_task_prompt,
            inputs=[task_dropdown],
            outputs=[current_prompt]
        )
        
        add_task_btn.click(
            fn=add_custom_task,
            inputs=[new_task_name, new_task_prompt],
            outputs=[task_status, task_dropdown, prompt_task_select]
        )
        
        edit_task_btn.click(
            fn=edit_task,
            inputs=[task_dropdown, current_prompt],
            outputs=[task_status, task_dropdown, prompt_task_select]
        )
        
        delete_task_btn.click(
            fn=delete_task,
            inputs=[task_dropdown],
            outputs=[task_status, task_dropdown, prompt_task_select]
        )
        
        delete_model_btn.click(
            fn=delete_model_config,
            inputs=[model_preset],
            outputs=[model_status, model_preset]
        )
        
        # æ–‡ä»¶å¤„ç†äº‹ä»¶ - æ”¯æŒæµå¼è¾“å‡º
        process_btn.click(
            fn=process_files_with_progress,
            inputs=[file_input, directory_input, min_length, max_length, use_smart_split, 
                   use_ai_keywords, prompt_task_select, custom_instruction, output_dir],
            outputs=[jsonl_output, paragraph_data_state, 
                    progress_info, file_progress, memory_info, processing_status],
            show_progress=True,
            queue=True
        )
        
        # æ®µè½ç¼–è¾‘äº‹ä»¶
        def update_paragraph_selector(paragraph_data):
            if paragraph_data:
                choices = [f"æ®µè½ {i+1}: {item['file_path']}" for i, item in enumerate(paragraph_data)]
                return gr.Dropdown(choices=choices), f"å…± {len(paragraph_data)} ä¸ªæ®µè½"
            return gr.Dropdown(choices=[]), "0 ä¸ªæ®µè½"
        
        def load_paragraph_content(paragraph_data, selected):
            if not paragraph_data or not selected:
                return "", "", "", "", ""
            
            try:
                index = int(selected.split(":")[0].replace("æ®µè½ ", "")) - 1
                if 0 <= index < len(paragraph_data):
                    item = paragraph_data[index]
                    return (
                        item['input'],
                        item['response'],
                        item['keywords'],
                        item['instruction'],
                        item['file_path']
                    )
            except:
                pass
            
            return "", "", "", "", ""
        
        paragraph_data_state.change(
            fn=update_paragraph_selector,
            inputs=[paragraph_data_state],
            outputs=[paragraph_selector, paragraph_count]
        )
        
        paragraph_selector.change(
            fn=load_paragraph_content,
            inputs=[paragraph_data_state, paragraph_selector],
            outputs=[input_preview, response_preview, edit_keywords, edit_instruction, file_source]
        )
        
        def update_paragraph(paragraph_data, selected, keywords, instruction):
            if not paragraph_data or not selected:
                return paragraph_data, "è¯·é€‰æ‹©æ®µè½"
            
            try:
                index = int(selected.split(":")[0].replace("æ®µè½ ", "")) - 1
                return update_paragraph_keywords(paragraph_data, index, keywords, instruction)
            except:
                return paragraph_data, "æ›´æ–°å¤±è´¥"
        
        update_btn.click(
            fn=update_paragraph,
            inputs=[paragraph_data_state, paragraph_selector, edit_keywords, edit_instruction],
            outputs=[paragraph_data_state, edit_status]
        )
        
        regenerate_btn.click(
            fn=regenerate_jsonl_from_paragraphs,
            inputs=[paragraph_data_state],
            outputs=[jsonl_output, edit_status]
        )
        
        # æ•°æ®å¯¼å‡ºäº‹ä»¶
        save_jsonl_btn.click(
            fn=save_jsonl,
            inputs=[jsonl_output, save_filename, save_output_dir],
            outputs=[save_status]
        )
        
        def convert_to_binidx_wrapper(jsonl_file, prefix, tokenizer):
            if jsonl_file is None:
                return "è¯·é€‰æ‹©JSONLæ–‡ä»¶"
            return convert_jsonl_to_binidx(jsonl_file.name, prefix, tokenizer)
        
        convert_btn.click(
            fn=convert_to_binidx_wrapper,
            inputs=[convert_jsonl_file, convert_output_prefix, tokenizer_type],
            outputs=[convert_status]
        )
        
        # æ·»åŠ ä½¿ç”¨è¯´æ˜
        with gr.Accordion("ğŸ“– ä½¿ç”¨è¯´æ˜", open=False):
            gr.Markdown("""
            ## ğŸš€ åŠŸèƒ½ç‰¹ç‚¹
            
            ### ğŸ¤– AIè®¾ç½®
            1. **å¤šæ¨¡å‹æ”¯æŒ**: æ”¯æŒDeepSeekã€OpenAIã€Claudeç­‰å¤šç§AIæ¨¡å‹
            2. **æç¤ºè¯ç®¡ç†**: å†…ç½®å¤šç§å°è¯´ç±»å‹çš„å…³é”®è¯æå–æ¨¡æ¿
            3. **è‡ªå®šä¹‰é…ç½®**: å¯ä¿å­˜å’Œç®¡ç†è‡ªå®šä¹‰æ¨¡å‹é…ç½®
            
            ### ğŸ“ æ–‡ä»¶å¤„ç†
            1. **æ‰¹é‡å¤„ç†**: æ”¯æŒå•æ–‡ä»¶æˆ–æ•´ä¸ªç›®å½•çš„æ‰¹é‡å¤„ç†
            2. **æ™ºèƒ½åˆ†æ®µ**: è€ƒè™‘å¯¹è¯ã€åœºæ™¯è½¬æ¢ç­‰è¿›è¡Œæ™ºèƒ½æ®µè½åˆ†å‰²
            3. **AIå…³é”®è¯**: ä½¿ç”¨AIæ¨¡å‹æå–æ›´å‡†ç¡®çš„å…³é”®è¯
            4. **è‡ªå®šä¹‰æŒ‡ä»¤**: æ”¯æŒè‡ªå®šä¹‰æŒ‡ä»¤æ¨¡æ¿
            
            ### âœï¸ æ®µè½ç¼–è¾‘
            1. **å¯è§†åŒ–ç¼–è¾‘**: æŸ¥çœ‹å’Œç¼–è¾‘æ¯ä¸ªæ®µè½çš„å…³é”®è¯å’ŒæŒ‡ä»¤
            2. **å®æ—¶é¢„è§ˆ**: å®æ—¶æŸ¥çœ‹æ®µè½å†…å®¹å’Œå½“å‰æŒ‡ä»¤
            3. **æ‰¹é‡æ›´æ–°**: æ”¯æŒé‡æ–°ç”Ÿæˆæ•´ä¸ªJSONLæ–‡ä»¶
            
            ### ğŸ’¾ æ•°æ®å¯¼å‡º
            1. **JSONLä¿å­˜**: ä¿å­˜ä¸ºæ ‡å‡†JSONLæ ¼å¼
            2. **BinIdxè½¬æ¢**: è½¬æ¢ä¸ºRWKVè®­ç»ƒæ ¼å¼
            3. **å¤šç§Tokenizer**: æ”¯æŒä¸åŒçš„åˆ†è¯å™¨
            
            ## ğŸ“ æ•°æ®æ ¼å¼
            
            ç”Ÿæˆçš„è®­ç»ƒæ•°æ®æ ¼å¼ï¼š
            ```
            {
              "text": "Instruction: è¯·æ ¹æ®å…³é”®è¯'æ­¦åŠŸã€å†…åŠ›ã€ä¿®ç‚¼'ç»­å†™å°è¯´æ®µè½\n\nInput: å‰ä¸€æ®µå°è¯´å†…å®¹...\n\nResponse: åä¸€æ®µå°è¯´å†…å®¹..."
            }
            ```
            
            ## ğŸ”§ ä½¿ç”¨æ­¥éª¤
            
            1. **AIè®¾ç½®**: é…ç½®AIæ¨¡å‹å’Œæç¤ºè¯æ¨¡æ¿
            2. **æ–‡ä»¶å¤„ç†**: ä¸Šä¼ æ–‡ä»¶æˆ–æŒ‡å®šç›®å½•ï¼Œè®¾ç½®å¤„ç†å‚æ•°
            3. **æ®µè½ç¼–è¾‘**: åœ¨ç¼–è¾‘é¡µé¢ä¼˜åŒ–å…³é”®è¯å’ŒæŒ‡ä»¤
            4. **æ•°æ®å¯¼å‡º**: ä¿å­˜JSONLæ–‡ä»¶æˆ–è½¬æ¢ä¸ºè®­ç»ƒæ ¼å¼
            """)
    
    return demo

if __name__ == "__main__":
    try:
        import jieba
    except ImportError:
        print("è¯·å…ˆå®‰è£…jieba: pip install jieba")
        exit(1)
    
    demo = create_interface()
    demo.launch(share=False, server_name="127.0.0.1", server_port=None, show_error=True)
