import gradio as gr
import json
import re
import jieba
from collections import Counter
import os
import requests
import subprocess
import sys
import tempfile
import shutil
from pathlib import Path
import argparse
import multiprocessing

# æ·»åŠ json2binidx_toolè·¯å¾„åˆ°sys.path
sys.path.append(os.path.join(os.path.dirname(__file__), 'json2binidx_tool', 'tools'))

# AI APIé…ç½®
DEFAULT_API_KEY = ""  # ç”¨æˆ·éœ€è¦å¡«å…¥è‡ªå·±çš„APIå¯†é’¥
DEFAULT_API_URL = "https://api.deepseek.com/v1/chat/completions"
DEFAULT_MODEL = "deepseek-chat"

class DataProcessor:
    def __init__(self):
        self.vocab_file = os.path.join(os.path.dirname(__file__), 'json2binidx_tool', 'rwkv_vocab_v20230424.txt')
        
    def process_single_qa(self, text_content):
        """å¤„ç†å•è½®é—®ç­”æ ¼å¼"""
        lines = text_content.strip().split('\n')
        jsonl_data = []
        
        for line in lines:
            line = line.strip()
            if not line:
                continue
                
            if '\t' in line:
                parts = line.split('\t', 1)
                if len(parts) == 2:
                    question, answer = parts
                    jsonl_data.append({
                        "text": f"User: {question.strip()}\n\nAssistant: {answer.strip()}"
                    })
            elif '|' in line:
                parts = line.split('|', 1)
                if len(parts) == 2:
                    question, answer = parts
                    jsonl_data.append({
                        "text": f"User: {question.strip()}\n\nAssistant: {answer.strip()}"
                    })
        
        return jsonl_data
    
    def process_novel_continuation(self, text_content):
        """å¤„ç†å°è¯´æ®µè½ç»­å†™æ•°æ®æ ¼å¼"""
        jsonl_data = []
        lines = text_content.strip().split('\n')
        
        current_prompt = ""
        current_continuation = ""
        in_continuation = False
        
        for line in lines:
            line = line.strip()
            if not line:
                continue
            
            if line.startswith("æ®µè½å¼€å¤´:") or line.startswith("å¼€å¤´:"):
                # ä¿å­˜ä¹‹å‰çš„æ•°æ®
                if current_prompt and current_continuation:
                    jsonl_data.append({
                        "text": f"User: {current_prompt}\n\nAssistant: {current_continuation}"
                    })
                
                current_prompt = line.replace("æ®µè½å¼€å¤´:", "").replace("å¼€å¤´:", "").strip()
                current_continuation = ""
                in_continuation = False
            elif line.startswith("ç»­å†™:") or line.startswith("åç»­:") or line.startswith("ç»§ç»­:"):
                current_continuation = line.replace("ç»­å†™:", "").replace("åç»­:", "").replace("ç»§ç»­:", "").strip()
                in_continuation = True
            elif in_continuation:
                current_continuation += "\n" + line
            else:
                if current_prompt:
                    current_prompt += "\n" + line
                else:
                    current_prompt = line
        
        # å¤„ç†æœ€åä¸€ç»„æ•°æ®
        if current_prompt and current_continuation:
            jsonl_data.append({
                "text": f"User: {current_prompt}\n\nAssistant: {current_continuation}"
            })
        
        return jsonl_data
    
    def process_chapter_expansion(self, text_content):
        """å¤„ç†ç« èŠ‚å¤§çº²æ‰©å†™æ•°æ®æ ¼å¼"""
        jsonl_data = []
        lines = text_content.strip().split('\n')
        
        current_outline = ""
        current_content = ""
        in_content = False
        
        for line in lines:
            line = line.strip()
            if not line:
                continue
            
            if line.startswith("å¤§çº²:") or line.startswith("ç« èŠ‚å¤§çº²:") or line.startswith("outline:"):
                # ä¿å­˜ä¹‹å‰çš„æ•°æ®
                if current_outline and current_content:
                    jsonl_data.append({
                        "text": f"User: {current_outline}\n\nAssistant: {current_content}"
                    })
                
                current_outline = line.replace("å¤§çº²:", "").replace("ç« èŠ‚å¤§çº²:", "").replace("outline:", "").strip()
                current_content = ""
                in_content = False
            elif line.startswith("å†…å®¹:") or line.startswith("ç« èŠ‚å†…å®¹:") or line.startswith("å®Œæ•´å†…å®¹:") or line.startswith("content:"):
                current_content = line.replace("å†…å®¹:", "").replace("ç« èŠ‚å†…å®¹:", "").replace("å®Œæ•´å†…å®¹:", "").replace("content:", "").strip()
                in_content = True
            elif in_content:
                current_content += "\n" + line
            else:
                if current_outline:
                    current_outline += "\n" + line
                else:
                    current_outline = line
        
        # å¤„ç†æœ€åä¸€ç»„æ•°æ®
        if current_outline and current_content:
            jsonl_data.append({
                "text": f"User: {current_outline}\n\nAssistant: {current_content}"
            })
        
        return jsonl_data
    
    def process_multi_turn(self, text_content):
        """å¤„ç†å¤šè½®å¯¹è¯æ ¼å¼"""
        conversations = text_content.strip().split('\n\n')
        jsonl_data = []
        
        for conv in conversations:
            if not conv.strip():
                continue
                
            lines = conv.strip().split('\n')
            dialogue_parts = []
            
            for i, line in enumerate(lines):
                line = line.strip()
                if not line:
                    continue
                    
                if i % 2 == 0:  # ç”¨æˆ·è¾“å…¥
                    dialogue_parts.append(f"User: {line}")
                else:  # åŠ©æ‰‹å›å¤
                    dialogue_parts.append(f"Assistant: {line}")
            
            if dialogue_parts:
                jsonl_data.append({
                    "text": "\n\n".join(dialogue_parts)
                })
        
        return jsonl_data
    
    def process_instruction(self, text_content):
        """å¤„ç†æŒ‡ä»¤é—®ç­”æ ¼å¼"""
        blocks = text_content.strip().split('\n\n\n')
        jsonl_data = []
        
        for block in blocks:
            if not block.strip():
                continue
                
            lines = block.strip().split('\n\n')
            if len(lines) >= 3:
                instruction = lines[0].strip()
                input_text = lines[1].strip()
                response = lines[2].strip()
                
                jsonl_data.append({
                    "text": f"Instruction: {instruction}\n\nInput: {input_text}\n\nResponse: {response}"
                })
            elif len(lines) == 2:
                instruction = lines[0].strip()
                response = lines[1].strip()
                
                jsonl_data.append({
                    "text": f"Instruction: {instruction}\n\nResponse: {response}"
                })
        
        return jsonl_data
    
    def process_long_text(self, text_content):
        """å¤„ç†é•¿æ–‡æœ¬æ ¼å¼"""
        # æŒ‰æ®µè½åˆ†å‰²
        paragraphs = text_content.strip().split('\n\n')
        jsonl_data = []
        
        for para in paragraphs:
            para = para.strip()
            if para and len(para) > 50:  # åªä¿ç•™è¾ƒé•¿çš„æ®µè½
                jsonl_data.append({
                    "text": para
                })
        
        return jsonl_data
    
    def process_article_with_title(self, text_content):
        """å¤„ç†å¸¦æ ‡é¢˜çš„æ–‡ç« æ ¼å¼"""
        lines = text_content.strip().split('\n')
        jsonl_data = []
        
        current_title = ""
        current_content = []
        
        for line in lines:
            line = line.strip()
            if not line:
                continue
                
            # æ£€æµ‹æ ‡é¢˜ï¼ˆå‡è®¾æ ‡é¢˜ç”¨ç‰¹æ®Šæ ‡è®°æˆ–æ ¼å¼ï¼‰
            if line.startswith('#') or line.startswith('ã€Š') and line.endswith('ã€‹'):
                if current_title and current_content:
                    content = '\n'.join(current_content)
                    jsonl_data.append({
                        "text": f"{current_title}\n{content}"
                    })
                
                current_title = line.strip('#').strip()
                if current_title.startswith('ã€Š') and current_title.endswith('ã€‹'):
                    current_title = current_title
                else:
                    current_title = f"ã€Š{current_title}ã€‹"
                current_content = []
            else:
                current_content.append(line)
        
        # å¤„ç†æœ€åä¸€ç¯‡æ–‡ç« 
        if current_title and current_content:
            content = '\n'.join(current_content)
            jsonl_data.append({
                "text": f"{current_title}\n{content}"
            })
        
        return jsonl_data
    
    def save_jsonl(self, data, output_path):
        """ä¿å­˜JSONLæ–‡ä»¶"""
        with open(output_path, 'w', encoding='utf-8') as f:
            for item in data:
                f.write(json.dumps(item, ensure_ascii=False) + '\n')
    
    def process_directory(self, directory_path, data_format):
        """å¤„ç†ç›®å½•ä¸‹çš„æ‰€æœ‰æ–‡æœ¬æ–‡ä»¶"""
        supported_extensions = ['.txt', '.json', '.jsonl']
        all_jsonl_data = []
        processed_files = []
        
        try:
            # éå†ç›®å½•ä¸‹çš„æ‰€æœ‰æ–‡ä»¶
            for root, dirs, files in os.walk(directory_path):
                for file in files:
                    file_path = os.path.join(root, file)
                    file_ext = os.path.splitext(file)[1].lower()
                    
                    if file_ext in supported_extensions:
                        try:
                            print(f"æ­£åœ¨å¤„ç†æ–‡ä»¶: {file_path}")
                            with open(file_path, 'r', encoding='utf-8') as f:
                                content = f.read()
                            
                            # æ ¹æ®æ ¼å¼å¤„ç†æ•°æ®
                            if data_format == "å•è½®é—®ç­”":
                                file_data = self.process_single_qa(content)
                            elif data_format == "å¤šè½®å¯¹è¯":
                                file_data = self.process_multi_turn(content)
                            elif data_format == "æŒ‡ä»¤é—®ç­”":
                                file_data = self.process_instruction(content)
                            elif data_format == "é•¿æ–‡æœ¬":
                                file_data = self.process_long_text(content)
                            elif data_format == "å¸¦æ ‡é¢˜æ–‡ç« ":
                                file_data = self.process_article_with_title(content)
                            elif data_format == "å°è¯´æ®µè½ç»­å†™":
                                file_data = self.process_novel_continuation(content)
                            elif data_format == "ç« èŠ‚å¤§çº²æ‰©å†™":
                                file_data = self.process_chapter_expansion(content)
                            else:
                                continue
                            
                            # ä¸ºæ¯æ¡æ•°æ®æ·»åŠ æ–‡ä»¶æ¥æºä¿¡æ¯
                            for item in file_data:
                                item['source_file'] = os.path.relpath(file_path, directory_path)
                            
                            all_jsonl_data.extend(file_data)
                            processed_files.append(file_path)
                            print(f"æ–‡ä»¶ {file} å¤„ç†å®Œæˆï¼Œç”Ÿæˆ {len(file_data)} æ¡æ•°æ®")
                            
                        except Exception as e:
                            print(f"å¤„ç†æ–‡ä»¶ {file_path} æ—¶å‡ºé”™: {str(e)}")
                            continue
            
            return all_jsonl_data, processed_files
            
        except Exception as e:
            print(f"å¤„ç†ç›®å½•æ—¶å‡ºé”™: {str(e)}")
            return [], []
    
    def convert_to_binidx(self, jsonl_path, output_prefix):
        """è½¬æ¢JSONLåˆ°binidxæ ¼å¼"""
        try:
            import subprocess
            
            # æ„å»ºpreprocess_data.pyçš„è·¯å¾„
            preprocess_script = os.path.join(os.path.dirname(__file__), 'json2binidx_tool', 'tools', 'preprocess_data.py')
            
            # æ„å»ºå‘½ä»¤è¡Œå‚æ•°
            cmd = [
                sys.executable,  # ä½¿ç”¨å½“å‰Pythonè§£é‡Šå™¨
                preprocess_script,
                '--input', jsonl_path,
                '--output-prefix', output_prefix,
                '--vocab', self.vocab_file,
                '--dataset-impl', 'mmap',
                '--tokenizer-type', 'RWKVTokenizer',
                '--append-eod'
            ]
            
            # æ‰§è¡Œè½¬æ¢å‘½ä»¤
            result = subprocess.run(cmd, capture_output=True, text=True, encoding='utf-8')
            
            if result.returncode == 0:
                return True, "è½¬æ¢æˆåŠŸå®Œæˆï¼"
            else:
                error_msg = result.stderr if result.stderr else result.stdout
                return False, f"è½¬æ¢å¤±è´¥: {error_msg}"
            
        except Exception as e:
            return False, f"è½¬æ¢å¤±è´¥: {str(e)}"

# å°è¯´å¤„ç†ç›¸å…³å‡½æ•°
def extract_keywords_with_ai(text, api_key, api_url=None, model=None):
    """ä½¿ç”¨AI APIæå–å…³é”®è¯"""
    if not api_key:
        return extract_keywords(text)  # å›é€€åˆ°æœ¬åœ°æ–¹æ³•
    
    # ä½¿ç”¨é»˜è®¤å€¼æˆ–ç”¨æˆ·æä¾›çš„å€¼
    api_url = api_url or DEFAULT_API_URL
    model = model or DEFAULT_MODEL
    
    try:
        headers = {
            "Authorization": f"Bearer {api_key}",
            "Content-Type": "application/json"
        }
        
        prompt = f"è¯·ä»ä»¥ä¸‹æ–‡æœ¬ä¸­æå–3-5ä¸ªæœ€é‡è¦çš„å…³é”®è¯ï¼Œç”¨é¡¿å·åˆ†éš”ï¼Œåªè¿”å›å…³é”®è¯ï¼Œä¸è¦å…¶ä»–å†…å®¹ï¼š\n\n{text[:500]}"  # é™åˆ¶æ–‡æœ¬é•¿åº¦
        
        data = {
            "model": model,
            "messages": [
                {"role": "user", "content": prompt}
            ],
            "max_tokens": 50,
            "temperature": 0.3
        }
        
        response = requests.post(api_url, headers=headers, json=data, timeout=10)
        
        if response.status_code == 200:
            result = response.json()
            keywords = result['choices'][0]['message']['content'].strip()
            return keywords if keywords else extract_keywords(text)
        else:
            print(f"APIè¯·æ±‚å¤±è´¥: {response.status_code}")
            return extract_keywords(text)
            
    except Exception as e:
        print(f"APIè°ƒç”¨å‡ºé”™: {str(e)}")
        return extract_keywords(text)

def extract_keywords_with_ai_custom(text, api_key, custom_instruction, api_url=None, model=None):
    """ä½¿ç”¨è‡ªå®šä¹‰æŒ‡ä»¤çš„AIå…³é”®è¯æå–"""
    if not api_key:
        return extract_keywords(text)
    
    # ä½¿ç”¨é»˜è®¤å€¼æˆ–ç”¨æˆ·æä¾›çš„å€¼
    api_url = api_url or DEFAULT_API_URL
    model = model or DEFAULT_MODEL
    
    try:
        headers = {
            "Authorization": f"Bearer {api_key}",
            "Content-Type": "application/json"
        }
        
        prompt = f"{custom_instruction}\n\næ–‡æœ¬å†…å®¹ï¼š{text[:500]}"
        
        data = {
            "model": model,
            "messages": [
                {"role": "user", "content": prompt}
            ],
            "max_tokens": 100,
            "temperature": 0.3
        }
        
        response = requests.post(api_url, headers=headers, json=data, timeout=10)
        
        if response.status_code == 200:
            result = response.json()
            keywords = result['choices'][0]['message']['content'].strip()
            return keywords if keywords else extract_keywords(text)
        else:
            print(f"APIè¯·æ±‚å¤±è´¥: {response.status_code}")
            return extract_keywords(text)
            
    except Exception as e:
        print(f"APIè°ƒç”¨å‡ºé”™: {str(e)}")
        return extract_keywords(text)

def extract_keywords(text, num_keywords=5):
    """ä»æ–‡æœ¬ä¸­æå–å…³é”®è¯"""
    # ä½¿ç”¨jiebaåˆ†è¯
    words = jieba.cut(text)
    # è¿‡æ»¤åœç”¨è¯å’Œæ ‡ç‚¹ç¬¦å·
    stop_words = {'çš„', 'äº†', 'åœ¨', 'æ˜¯', 'æˆ‘', 'æœ‰', 'å’Œ', 'å°±', 'ä¸', 'äºº', 'éƒ½', 'ä¸€', 'ä¸€ä¸ª', 'ä¸Š', 'ä¹Ÿ', 'å¾ˆ', 'åˆ°', 'è¯´', 'è¦', 'å»', 'ä½ ', 'ä¼š', 'ç€', 'æ²¡æœ‰', 'çœ‹', 'å¥½', 'è‡ªå·±', 'è¿™'}
    filtered_words = [word for word in words if len(word) > 1 and word not in stop_words and word.isalpha()]
    
    # ç»Ÿè®¡è¯é¢‘
    word_freq = Counter(filtered_words)
    # è¿”å›æœ€å¸¸è§çš„å…³é”®è¯
    keywords = [word for word, freq in word_freq.most_common(num_keywords)]
    return 'ã€'.join(keywords) if keywords else 'ç»­å†™å°è¯´'

def clean_text(text):
    """æ¸…ç†æ–‡æœ¬ï¼Œå»é™¤å¤šä½™çš„ç©ºè¡Œå’Œæ ¼å¼"""
    # å»é™¤å¤šä½™çš„ç©ºç™½å­—ç¬¦
    text = re.sub(r'\s+', ' ', text)
    # å»é™¤å¯èƒ½çš„æ ‡é¢˜æ ‡è®°
    text = re.sub(r'^ç¬¬[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹å\d]+ç« .*?\n', '', text, flags=re.MULTILINE)
    text = re.sub(r'^ç« èŠ‚.*?\n', '', text, flags=re.MULTILINE)
    # å»é™¤ç©ºè¡Œ
    lines = [line.strip() for line in text.split('\n') if line.strip()]
    return '\n'.join(lines)

def smart_split_paragraphs(text, min_length=100, max_length=500, use_smart=True):
    """æ™ºèƒ½åˆ†å‰²æ®µè½"""
    if use_smart:
        # æ™ºèƒ½åˆ†æ®µï¼šè€ƒè™‘å¯¹è¯ã€åœºæ™¯è½¬æ¢ç­‰
        paragraphs = []
        
        # å…ˆæŒ‰è‡ªç„¶æ®µåˆ†å‰²
        natural_paragraphs = [p.strip() for p in text.split('\n\n') if p.strip()]
        
        for para in natural_paragraphs:
            if len(para) <= max_length and len(para) >= min_length:
                paragraphs.append(para)
            elif len(para) > max_length:
                # å¯¹è¿‡é•¿æ®µè½è¿›è¡ŒäºŒæ¬¡åˆ†å‰²
                sub_paragraphs = split_long_paragraph(para, min_length, max_length)
                paragraphs.extend(sub_paragraphs)
            else:
                # å¯¹è¿‡çŸ­æ®µè½å°è¯•åˆå¹¶
                if paragraphs and len(paragraphs[-1] + para) <= max_length:
                    paragraphs[-1] += '\n' + para
                else:
                    paragraphs.append(para)
        
        return paragraphs
    else:
        return split_into_paragraphs(text, min_length, max_length)

def split_long_paragraph(text, min_length=100, max_length=500):
    """åˆ†å‰²è¿‡é•¿çš„æ®µè½"""
    # ä¼˜å…ˆæŒ‰å¯¹è¯åˆ†å‰²
    if '"' in text or '"' in text or '"' in text:
        # ä½¿ç”¨ç®€å•çš„æ–¹æ³•æŒ‰å¯¹è¯æ ‡è®°åˆ†å‰²
        parts = re.split(r'(["""]+[^"""]*["""]+)', text)
        result = []
        current = ""
        
        for part in parts:
            if len(current + part) <= max_length:
                current += part
            else:
                if current and len(current) >= min_length:
                    result.append(current)
                current = part
        
        if current and len(current) >= min_length:
            result.append(current)
        
        return result if result else split_into_paragraphs(text, min_length, max_length)
    else:
        return split_into_paragraphs(text, min_length, max_length)

def split_into_paragraphs(text, min_length=100, max_length=500):
    """å°†æ–‡æœ¬åˆ†å‰²æˆåˆé€‚é•¿åº¦çš„æ®µè½"""
    # æŒ‰å¥å·ã€é—®å·ã€æ„Ÿå¹å·åˆ†å‰²
    sentences = re.split(r'[ã€‚ï¼ï¼Ÿ]', text)
    paragraphs = []
    current_paragraph = ""
    
    for sentence in sentences:
        sentence = sentence.strip()
        if not sentence:
            continue
            
        # å¦‚æœå½“å‰æ®µè½åŠ ä¸Šæ–°å¥å­è¶…è¿‡æœ€å¤§é•¿åº¦ï¼Œåˆ™ä¿å­˜å½“å‰æ®µè½
        if len(current_paragraph + sentence) > max_length and len(current_paragraph) >= min_length:
            if current_paragraph:
                paragraphs.append(current_paragraph)
            current_paragraph = sentence
        else:
            if current_paragraph:
                current_paragraph += sentence + "ã€‚"
            else:
                current_paragraph = sentence + "ã€‚"
    
    # æ·»åŠ æœ€åä¸€ä¸ªæ®µè½
    if current_paragraph and len(current_paragraph) >= min_length:
        paragraphs.append(current_paragraph)
    
    return paragraphs

def process_novel(file_obj, min_paragraph_length, max_paragraph_length, use_smart_split, use_ai_keywords, api_key, custom_instruction, api_url=None, model=None):
    """å¤„ç†å°è¯´æ–‡ä»¶å¹¶ç”Ÿæˆè®­ç»ƒæ•°æ®"""
    if file_obj is None:
        return "è¯·ä¸Šä¼ å°è¯´æ–‡ä»¶", "", []
    
    try:
        # ä¿®å¤æ–‡ä»¶è¯»å–é”™è¯¯
        if hasattr(file_obj, 'read'):
            content = file_obj.read()
        else:
            # å¦‚æœfile_objæ˜¯æ–‡ä»¶è·¯å¾„å­—ç¬¦ä¸²
            with open(file_obj, 'r', encoding='utf-8', errors='ignore') as f:
                content = f.read()
        
        if isinstance(content, bytes):
            content = content.decode('utf-8', errors='ignore')
        
        # æ¸…ç†æ–‡æœ¬
        cleaned_text = clean_text(content)
        
        # åˆ†å‰²æ®µè½ï¼ˆä½¿ç”¨æ™ºèƒ½åˆ†æ®µæˆ–ä¼ ç»Ÿåˆ†æ®µï¼‰
        if use_smart_split:
            paragraphs = smart_split_paragraphs(cleaned_text, min_paragraph_length, max_paragraph_length, True)
        else:
            paragraphs = split_into_paragraphs(cleaned_text, min_paragraph_length, max_paragraph_length)
        
        if len(paragraphs) < 2:
            return "æ–‡æœ¬å¤ªçŸ­ï¼Œæ— æ³•ç”Ÿæˆè®­ç»ƒæ•°æ®", "", []
        
        # ç”Ÿæˆè®­ç»ƒæ•°æ®
        training_data = []
        paragraph_data = []  # ç”¨äºç•Œé¢æ˜¾ç¤ºå’Œç¼–è¾‘
        
        for i in range(len(paragraphs) - 1):
            input_text = paragraphs[i]
            response_text = paragraphs[i + 1]
            
            # æå–å…³é”®è¯ï¼ˆä½¿ç”¨AIæˆ–æœ¬åœ°æ–¹æ³•ï¼‰
            if use_ai_keywords and api_key:
                keywords = extract_keywords_with_ai(response_text, api_key, api_url, model)
            else:
                keywords = extract_keywords(response_text)
            
            # ä½¿ç”¨è‡ªå®šä¹‰æŒ‡ä»¤æ¨¡æ¿æˆ–é»˜è®¤æ¨¡æ¿
            if custom_instruction:
                instruction = custom_instruction.replace('{keywords}', keywords)
            else:
                instruction = f"è¯·æ ¹æ®å…³é”®è¯'{keywords}'ç»­å†™å°è¯´æ®µè½"
            
            data_item = {
                "text": f"Instruction: {instruction}\n\nInput: {input_text}\n\nResponse: {response_text}"
            }
            training_data.append(data_item)
            
            # ä¿å­˜æ®µè½æ•°æ®ç”¨äºç•Œé¢ç¼–è¾‘
            paragraph_data.append({
                "index": i,
                "input": input_text,
                "response": response_text,
                "keywords": keywords,
                "instruction": instruction
            })
        
        # è½¬æ¢ä¸ºJSONLæ ¼å¼
        jsonl_output = "\n".join([json.dumps(item, ensure_ascii=False) for item in training_data])
        
        status_message = f"å¤„ç†å®Œæˆï¼ç”Ÿæˆäº† {len(training_data)} æ¡è®­ç»ƒæ•°æ®"
        
        return status_message, jsonl_output, paragraph_data
        
    except Exception as e:
        return f"å¤„ç†æ–‡ä»¶æ—¶å‡ºé”™: {str(e)}", "", []

def save_jsonl(jsonl_content, filename):
    """ä¿å­˜JSONLå†…å®¹åˆ°æ–‡ä»¶"""
    if not jsonl_content:
        return "æ²¡æœ‰å†…å®¹å¯ä¿å­˜"
    
    try:
        with open(filename, 'w', encoding='utf-8') as f:
            f.write(jsonl_content)
        return f"æ–‡ä»¶å·²ä¿å­˜ä¸º: {filename}"
    except Exception as e:
        return f"ä¿å­˜æ–‡ä»¶æ—¶å‡ºé”™: {str(e)}"

# æ®µè½ç¼–è¾‘ç›¸å…³å‡½æ•°
def load_jsonl_for_editing(file_obj):
    """åŠ è½½JSONLæ–‡ä»¶ç”¨äºç¼–è¾‘"""
    if file_obj is None:
        return [], "è¯·ä¸Šä¼ JSONLæ–‡ä»¶", [], "è¯·å…ˆä¸Šä¼ JSONLæ–‡ä»¶"
    
    try:
        # è¯»å–æ–‡ä»¶å†…å®¹
        if hasattr(file_obj, 'read'):
            content = file_obj.read()
        else:
            with open(file_obj, 'r', encoding='utf-8') as f:
                content = f.read()
        
        if isinstance(content, bytes):
            content = content.decode('utf-8', errors='ignore')
        
        # è§£æJSONL
        paragraph_data = []
        lines = content.strip().split('\n')
        
        for i, line in enumerate(lines):
            if line.strip():
                try:
                    data = json.loads(line)
                    # è§£ætextå­—æ®µä¸­çš„Instructionã€Inputã€Response
                    text = data.get('text', '')
                    
                    # ç®€å•è§£ææ ¼å¼
                    instruction_match = re.search(r'Instruction: (.*?)\n\nInput:', text, re.DOTALL)
                    input_match = re.search(r'Input: (.*?)\n\nResponse:', text, re.DOTALL)
                    response_match = re.search(r'Response: (.*?)$', text, re.DOTALL)
                    
                    instruction = instruction_match.group(1).strip() if instruction_match else ""
                    input_text = input_match.group(1).strip() if input_match else ""
                    response_text = response_match.group(1).strip() if response_match else ""
                    
                    # ä»instructionä¸­æå–å…³é”®è¯
                    keywords_match = re.search(r"å…³é”®è¯'([^']*)'|å…³é”®è¯'([^']*)'|å…³é”®è¯'([^']*)'|å…³é”®è¯\"([^\"]*)\"|å…³é”®è¯\"([^\"]*)\"|å…³é”®è¯\"([^\"]*)\"", instruction)
                    keywords = ""
                    if keywords_match:
                        for group in keywords_match.groups():
                            if group:
                                keywords = group
                                break
                    
                    paragraph_data.append({
                        "index": i,
                        "instruction": instruction,
                        "input": input_text,
                        "response": response_text,
                        "keywords": keywords,
                        "original_text": text
                    })
                except json.JSONDecodeError:
                    continue
        
        if not paragraph_data:
            return [], "JSONLæ–‡ä»¶æ ¼å¼é”™è¯¯æˆ–ä¸ºç©º", [], "JSONLæ–‡ä»¶æ ¼å¼é”™è¯¯æˆ–ä¸ºç©º"
        
        # ç”Ÿæˆç»Ÿè®¡ä¿¡æ¯
        stats = f"æ€»æ®µè½æ•°: {len(paragraph_data)}"
        
        # ç”Ÿæˆé€‰æ‹©é€‰é¡¹
        choices = [f"{i}: {data['input'][:50]}..." for i, data in enumerate(paragraph_data)]
        
        # ç”ŸæˆHTMLåˆ—è¡¨
        html_content = generate_paragraph_list_html(paragraph_data, [])
        
        return paragraph_data, stats, choices, html_content
        
    except Exception as e:
        return [], f"åŠ è½½æ–‡ä»¶æ—¶å‡ºé”™: {str(e)}", [], f"åŠ è½½æ–‡ä»¶æ—¶å‡ºé”™: {str(e)}"

def generate_paragraph_list_html(paragraph_data, selected_indices):
    """ç”Ÿæˆæ®µè½åˆ—è¡¨çš„HTML"""
    if not paragraph_data:
        return "è¯·å…ˆä¸Šä¼ JSONLæ–‡ä»¶"
    
    html = "<div style='max-height: 400px; overflow-y: auto;'>"
    
    for i, data in enumerate(paragraph_data):
        selected_class = "background-color: #e3f2fd;" if i in selected_indices else ""
        
        html += f"""
        <div style='border: 1px solid #ddd; margin: 5px 0; padding: 10px; border-radius: 5px; {selected_class}'>
            <div style='font-weight: bold; color: #1976d2;'>æ®µè½ {i + 1}</div>
            <div style='margin: 5px 0;'><strong>å…³é”®è¯:</strong> {data.get('keywords', 'æ— ')}</div>
            <div style='margin: 5px 0;'><strong>è¾“å…¥:</strong> {data.get('input', '')[:100]}{'...' if len(data.get('input', '')) > 100 else ''}</div>
            <div style='margin: 5px 0;'><strong>å“åº”:</strong> {data.get('response', '')[:100]}{'...' if len(data.get('response', '')) > 100 else ''}</div>
        </div>
        """
    
    html += "</div>"
    return html

def select_all_paragraphs(paragraph_data):
    """å…¨é€‰æ®µè½"""
    if not paragraph_data:
        return []
    return list(range(len(paragraph_data)))

def deselect_all_paragraphs():
    """å–æ¶ˆå…¨é€‰"""
    return []

def delete_selected_paragraphs(paragraph_data, selected_indices):
    """åˆ é™¤é€‰ä¸­çš„æ®µè½"""
    if not selected_indices or not paragraph_data:
        return paragraph_data, "æ²¡æœ‰é€‰æ‹©è¦åˆ é™¤çš„æ®µè½", [], generate_paragraph_list_html(paragraph_data, [])
    
    # æŒ‰ç´¢å¼•å€’åºåˆ é™¤ï¼Œé¿å…ç´¢å¼•å˜åŒ–é—®é¢˜
    selected_indices = sorted(selected_indices, reverse=True)
    new_paragraph_data = paragraph_data.copy()
    
    for idx in selected_indices:
        if 0 <= idx < len(new_paragraph_data):
            del new_paragraph_data[idx]
    
    # é‡æ–°ç¼–å·
    for i, data in enumerate(new_paragraph_data):
        data['index'] = i
    
    # æ›´æ–°é€‰æ‹©é€‰é¡¹
    choices = [f"{i}: {data['input'][:50]}..." for i, data in enumerate(new_paragraph_data)]
    
    # ç”Ÿæˆæ–°çš„HTML
    html_content = generate_paragraph_list_html(new_paragraph_data, [])
    
    stats = f"æ€»æ®µè½æ•°: {len(new_paragraph_data)} (å·²åˆ é™¤ {len(selected_indices)} ä¸ªæ®µè½)"
    
    return new_paragraph_data, stats, choices, html_content

def ai_regenerate_keywords_for_selected(paragraph_data, selected_indices, api_key, prompt_template, api_url, model):
    """ä¸ºé€‰ä¸­æ®µè½AIé‡æ–°ç”Ÿæˆå…³é”®è¯"""
    if not selected_indices or not paragraph_data or not api_key:
        return paragraph_data, "è¯·é€‰æ‹©æ®µè½å¹¶å¡«å…¥APIå¯†é’¥", generate_paragraph_list_html(paragraph_data, selected_indices)
    
    updated_count = 0
    new_paragraph_data = paragraph_data.copy()
    
    for idx in selected_indices:
        if 0 <= idx < len(new_paragraph_data):
            response_text = new_paragraph_data[idx]['response']
            
            # ä½¿ç”¨AIé‡æ–°ç”Ÿæˆå…³é”®è¯
            new_keywords = extract_keywords_with_ai_custom(
                response_text, api_key, prompt_template, api_url, model
            )
            
            if new_keywords and new_keywords != new_paragraph_data[idx]['keywords']:
                new_paragraph_data[idx]['keywords'] = new_keywords
                # æ›´æ–°instruction
                new_instruction = f"è¯·æ ¹æ®å…³é”®è¯'{new_keywords}'ç»­å†™å°è¯´æ®µè½"
                new_paragraph_data[idx]['instruction'] = new_instruction
                updated_count += 1
    
    # ç”Ÿæˆæ–°çš„HTML
    html_content = generate_paragraph_list_html(new_paragraph_data, selected_indices)
    
    stats = f"å·²æ›´æ–° {updated_count} ä¸ªæ®µè½çš„å…³é”®è¯"
    
    return new_paragraph_data, stats, html_content

def convert_jsonl_to_binidx(jsonl_file, output_prefix, tokenizer_type="RWKVTokenizer"):
    """å°†JSONLæ–‡ä»¶è½¬æ¢ä¸ºbinidxæ ¼å¼"""
    try:
        # æ£€æŸ¥json2binidxå·¥å…·æ˜¯å¦å­˜åœ¨ï¼Œä½¿ç”¨ç»å¯¹è·¯å¾„
        current_dir = Path(__file__).parent
        tool_dir = current_dir / "json2binidx_tool"
        if not tool_dir.exists():
            return "é”™è¯¯ï¼šæ‰¾ä¸åˆ°json2binidx_toolç›®å½•"
        
        preprocess_script = tool_dir / "tools" / "preprocess_data.py"
        if not preprocess_script.exists():
            return "é”™è¯¯ï¼šæ‰¾ä¸åˆ°preprocess_data.pyè„šæœ¬"
        
        # é€‰æ‹©tokenizeræ–‡ä»¶
        if tokenizer_type == "RWKVTokenizer":
            vocab_file = tool_dir / "rwkv_vocab_v20230424.txt"
        else:
            vocab_file = tool_dir / "20B_tokenizer.json"
        
        if not vocab_file.exists():
            return f"é”™è¯¯ï¼šæ‰¾ä¸åˆ°tokenizeræ–‡ä»¶: {vocab_file}"
        
        # æ„å»ºè½¬æ¢å‘½ä»¤
        cmd = [
            sys.executable,
            str(preprocess_script),
            "--input", jsonl_file,
            "--output-prefix", output_prefix,
            "--vocab", str(vocab_file),
            "--dataset-impl", "mmap",
            "--tokenizer-type", tokenizer_type,
            "--append-eod"
        ]
        
        # æ‰§è¡Œè½¬æ¢
        result = subprocess.run(cmd, capture_output=True, text=True, cwd=str(tool_dir))
        
        if result.returncode == 0:
            return f"è½¬æ¢æˆåŠŸï¼ç”Ÿæˆæ–‡ä»¶: {output_prefix}.bin å’Œ {output_prefix}.idx"
        else:
            return f"è½¬æ¢å¤±è´¥: {result.stderr}"
            
    except Exception as e:
        return f"è½¬æ¢è¿‡ç¨‹ä¸­å‡ºé”™: {str(e)}"

# åˆ›å»ºä¸»ç•Œé¢
def create_interface():
    processor = DataProcessor()
    
    with gr.Blocks(title="RWKVæ•°æ®é›†å¤„ç†å·¥å…·", theme=gr.themes.Ocean()) as demo:
        gr.Markdown("# ğŸš€ RWKVæ•°æ®é›†å¤„ç†å·¥å…·")
        gr.Markdown("é›†æˆé€šç”¨æ•°æ®å¤„ç†ã€å°è¯´ç±»å¢å¼ºå¤„ç†å’Œæ•°æ®æ ¼å¼è½¬æ¢åŠŸèƒ½çš„ä¸€ç«™å¼å·¥å…·")
        
        with gr.Tabs():
            # é€šç”¨æ•°æ®å¤„ç†æ ‡ç­¾é¡µ
            with gr.TabItem("ğŸ“‹ é€šç”¨æ•°æ®å¤„ç†"):
                with gr.Row():
                    with gr.Column(scale=1):
                        gr.Markdown("### ğŸ“ æ•°æ®è¾“å…¥")
                        
                        input_mode = gr.Radio(
                            choices=["æ–‡ä»¶ä¸Šä¼ ", "ç›®å½•å¤„ç†"],
                            label="è¾“å…¥æ¨¡å¼",
                            value="æ–‡ä»¶ä¸Šä¼ ",
                            info="é€‰æ‹©å¤„ç†å•ä¸ªæ–‡ä»¶è¿˜æ˜¯æ•´ä¸ªç›®å½•"
                        )
                        
                        file_input = gr.File(
                            label="ä¸Šä¼ æ–‡æœ¬æ–‡ä»¶",
                            file_types=[".txt", ".json", ".jsonl"],
                            type="binary",
                            visible=True
                        )
                        
                        directory_input = gr.Textbox(
                            label="ç›®å½•è·¯å¾„",
                            placeholder="è¯·è¾“å…¥è¦å¤„ç†çš„ç›®å½•å®Œæ•´è·¯å¾„ï¼Œä¾‹å¦‚ï¼šD:\\data\\texts",
                            info="ç›®å½•ä¸‹çš„æ‰€æœ‰.txtã€.jsonã€.jsonlæ–‡ä»¶å°†è¢«å¤„ç†å¹¶åˆå¹¶",
                            visible=False
                        )
                        
                        data_format = gr.Dropdown(
                            choices=["å•è½®é—®ç­”", "å¤šè½®å¯¹è¯", "æŒ‡ä»¤é—®ç­”", "é•¿æ–‡æœ¬", "å¸¦æ ‡é¢˜æ–‡ç« ", "å°è¯´æ®µè½ç»­å†™", "ç« èŠ‚å¤§çº²æ‰©å†™"],
                            label="æ•°æ®æ ¼å¼",
                            value="å•è½®é—®ç­”"
                        )
                        
                        system_prompt = gr.Textbox(
                            label="ç³»ç»Ÿæç¤ºï¼ˆå¯é€‰ï¼‰",
                            placeholder="ä¾‹å¦‚ï¼šä½ æ˜¯ä¸€ä¸ªæœ‰ç”¨çš„AIåŠ©æ‰‹...",
                            lines=3
                        )
                        
                        process_btn = gr.Button("ğŸ”„ å¤„ç†æ•°æ®", variant="primary")
                        
                    with gr.Column(scale=2):
                        gr.Markdown("### ğŸ“Š å¤„ç†ç»“æœ")
                        
                        status_output = gr.Textbox(
                            label="å¤„ç†çŠ¶æ€",
                            interactive=False
                        )
                        
                        preview_output = gr.Textbox(
                            label="æ•°æ®é¢„è§ˆ",
                            lines=10,
                            interactive=False
                        )
                        
                        jsonl_content = gr.Textbox(
                            label="JSONLå†…å®¹",
                            lines=5,
                            interactive=False
                        )
                
                # æ ¼å¼è¯´æ˜
                with gr.Accordion("ğŸ“– æ•°æ®æ ¼å¼è¯´æ˜", open=False):
                    gr.Markdown(
                        """
                        ### å•è½®é—®ç­”æ ¼å¼
                        æ¯è¡Œä¸€ä¸ªé—®ç­”å¯¹ï¼Œç”¨åˆ¶è¡¨ç¬¦æˆ–ç«–çº¿åˆ†éš”ï¼š
                        ```
                        é—®é¢˜1    ç­”æ¡ˆ1
                        é—®é¢˜2    ç­”æ¡ˆ2
                        ```
                        
                        ### å¤šè½®å¯¹è¯æ ¼å¼
                        æ¯ä¸ªå¯¹è¯ç”¨ç©ºè¡Œåˆ†éš”ï¼Œå¥‡æ•°è¡Œä¸ºç”¨æˆ·è¾“å…¥ï¼Œå¶æ•°è¡Œä¸ºåŠ©æ‰‹å›å¤ï¼š
                        ```
                        ä½ å¥½
                        ä½ å¥½ï¼å¾ˆé«˜å…´è§åˆ°ä½ 
                        ä»Šå¤©å¤©æ°”æ€ä¹ˆæ ·ï¼Ÿ
                        ä»Šå¤©å¤©æ°”å¾ˆå¥½
                        
                        æ–°çš„å¯¹è¯å¼€å§‹
                        å¥½çš„
                        ```
                        
                        ### æŒ‡ä»¤é—®ç­”æ ¼å¼
                        æ¯ä¸ªæ ·æœ¬ç”¨ä¸‰ä¸ªç©ºè¡Œåˆ†éš”ï¼š
                        ```
                        è¯·æ€»ç»“ä»¥ä¸‹å†…å®¹
                        
                        è¿™æ˜¯éœ€è¦æ€»ç»“çš„å†…å®¹...
                        
                        è¿™æ˜¯æ€»ç»“ç»“æœ...
                        
                        
                        
                        ä¸‹ä¸€ä¸ªæŒ‡ä»¤
                        
                        è¾“å…¥å†…å®¹
                        
                        å“åº”å†…å®¹
                        ```
                        
                        ### é•¿æ–‡æœ¬æ ¼å¼
                        ç›´æ¥è¾“å…¥æ–‡ç« å†…å®¹ï¼Œå·¥å…·ä¼šè‡ªåŠ¨æŒ‰æ®µè½åˆ†å‰²ã€‚
                        
                        ### å¸¦æ ‡é¢˜æ–‡ç« æ ¼å¼
                        æ ‡é¢˜ç”¨#å¼€å¤´æˆ–ç”¨ã€Šã€‹åŒ…å›´ï¼š
                        ```
                        # æ–‡ç« æ ‡é¢˜1
                        æ–‡ç« å†…å®¹...
                        
                        ã€Šæ–‡ç« æ ‡é¢˜2ã€‹
                        æ–‡ç« å†…å®¹...
                        ```
                        """
                    )
            
            # å°è¯´ç±»å¢å¼ºå¤„ç†æ ‡ç­¾é¡µ
            with gr.TabItem("ğŸ“š å°è¯´ç±»å¢å¼ºå¤„ç†"):
                # çŠ¶æ€å˜é‡å­˜å‚¨æ®µè½æ•°æ®
                paragraph_data_state = gr.State([])
                
                with gr.Tabs():
                    # æ–‡ä»¶å¤„ç†å­æ ‡ç­¾é¡µ
                    with gr.TabItem("ğŸ“„ æ–‡ä»¶å¤„ç†"):
                        with gr.Row():
                            with gr.Column(scale=1):
                                gr.Markdown("### ğŸ“ æ–‡ä»¶ä¸Šä¼ ")
                                novel_file_input = gr.File(
                                    label="ä¸Šä¼ å°è¯´æ–‡ä»¶",
                                    file_types=[".txt", ".md"],
                                    type="filepath"
                                )
                                
                                novel_directory_input = gr.Textbox(
                                    label="å°è¯´ç›®å½•è·¯å¾„ï¼ˆå¯é€‰ï¼‰",
                                    placeholder="è¾“å…¥åŒ…å«å¤šä¸ªå°è¯´æ–‡ä»¶çš„ç›®å½•è·¯å¾„",
                                    info="æ‰¹é‡å¤„ç†ç›®å½•ä¸‹çš„æ‰€æœ‰å°è¯´æ–‡ä»¶"
                                )
                                
                                gr.Markdown("### âš™ï¸ å¤„ç†è®¾ç½®")
                                with gr.Row():
                                    min_length = gr.Slider(
                                        minimum=50,
                                        maximum=300,
                                        value=100,
                                        step=10,
                                        label="æœ€å°æ®µè½é•¿åº¦"
                                    )
                                    max_length = gr.Slider(
                                        minimum=200,
                                        maximum=800,
                                        value=500,
                                        step=50,
                                        label="æœ€å¤§æ®µè½é•¿åº¦"
                                    )
                                
                                use_smart_split = gr.Checkbox(
                                    label="ğŸ§  ä½¿ç”¨æ™ºèƒ½åˆ†æ®µ",
                                    value=True,
                                    info="è€ƒè™‘å¯¹è¯ã€åœºæ™¯è½¬æ¢ç­‰è¿›è¡Œæ™ºèƒ½åˆ†æ®µ"
                                )
                                
                                gr.Markdown("### ğŸ¤– AIè®¾ç½®")
                                use_ai_keywords = gr.Checkbox(
                                    label="ä½¿ç”¨AIæå–å…³é”®è¯",
                                    value=False,
                                    info="ä½¿ç”¨AI APIæå–æ›´å‡†ç¡®çš„å…³é”®è¯"
                                )
                                
                                with gr.Group(visible=False) as ai_settings_group:
                                    api_key_input = gr.Textbox(
                                        label="API Key",
                                        type="password",
                                        placeholder="sk-...",
                                        info="è¾“å…¥æ‚¨çš„APIå¯†é’¥"
                                    )
                                    
                                    api_url_input = gr.Textbox(
                                        label="API URL",
                                        value=DEFAULT_API_URL,
                                        placeholder="https://api.deepseek.com/v1/chat/completions",
                                        info="æ”¯æŒOpenAIå…¼å®¹æ¥å£ï¼Œå¦‚vLLMæœåŠ¡"
                                    )
                                    
                                    model_input = gr.Textbox(
                                        label="æ¨¡å‹åç§°",
                                        value=DEFAULT_MODEL,
                                        placeholder="deepseek-chat",
                                        info="æŒ‡å®šè¦ä½¿ç”¨çš„æ¨¡å‹åç§°"
                                    )
                                
                                custom_instruction = gr.Textbox(
                                    label="è‡ªå®šä¹‰æŒ‡ä»¤æ¨¡æ¿",
                                    placeholder="è¯·æ ¹æ®å…³é”®è¯'{keywords}'ç»­å†™å°è¯´æ®µè½",
                                    info="ä½¿ç”¨{keywords}ä½œä¸ºå…³é”®è¯å ä½ç¬¦"
                                )
                                
                                novel_process_btn = gr.Button("ğŸš€ å¤„ç†å°è¯´", variant="primary", size="lg")
                                
                            with gr.Column(scale=2):
                                novel_status_output = gr.Textbox(
                                    label="ğŸ“Š å¤„ç†çŠ¶æ€",
                                    interactive=False
                                )
                                
                                novel_jsonl_output = gr.Textbox(
                                    label="ğŸ“„ ç”Ÿæˆçš„JSONLæ•°æ®",
                                    lines=15,
                                    max_lines=25,
                                    interactive=False
                                )
                                
                                with gr.Row():
                                    novel_filename_input = gr.Textbox(
                                        value="novel_training_data.jsonl",
                                        label="ğŸ’¾ ä¿å­˜æ–‡ä»¶å",
                                        scale=3
                                    )
                                    novel_save_btn = gr.Button("ğŸ’¾ ä¿å­˜JSONL", scale=1)
                                
                                novel_save_status = gr.Textbox(
                                    label="ğŸ’¾ ä¿å­˜çŠ¶æ€",
                                    interactive=False
                                )
                    
                    # æ®µè½ç¼–è¾‘å­æ ‡ç­¾é¡µ
                    with gr.TabItem("âœï¸ æ®µè½ç¼–è¾‘"):
                        with gr.Row():
                            with gr.Column(scale=1):
                                gr.Markdown("### ğŸ“‚ JSONLæ–‡ä»¶ä¸Šä¼ ")
                                edit_jsonl_file = gr.File(
                                    label="ä¸Šä¼ JSONLæ–‡ä»¶",
                                    file_types=[".jsonl"]
                                )
                                
                                gr.Markdown("### ğŸ“Š æ®µè½ç»Ÿè®¡")
                                paragraph_stats = gr.Textbox(
                                    label="æ®µè½ç»Ÿè®¡ä¿¡æ¯",
                                    interactive=False
                                )
                                
                                gr.Markdown("### ğŸ¯ é€‰æ‹©ç®¡ç†")
                                with gr.Row():
                                    select_all_btn = gr.Button("å…¨é€‰", size="sm")
                                    deselect_all_btn = gr.Button("å–æ¶ˆå…¨é€‰", size="sm")
                                
                                selected_indices = gr.CheckboxGroup(
                                    label="æ‰‹åŠ¨é€‰æ‹©æ®µè½",
                                    choices=[],
                                    value=[]
                                )
                                
                                with gr.Row():
                                    delete_selected_btn = gr.Button("ğŸ—‘ï¸ åˆ é™¤é€‰ä¸­", variant="stop")
                                    ai_regenerate_btn = gr.Button("ğŸ¤– AIé‡æ–°ç”Ÿæˆå…³é”®è¯", variant="secondary")
                                
                                gr.Markdown("### ğŸ¤– AIè®¾ç½®")
                                edit_prompt_template = gr.Textbox(
                                    label="æç¤ºè¯æ¨¡æ¿",
                                    value="è¯·ä»ä»¥ä¸‹æ–‡æœ¬ä¸­æå–3-5ä¸ªæœ€é‡è¦çš„å…³é”®è¯ï¼Œç”¨é¡¿å·åˆ†éš”ï¼Œåªè¿”å›å…³é”®è¯ï¼š",
                                    lines=2
                                )
                                
                                edit_api_key = gr.Textbox(
                                    label="API Key",
                                    type="password",
                                    placeholder="sk-..."
                                )
                                
                                edit_api_url = gr.Textbox(
                                    label="API URL",
                                    value=DEFAULT_API_URL,
                                    placeholder="https://api.deepseek.com/v1/chat/completions"
                                )
                                
                                edit_model = gr.Textbox(
                                    label="æ¨¡å‹åç§°",
                                    value=DEFAULT_MODEL,
                                    placeholder="deepseek-chat"
                                )
                            
                            with gr.Column(scale=2):
                                gr.Markdown("### ğŸ“ æ®µè½åˆ—è¡¨")
                                paragraph_list = gr.HTML(
                                    label="æ®µè½åˆ—è¡¨",
                                    value="è¯·å…ˆä¸Šä¼ JSONLæ–‡ä»¶"
                                )
                                
                                gr.Markdown("### âœï¸ ç¼–è¾‘æŒ‡ä»¤")
                                edit_instruction = gr.Textbox(
                                    label="ç¼–è¾‘è¯´æ˜",
                                    placeholder="é€‰æ‹©æ®µè½åï¼Œå¯ä»¥åœ¨è¿™é‡ŒæŸ¥çœ‹å’Œç¼–è¾‘æŒ‡ä»¤å†…å®¹",
                                    lines=3
                                )
                                
                                with gr.Row():
                                    update_paragraph_btn = gr.Button("ğŸ“ æ›´æ–°æ®µè½", variant="primary")
                                    regenerate_paragraph_btn = gr.Button("ğŸ”„ é‡æ–°ç”Ÿæˆ", variant="secondary")
                                
                                gr.Markdown("### ğŸ‘€ é¢„è§ˆ")
                                with gr.Row():
                                    with gr.Column():
                                        input_preview = gr.Textbox(
                                            label="è¾“å…¥å†…å®¹",
                                            lines=5,
                                            interactive=False
                                        )
                                    with gr.Column():
                                        response_preview = gr.Textbox(
                                            label="å“åº”å†…å®¹",
                                            lines=5,
                                            interactive=False
                                        )
            
            # æ•°æ®æ ¼å¼è½¬æ¢æ ‡ç­¾é¡µ
            with gr.TabItem("ğŸ”„ æ•°æ®æ ¼å¼è½¬æ¢"):
                with gr.Row():
                    with gr.Column():
                        gr.Markdown("### ğŸ“Š JSONLè½¬BinIdx")
                        gr.Markdown("å°†JSONLæ ¼å¼è½¬æ¢ä¸ºRWKVè®­ç»ƒæ‰€éœ€çš„BinIdxæ ¼å¼")
                        
                        convert_jsonl_file = gr.File(
                            label="é€‰æ‹©JSONLæ–‡ä»¶",
                            file_types=[".jsonl"]
                        )
                        
                        output_prefix = gr.Textbox(
                            label="è¾“å‡ºæ–‡ä»¶å‰ç¼€",
                            value="./data/training_data",
                            placeholder="./data/training_data"
                        )
                        
                        tokenizer_type = gr.Radio(
                            choices=["RWKVTokenizer", "HFTokenizer"],
                            value="RWKVTokenizer",
                            label="Tokenizerç±»å‹",
                            info="RWKVTokenizerç”¨äºå¤šè¯­è¨€æ¨¡å‹ï¼ŒHFTokenizerç”¨äºGPT-NeoXæ¨¡å‹"
                        )
                        
                        convert_btn = gr.Button("ğŸ”„ å¼€å§‹è½¬æ¢", variant="primary")
                        
                    with gr.Column():
                        convert_status = gr.Textbox(
                            label="ğŸ”„ è½¬æ¢çŠ¶æ€",
                            lines=10,
                            interactive=False
                        )
                        
                        gr.Markdown("### ğŸ’¾ ä¸‹è½½é€‰é¡¹")
                        
                        download_output_name = gr.Textbox(
                            label="è¾“å‡ºæ–‡ä»¶å",
                            value="training_data",
                            placeholder="ä¸åŒ…å«æ‰©å±•å"
                        )
                        
                        download_output_path = gr.Textbox(
                            label="è¾“å‡ºè·¯å¾„ï¼ˆå¯é€‰ï¼‰",
                            placeholder="ç•™ç©ºåˆ™ä½¿ç”¨é»˜è®¤è·¯å¾„ï¼ˆç”¨æˆ·ç›®å½•/RWKV_Downloadsï¼‰",
                            info="æŒ‡å®šè‡ªå®šä¹‰è¾“å‡ºç›®å½•çš„å®Œæ•´è·¯å¾„"
                        )
                        
                        download_jsonl = gr.DownloadButton(
                            label="ğŸ“¥ ä¸‹è½½JSONLæ–‡ä»¶",
                            variant="secondary"
                        )
                        
                        download_status = gr.Textbox(
                            label="ä¸‹è½½çŠ¶æ€",
                            interactive=False,
                            visible=False
                        )
        
        # é€šç”¨æ•°æ®å¤„ç†äº‹ä»¶ç»‘å®š
        def toggle_input_mode(mode):
            if mode == "æ–‡ä»¶ä¸Šä¼ ":
                return gr.update(visible=True), gr.update(visible=False)
            else:  # ç›®å½•å¤„ç†
                return gr.update(visible=False), gr.update(visible=True)
        
        def process_file(file_obj, data_format, system_prompt=""):
            if file_obj is None:
                return "è¯·ä¸Šä¼ æ–‡ä»¶", "", ""
            
            try:
                # è¯»å–æ–‡ä»¶å†…å®¹
                if hasattr(file_obj, 'read'):
                    if hasattr(file_obj, 'name'):
                        print(f"æ­£åœ¨å¤„ç†æ–‡ä»¶: {file_obj.name}")
                    content = file_obj.read()
                    if isinstance(content, bytes):
                        content = content.decode('utf-8')
                elif isinstance(file_obj, bytes):
                    content = file_obj.decode('utf-8')
                elif isinstance(file_obj, str):
                    with open(file_obj, 'r', encoding='utf-8') as f:
                        content = f.read()
                else:
                    return f"ä¸æ”¯æŒçš„æ–‡ä»¶ç±»å‹: {type(file_obj)}", "", ""
                
                # æ ¹æ®æ ¼å¼å¤„ç†æ•°æ®
                if data_format == "å•è½®é—®ç­”":
                    jsonl_data = processor.process_single_qa(content)
                elif data_format == "å¤šè½®å¯¹è¯":
                    jsonl_data = processor.process_multi_turn(content)
                elif data_format == "æŒ‡ä»¤é—®ç­”":
                    jsonl_data = processor.process_instruction(content)
                elif data_format == "é•¿æ–‡æœ¬":
                    jsonl_data = processor.process_long_text(content)
                elif data_format == "å¸¦æ ‡é¢˜æ–‡ç« ":
                    jsonl_data = processor.process_article_with_title(content)
                elif data_format == "å°è¯´æ®µè½ç»­å†™":
                    jsonl_data = processor.process_novel_continuation(content)
                elif data_format == "ç« èŠ‚å¤§çº²æ‰©å†™":
                    jsonl_data = processor.process_chapter_expansion(content)
                else:
                    return "ä¸æ”¯æŒçš„æ•°æ®æ ¼å¼", "", ""
                
                # å¦‚æœæœ‰ç³»ç»Ÿæç¤ºï¼Œæ·»åŠ åˆ°æ¯æ¡æ•°æ®ä¸­
                if system_prompt.strip():
                    for item in jsonl_data:
                        if data_format in ["å•è½®é—®ç­”", "å¤šè½®å¯¹è¯"]:
                            item["text"] = f"System: {system_prompt.strip()}\n\n{item['text']}"
                
                # ç”Ÿæˆé¢„è§ˆ
                preview = "\n".join([json.dumps(item, ensure_ascii=False, indent=2) for item in jsonl_data[:3]])
                if len(jsonl_data) > 3:
                    preview += f"\n\n... è¿˜æœ‰ {len(jsonl_data) - 3} æ¡æ•°æ®"
                
                # ç”Ÿæˆä¸‹è½½å†…å®¹
                download_content = "\n".join([json.dumps(item, ensure_ascii=False) for item in jsonl_data])
                
                return f"å¤„ç†å®Œæˆï¼å…±ç”Ÿæˆ {len(jsonl_data)} æ¡æ•°æ®", preview, download_content
                
            except Exception as e:
                return f"å¤„ç†å¤±è´¥: {str(e)}", "", ""
        
        def process_directory(directory_path, data_format, system_prompt=""):
            if not directory_path or not os.path.exists(directory_path):
                return "è¯·æä¾›æœ‰æ•ˆçš„ç›®å½•è·¯å¾„", "", ""
            
            if not os.path.isdir(directory_path):
                return "æä¾›çš„è·¯å¾„ä¸æ˜¯ç›®å½•", "", ""
            
            try:
                # å¤„ç†ç›®å½•ä¸‹çš„æ‰€æœ‰æ–‡ä»¶
                jsonl_data, processed_files = processor.process_directory(directory_path, data_format)
                
                if not jsonl_data:
                    return "ç›®å½•ä¸­æ²¡æœ‰æ‰¾åˆ°å¯å¤„ç†çš„æ–‡ä»¶æˆ–å¤„ç†å¤±è´¥", "", ""
                
                # å¦‚æœæœ‰ç³»ç»Ÿæç¤ºï¼Œæ·»åŠ åˆ°æ¯æ¡æ•°æ®ä¸­
                if system_prompt.strip():
                    for item in jsonl_data:
                        if data_format in ["å•è½®é—®ç­”", "å¤šè½®å¯¹è¯"]:
                            item["text"] = f"System: {system_prompt.strip()}\n\n{item['text']}"
                
                # ç”Ÿæˆé¢„è§ˆ
                preview = "\n".join([json.dumps(item, ensure_ascii=False, indent=2) for item in jsonl_data[:3]])
                if len(jsonl_data) > 3:
                    preview += f"\n\n... è¿˜æœ‰ {len(jsonl_data) - 3} æ¡æ•°æ®"
                
                # æ·»åŠ å¤„ç†æ–‡ä»¶åˆ—è¡¨åˆ°é¢„è§ˆ
                files_info = f"\n\nå·²å¤„ç†çš„æ–‡ä»¶ ({len(processed_files)} ä¸ª):\n" + "\n".join([f"- {os.path.basename(f)}" for f in processed_files[:10]])
                if len(processed_files) > 10:
                    files_info += f"\n... è¿˜æœ‰ {len(processed_files) - 10} ä¸ªæ–‡ä»¶"
                preview += files_info
                
                # ç”Ÿæˆä¸‹è½½å†…å®¹
                download_content = "\n".join([json.dumps(item, ensure_ascii=False) for item in jsonl_data])
                
                return f"å¤„ç†å®Œæˆï¼å…±å¤„ç† {len(processed_files)} ä¸ªæ–‡ä»¶ï¼Œç”Ÿæˆ {len(jsonl_data)} æ¡æ•°æ®", preview, download_content
                
            except Exception as e:
                return f"å¤„ç†å¤±è´¥: {str(e)}", "", ""
        
        def process_data(input_mode, file_input, directory_input, data_format, system_prompt):
            if input_mode == "æ–‡ä»¶ä¸Šä¼ ":
                return process_file(file_input, data_format, system_prompt)
            else:  # ç›®å½•å¤„ç†
                return process_directory(directory_input, data_format, system_prompt)
        
        def prepare_jsonl_download(jsonl_content, output_name, custom_output_path=""):
            """å‡†å¤‡JSONLæ–‡ä»¶ä¸‹è½½"""
            if not jsonl_content.strip():
                return None, gr.update(visible=False)
            
            if not output_name.strip():
                output_name = "training_data"
            
            # ç¡®å®šä¿å­˜è·¯å¾„
            if custom_output_path.strip():
                download_dir = custom_output_path.strip()
            else:
                user_home = os.path.expanduser("~")
                download_dir = os.path.join(user_home, "RWKV_Downloads")
            
            # åˆ›å»ºç›®å½•
            try:
                os.makedirs(download_dir, exist_ok=True)
            except Exception as e:
                # å¦‚æœåˆ›å»ºç›®å½•å¤±è´¥ï¼Œä½¿ç”¨ä¸´æ—¶ç›®å½•
                download_dir = tempfile.gettempdir()
            
            # åˆ›å»ºæ–‡ä»¶è·¯å¾„
            jsonl_file_path = os.path.join(download_dir, f"{output_name}.jsonl")
            
            # ä¿å­˜æ–‡ä»¶
            try:
                with open(jsonl_file_path, 'w', encoding='utf-8') as f:
                    f.write(jsonl_content)
                status_msg = f"JSONLæ–‡ä»¶å·²ä¿å­˜åˆ°: {jsonl_file_path}"
                return jsonl_file_path, gr.update(value=status_msg, visible=True)
            except Exception as e:
                # å¦‚æœä¿å­˜å¤±è´¥ï¼Œåˆ›å»ºä¸´æ—¶æ–‡ä»¶
                temp_file = tempfile.NamedTemporaryFile(mode='w', suffix='.jsonl', delete=False, encoding='utf-8')
                temp_file.write(jsonl_content)
                temp_file.close()
                status_msg = f"ä¿å­˜åˆ°è‡ªå®šä¹‰è·¯å¾„å¤±è´¥ï¼Œå·²ä¿å­˜åˆ°ä¸´æ—¶æ–‡ä»¶: {temp_file.name}"
                return temp_file.name, gr.update(value=status_msg, visible=True)
        
        def convert_to_binidx_wrapper(jsonl_content, output_name, custom_output_path=""):
            if not jsonl_content.strip():
                return "è¯·å…ˆå¤„ç†æ•°æ®ç”ŸæˆJSONLå†…å®¹"
            
            if not output_name.strip():
                output_name = "output"
            
            try:
                # åˆ›å»ºä¸´æ—¶ç›®å½•
                temp_dir = tempfile.mkdtemp()
                jsonl_path = os.path.join(temp_dir, "temp.jsonl")
                output_prefix = os.path.join(temp_dir, output_name)
                
                # ä¿å­˜JSONLæ–‡ä»¶
                with open(jsonl_path, 'w', encoding='utf-8') as f:
                    f.write(jsonl_content)
                
                # è½¬æ¢ä¸ºbinidx
                success, message = processor.convert_to_binidx(jsonl_path, output_prefix)
                
                if success:
                    # æ ¹æ®preprocess_data.pyçš„è¾“å‡ºæ ¼å¼ï¼Œæ–‡ä»¶åæ˜¯ {output_prefix}_text_document.bin/idx
                    bin_file = output_prefix + "_text_document.bin"
                    idx_file = output_prefix + "_text_document.idx"
                    
                    if os.path.exists(bin_file) and os.path.exists(idx_file):
                        # ç¡®å®šè¾“å‡ºç›®å½•
                        if custom_output_path.strip():
                            download_dir = custom_output_path.strip()
                        else:
                            user_home = os.path.expanduser("~")
                            download_dir = os.path.join(user_home, "RWKV_Downloads")
                        
                        # åˆ›å»ºè¾“å‡ºç›®å½•
                        try:
                            os.makedirs(download_dir, exist_ok=True)
                        except PermissionError:
                            return f"æƒé™é”™è¯¯ï¼šæ— æ³•åˆ›å»ºç›®å½• {download_dir}ï¼Œè¯·æ£€æŸ¥è·¯å¾„æƒé™æˆ–ä½¿ç”¨å…¶ä»–è·¯å¾„"
                        except Exception as e:
                            return f"åˆ›å»ºç›®å½•å¤±è´¥ï¼š{str(e)}"
                        
                        # å¤åˆ¶æ–‡ä»¶åˆ°ä¸‹è½½ç›®å½•
                        final_bin = os.path.join(download_dir, f"{output_name}.bin")
                        final_idx = os.path.join(download_dir, f"{output_name}.idx")
                        
                        shutil.copy2(bin_file, final_bin)
                        shutil.copy2(idx_file, final_idx)
                        
                        return f"è½¬æ¢æˆåŠŸï¼æ–‡ä»¶å·²ä¿å­˜åˆ°:\n{download_dir}\\{output_name}.bin\n{download_dir}\\{output_name}.idx"
                    else:
                        return "è½¬æ¢å¤±è´¥ï¼šæœªæ‰¾åˆ°ç”Ÿæˆçš„æ–‡ä»¶"
                else:
                    return message
                    
            except Exception as e:
                return f"è½¬æ¢å¤±è´¥: {str(e)}"
            finally:
                # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
                if 'temp_dir' in locals():
                    shutil.rmtree(temp_dir, ignore_errors=True)
        
        # å°è¯´å¤„ç†ç›¸å…³å‡½æ•°
        def toggle_api_key_visibility(use_ai):
            return gr.update(visible=use_ai)
        
        def process_novel_and_update(file_obj, min_len, max_len, smart_split, ai_keywords, api_key, custom_instr):
            status, jsonl, paragraphs = process_novel(file_obj, min_len, max_len, smart_split, ai_keywords, api_key, custom_instr)
            return status, jsonl, paragraphs
        
        # äº‹ä»¶ç»‘å®š
        input_mode.change(
            fn=toggle_input_mode,
            inputs=[input_mode],
            outputs=[file_input, directory_input]
        )
        
        process_btn.click(
            fn=process_data,
            inputs=[input_mode, file_input, directory_input, data_format, system_prompt],
            outputs=[status_output, preview_output, jsonl_content]
        )
        
        # å°è¯´å¤„ç†äº‹ä»¶
        use_ai_keywords.change(
            fn=toggle_api_key_visibility,
            inputs=[use_ai_keywords],
            outputs=[api_key_input]
        )
        
        novel_process_btn.click(
            fn=process_novel_and_update,
            inputs=[novel_file_input, min_length, max_length, use_smart_split, use_ai_keywords, api_key_input, custom_instruction],
            outputs=[novel_status_output, novel_jsonl_output, paragraph_data_state]
        )
        
        novel_save_btn.click(
            fn=save_jsonl,
            inputs=[novel_jsonl_output, novel_filename_input],
            outputs=[novel_save_status]
        )
        
        # æ®µè½ç¼–è¾‘ç›¸å…³äº‹ä»¶
        edit_jsonl_file.upload(
            fn=load_jsonl_for_editing,
            inputs=[edit_jsonl_file],
            outputs=[edit_paragraph_data, edit_paragraph_stats, edit_paragraph_list]
        )
        
        edit_select_all_btn.click(
            fn=select_all_paragraphs,
            inputs=[edit_paragraph_data],
            outputs=[edit_selected_indices, edit_paragraph_list]
        )
        
        edit_deselect_all_btn.click(
            fn=deselect_all_paragraphs,
            inputs=[],
            outputs=[edit_selected_indices, edit_paragraph_list]
        )
        
        edit_delete_btn.click(
            fn=delete_selected_paragraphs,
            inputs=[edit_paragraph_data, edit_selected_indices],
            outputs=[edit_paragraph_data, edit_paragraph_stats, edit_paragraph_list, edit_selected_indices]
        )
        
        edit_ai_regen_btn.click(
            fn=ai_regenerate_keywords_for_selected,
            inputs=[edit_paragraph_data, edit_selected_indices, edit_api_key, edit_prompt_template, edit_api_url, edit_model],
            outputs=[edit_paragraph_data, edit_paragraph_list, edit_ai_status]
        )
        
        # è½¬æ¢ç›¸å…³äº‹ä»¶
        download_jsonl.click(
            fn=prepare_jsonl_download,
            inputs=[jsonl_content, download_output_name, download_output_path],
            outputs=[download_jsonl, download_status]
        )
        
        convert_btn.click(
            fn=convert_to_binidx_wrapper,
            inputs=[jsonl_content, download_output_name, download_output_path],
            outputs=[convert_status]
        )
    
    return demo

if __name__ == "__main__":
    # æ£€æŸ¥å¿…è¦æ–‡ä»¶
    vocab_file = os.path.join(os.path.dirname(__file__), 'json2binidx_tool', 'rwkv_vocab_v20230424.txt')
    if not os.path.exists(vocab_file):
        print(f"é”™è¯¯ï¼šæ‰¾ä¸åˆ°è¯æ±‡æ–‡ä»¶ {vocab_file}")
        print("è¯·ç¡®ä¿json2binidx_toolç›®å½•å­˜åœ¨ä¸”åŒ…å«rwkv_vocab_v20230424.txtæ–‡ä»¶")
        sys.exit(1)
    
    # ç¡®ä¿jiebaå·²å®‰è£…
    try:
        import jieba
    except ImportError:
        print("è¯·å…ˆå®‰è£…jieba: pip install jieba")
        exit(1)
    
    demo = create_interface()
    demo.launch(
        server_name="127.0.0.1",
        server_port=7863,
        share=False,
        show_error=True
    )